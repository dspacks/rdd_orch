{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Development Environment (ADE) for Healthcare Data Documentation\n",
    "\n",
    "## Version 3.0\n",
    "\n",
    "This notebook implements a specialized development environment for building, testing, and managing AI agents using the Google Gemini API and Agent Development Kit (ADK).\n",
    "\n",
    "### System Overview\n",
    "\n",
    "The ADE creates an \"Orchestrator\" and team of sub-agents that:\n",
    "- Ingest complex, technical, and often imperfect data specifications (CSV, XML, JSON)\n",
    "- Produce comprehensive, human-readable documentation\n",
    "- Provide Human-in-the-Loop (HITL) review workflows\n",
    "- Manage context using the \"Toon\" notation system\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **SQLite Database** - Project-local persistence\n",
    "2. **Toon System** - Context management for large files\n",
    "3. **Core Agents** - Specialized AI agents for data processing\n",
    "4. **ReviewQueue** - Human-in-the-loop workflow\n",
    "5. **Orchestrator** - Agent chaining and workflow management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q google-generativeai sqlite3 pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sqlite3\nimport json\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Any, Tuple\nfrom enum import Enum\nimport google.generativeai as genai\nfrom dataclasses import dataclass, asdict\nimport hashlib\nimport os\nimport time"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Gemini API\n",
    "# Note: In Kaggle, add your API key as a secret named 'GOOGLE_API_KEY'\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "GEMINI_API_KEY = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### ⚠️ Important: API Rate Limits\n\n**Gemini API Free Tier Limits:**\n- **10 requests per minute** per model\n- The system automatically handles rate limiting with delays between requests\n- Each API call will wait ~6 seconds to stay within limits\n- Processing 7 variables takes approximately **2-3 minutes** due to rate limiting\n\n**Tips for Faster Processing:**\n1. Use `auto_approve=True` for testing (skips manual review)\n2. Start with a small dataset (3-5 variables) to test\n3. For production with paid tier, limits are much higher\n4. The system automatically retries with exponential backoff if limits are hit\n\n**If you see quota errors:**\n- The system will automatically retry (up to 3 times)\n- Wait times increase with each retry (6s, 12s, 24s)\n- Consider upgrading to paid tier for higher quotas",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Database Schema and Setup\n",
    "\n",
    "The SQLite database is the backbone of the HITL workflow and provides persistent storage for all project data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatabaseManager:\n",
    "    \"\"\"Manages SQLite database operations for the ADE project.\"\"\"\n",
    "    \n",
    "    def __init__(self, db_path: str = \"project.db\"):\n",
    "        self.db_path = db_path\n",
    "        self.conn = None\n",
    "        self.cursor = None\n",
    "        \n",
    "    def connect(self):\n",
    "        \"\"\"Establish database connection.\"\"\"\n",
    "        self.conn = sqlite3.connect(self.db_path)\n",
    "        self.conn.row_factory = sqlite3.Row\n",
    "        self.cursor = self.conn.cursor()\n",
    "        \n",
    "    def close(self):\n",
    "        \"\"\"Close database connection.\"\"\"\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "            \n",
    "    def initialize_schema(self):\n",
    "        \"\"\"Create all required tables for the ADE system.\"\"\"\n",
    "        \n",
    "        # Agents table\n",
    "        self.cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS Agents (\n",
    "            agent_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            name TEXT NOT NULL UNIQUE,\n",
    "            system_prompt TEXT NOT NULL,\n",
    "            agent_type TEXT NOT NULL,\n",
    "            config JSON,\n",
    "            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "        \"\"\")\n",
    "        \n",
    "        # Toons table - Context snippets\n",
    "        self.cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS Toons (\n",
    "            toon_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            name TEXT NOT NULL UNIQUE,\n",
    "            toon_type TEXT NOT NULL CHECK(toon_type IN (\n",
    "                'Toon_Summary', 'Toon_Chunk', 'Toon_Instruction', \n",
    "                'Toon_Version', 'Toon_Design', 'Toon_Mapping'\n",
    "            )),\n",
    "            content TEXT NOT NULL,\n",
    "            metadata JSON,\n",
    "            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "        \"\"\")\n",
    "        \n",
    "        # Jobs table\n",
    "        self.cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS Jobs (\n",
    "            job_id TEXT PRIMARY KEY,\n",
    "            source_file TEXT NOT NULL,\n",
    "            status TEXT NOT NULL DEFAULT 'Running' CHECK(status IN (\n",
    "                'Running', 'Completed', 'Failed', 'Paused'\n",
    "            )),\n",
    "            metadata JSON,\n",
    "            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "        \"\"\")\n",
    "        \n",
    "        # ReviewQueue table - HITL workflow\n",
    "        self.cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS ReviewQueue (\n",
    "            item_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            job_id TEXT NOT NULL,\n",
    "            status TEXT NOT NULL DEFAULT 'Pending' CHECK(status IN (\n",
    "                'Pending', 'Approved', 'Rejected', 'Needs_Clarification'\n",
    "            )),\n",
    "            source_agent TEXT NOT NULL,\n",
    "            target_agent TEXT,\n",
    "            source_data TEXT NOT NULL,\n",
    "            generated_content TEXT NOT NULL,\n",
    "            approved_content TEXT,\n",
    "            rejection_feedback TEXT,\n",
    "            clarification_response TEXT,\n",
    "            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "            FOREIGN KEY (job_id) REFERENCES Jobs(job_id)\n",
    "        )\n",
    "        \"\"\")\n",
    "        \n",
    "        # SystemState table - Application state\n",
    "        self.cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS SystemState (\n",
    "            state_key TEXT PRIMARY KEY,\n",
    "            state_value TEXT NOT NULL,\n",
    "            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "        \"\"\")\n",
    "        \n",
    "        # SessionHistory table - Chat logs\n",
    "        self.cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS SessionHistory (\n",
    "            history_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            job_id TEXT,\n",
    "            role TEXT NOT NULL CHECK(role IN ('user', 'assistant', 'system')),\n",
    "            content TEXT NOT NULL,\n",
    "            metadata JSON,\n",
    "            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "            FOREIGN KEY (job_id) REFERENCES Jobs(job_id)\n",
    "        )\n",
    "        \"\"\")\n",
    "        \n",
    "        self.conn.commit()\n",
    "        print(\"Database schema initialized successfully.\")\n",
    "        \n",
    "    def execute_query(self, query: str, params: tuple = ()):\n",
    "        \"\"\"Execute a query and return results.\"\"\"\n",
    "        self.cursor.execute(query, params)\n",
    "        return self.cursor.fetchall()\n",
    "    \n",
    "    def execute_update(self, query: str, params: tuple = ()):\n",
    "        \"\"\"Execute an update/insert query.\"\"\"\n",
    "        self.cursor.execute(query, params)\n",
    "        self.conn.commit()\n",
    "        return self.cursor.lastrowid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize database\n",
    "db = DatabaseManager(\"project.db\")\n",
    "db.connect()\n",
    "db.initialize_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Toon System - Context Management\n",
    "\n",
    "The \"Toon\" system is a critical component for managing large files and providing granular control over agent context. A Toon is a discrete, named snippet of context stored in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToonType(Enum):\n",
    "    \"\"\"Enumeration of Toon types.\"\"\"\n",
    "    SUMMARY = \"Toon_Summary\"\n",
    "    CHUNK = \"Toon_Chunk\"\n",
    "    INSTRUCTION = \"Toon_Instruction\"\n",
    "    VERSION = \"Toon_Version\"\n",
    "    DESIGN = \"Toon_Design\"\n",
    "    MAPPING = \"Toon_Mapping\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Toon:\n",
    "    \"\"\"Represents a context snippet (Toon).\"\"\"\n",
    "    name: str\n",
    "    toon_type: ToonType\n",
    "    content: str\n",
    "    metadata: Optional[Dict[str, Any]] = None\n",
    "    toon_id: Optional[int] = None\n",
    "    \n",
    "\n",
    "class ToonManager:\n",
    "    \"\"\"Manages the Toon Library for context management.\"\"\"\n",
    "    \n",
    "    def __init__(self, db_manager: DatabaseManager):\n",
    "        self.db = db_manager\n",
    "        \n",
    "    def create_toon(self, name: str, toon_type: ToonType, content: str, \n",
    "                    metadata: Optional[Dict] = None) -> int:\n",
    "        \"\"\"Create a new Toon in the library.\"\"\"\n",
    "        query = \"\"\"\n",
    "        INSERT INTO Toons (name, toon_type, content, metadata)\n",
    "        VALUES (?, ?, ?, ?)\n",
    "        \"\"\"\n",
    "        metadata_json = json.dumps(metadata) if metadata else None\n",
    "        toon_id = self.db.execute_update(\n",
    "            query, \n",
    "            (name, toon_type.value, content, metadata_json)\n",
    "        )\n",
    "        print(f\"Created Toon '{name}' (ID: {toon_id}, Type: {toon_type.value})\")\n",
    "        return toon_id\n",
    "    \n",
    "    def get_toon(self, toon_id: int) -> Optional[Toon]:\n",
    "        \"\"\"Retrieve a Toon by ID.\"\"\"\n",
    "        query = \"SELECT * FROM Toons WHERE toon_id = ?\"\n",
    "        result = self.db.execute_query(query, (toon_id,))\n",
    "        if result:\n",
    "            row = result[0]\n",
    "            return Toon(\n",
    "                toon_id=row['toon_id'],\n",
    "                name=row['name'],\n",
    "                toon_type=ToonType(row['toon_type']),\n",
    "                content=row['content'],\n",
    "                metadata=json.loads(row['metadata']) if row['metadata'] else None\n",
    "            )\n",
    "        return None\n",
    "    \n",
    "    def get_toon_by_name(self, name: str) -> Optional[Toon]:\n",
    "        \"\"\"Retrieve a Toon by name.\"\"\"\n",
    "        query = \"SELECT * FROM Toons WHERE name = ?\"\n",
    "        result = self.db.execute_query(query, (name,))\n",
    "        if result:\n",
    "            row = result[0]\n",
    "            return Toon(\n",
    "                toon_id=row['toon_id'],\n",
    "                name=row['name'],\n",
    "                toon_type=ToonType(row['toon_type']),\n",
    "                content=row['content'],\n",
    "                metadata=json.loads(row['metadata']) if row['metadata'] else None\n",
    "            )\n",
    "        return None\n",
    "    \n",
    "    def list_toons(self, toon_type: Optional[ToonType] = None) -> List[Toon]:\n",
    "        \"\"\"List all Toons, optionally filtered by type.\"\"\"\n",
    "        if toon_type:\n",
    "            query = \"SELECT * FROM Toons WHERE toon_type = ? ORDER BY created_at DESC\"\n",
    "            results = self.db.execute_query(query, (toon_type.value,))\n",
    "        else:\n",
    "            query = \"SELECT * FROM Toons ORDER BY created_at DESC\"\n",
    "            results = self.db.execute_query(query)\n",
    "        \n",
    "        toons = []\n",
    "        for row in results:\n",
    "            toons.append(Toon(\n",
    "                toon_id=row['toon_id'],\n",
    "                name=row['name'],\n",
    "                toon_type=ToonType(row['toon_type']),\n",
    "                content=row['content'],\n",
    "                metadata=json.loads(row['metadata']) if row['metadata'] else None\n",
    "            ))\n",
    "        return toons\n",
    "    \n",
    "    def update_toon(self, toon_id: int, content: Optional[str] = None,\n",
    "                    metadata: Optional[Dict] = None):\n",
    "        \"\"\"Update a Toon's content or metadata.\"\"\"\n",
    "        updates = []\n",
    "        params = []\n",
    "        \n",
    "        if content is not None:\n",
    "            updates.append(\"content = ?\")\n",
    "            params.append(content)\n",
    "        \n",
    "        if metadata is not None:\n",
    "            updates.append(\"metadata = ?\")\n",
    "            params.append(json.dumps(metadata))\n",
    "        \n",
    "        if updates:\n",
    "            updates.append(\"updated_at = CURRENT_TIMESTAMP\")\n",
    "            query = f\"UPDATE Toons SET {', '.join(updates)} WHERE toon_id = ?\"\n",
    "            params.append(toon_id)\n",
    "            self.db.execute_update(query, tuple(params))\n",
    "            print(f\"Updated Toon ID {toon_id}\")\n",
    "    \n",
    "    def delete_toon(self, toon_id: int):\n",
    "        \"\"\"Delete a Toon from the library.\"\"\"\n",
    "        query = \"DELETE FROM Toons WHERE toon_id = ?\"\n",
    "        self.db.execute_update(query, (toon_id,))\n",
    "        print(f\"Deleted Toon ID {toon_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Review Queue - Human-in-the-Loop Workflow\n",
    "\n",
    "The ReviewQueue manages the HITL workflow, allowing human reviewers to approve, reject, or request clarification on agent-generated content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewStatus(Enum):\n",
    "    \"\"\"Status values for review items.\"\"\"\n",
    "    PENDING = \"Pending\"\n",
    "    APPROVED = \"Approved\"\n",
    "    REJECTED = \"Rejected\"\n",
    "    NEEDS_CLARIFICATION = \"Needs_Clarification\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ReviewItem:\n",
    "    \"\"\"Represents an item in the review queue.\"\"\"\n",
    "    job_id: str\n",
    "    source_agent: str\n",
    "    source_data: str\n",
    "    generated_content: str\n",
    "    status: ReviewStatus = ReviewStatus.PENDING\n",
    "    target_agent: Optional[str] = None\n",
    "    approved_content: Optional[str] = None\n",
    "    rejection_feedback: Optional[str] = None\n",
    "    clarification_response: Optional[str] = None\n",
    "    item_id: Optional[int] = None\n",
    "\n",
    "\n",
    "class ReviewQueueManager:\n",
    "    \"\"\"Manages the review queue for HITL workflows.\"\"\"\n",
    "    \n",
    "    def __init__(self, db_manager: DatabaseManager):\n",
    "        self.db = db_manager\n",
    "    \n",
    "    def add_item(self, item: ReviewItem) -> int:\n",
    "        \"\"\"Add an item to the review queue.\"\"\"\n",
    "        query = \"\"\"\n",
    "        INSERT INTO ReviewQueue \n",
    "        (job_id, status, source_agent, target_agent, source_data, generated_content)\n",
    "        VALUES (?, ?, ?, ?, ?, ?)\n",
    "        \"\"\"\n",
    "        item_id = self.db.execute_update(\n",
    "            query,\n",
    "            (item.job_id, item.status.value, item.source_agent, \n",
    "             item.target_agent, item.source_data, item.generated_content)\n",
    "        )\n",
    "        print(f\"Added item {item_id} to review queue (Status: {item.status.value})\")\n",
    "        return item_id\n",
    "    \n",
    "    def get_pending_items(self, job_id: Optional[str] = None) -> List[ReviewItem]:\n",
    "        \"\"\"Get all pending review items, optionally filtered by job_id.\"\"\"\n",
    "        if job_id:\n",
    "            query = \"SELECT * FROM ReviewQueue WHERE status = 'Pending' AND job_id = ?\"\n",
    "            results = self.db.execute_query(query, (job_id,))\n",
    "        else:\n",
    "            query = \"SELECT * FROM ReviewQueue WHERE status = 'Pending'\"\n",
    "            results = self.db.execute_query(query)\n",
    "        \n",
    "        return [self._row_to_item(row) for row in results]\n",
    "    \n",
    "    def get_clarification_items(self, job_id: Optional[str] = None) -> List[ReviewItem]:\n",
    "        \"\"\"Get items needing clarification.\"\"\"\n",
    "        if job_id:\n",
    "            query = \"SELECT * FROM ReviewQueue WHERE status = 'Needs_Clarification' AND job_id = ?\"\n",
    "            results = self.db.execute_query(query, (job_id,))\n",
    "        else:\n",
    "            query = \"SELECT * FROM ReviewQueue WHERE status = 'Needs_Clarification'\"\n",
    "            results = self.db.execute_query(query)\n",
    "        \n",
    "        return [self._row_to_item(row) for row in results]\n",
    "    \n",
    "    def approve_item(self, item_id: int, approved_content: Optional[str] = None):\n",
    "        \"\"\"Approve a review item, optionally with edited content.\"\"\"\n",
    "        query = \"\"\"\n",
    "        UPDATE ReviewQueue \n",
    "        SET status = 'Approved', approved_content = ?, updated_at = CURRENT_TIMESTAMP\n",
    "        WHERE item_id = ?\n",
    "        \"\"\"\n",
    "        # If no approved content provided, use the generated content\n",
    "        if approved_content is None:\n",
    "            item = self.get_item(item_id)\n",
    "            approved_content = item.generated_content\n",
    "        \n",
    "        self.db.execute_update(query, (approved_content, item_id))\n",
    "        print(f\"Approved item {item_id}\")\n",
    "    \n",
    "    def reject_item(self, item_id: int, feedback: str):\n",
    "        \"\"\"Reject a review item with feedback.\"\"\"\n",
    "        query = \"\"\"\n",
    "        UPDATE ReviewQueue \n",
    "        SET status = 'Rejected', rejection_feedback = ?, updated_at = CURRENT_TIMESTAMP\n",
    "        WHERE item_id = ?\n",
    "        \"\"\"\n",
    "        self.db.execute_update(query, (feedback, item_id))\n",
    "        print(f\"Rejected item {item_id}\")\n",
    "    \n",
    "    def request_clarification(self, item_id: int, question: str):\n",
    "        \"\"\"Mark item as needing clarification.\"\"\"\n",
    "        query = \"\"\"\n",
    "        UPDATE ReviewQueue \n",
    "        SET status = 'Needs_Clarification', generated_content = ?, updated_at = CURRENT_TIMESTAMP\n",
    "        WHERE item_id = ?\n",
    "        \"\"\"\n",
    "        self.db.execute_update(query, (question, item_id))\n",
    "        print(f\"Requested clarification for item {item_id}\")\n",
    "    \n",
    "    def submit_clarification(self, item_id: int, response: str):\n",
    "        \"\"\"Submit clarification response and return to pending.\"\"\"\n",
    "        query = \"\"\"\n",
    "        UPDATE ReviewQueue \n",
    "        SET clarification_response = ?, status = 'Pending', updated_at = CURRENT_TIMESTAMP\n",
    "        WHERE item_id = ?\n",
    "        \"\"\"\n",
    "        self.db.execute_update(query, (response, item_id))\n",
    "        print(f\"Submitted clarification for item {item_id}\")\n",
    "    \n",
    "    def get_item(self, item_id: int) -> Optional[ReviewItem]:\n",
    "        \"\"\"Get a specific review item.\"\"\"\n",
    "        query = \"SELECT * FROM ReviewQueue WHERE item_id = ?\"\n",
    "        results = self.db.execute_query(query, (item_id,))\n",
    "        if results:\n",
    "            return self._row_to_item(results[0])\n",
    "        return None\n",
    "    \n",
    "    def get_approved_items(self, job_id: str) -> List[ReviewItem]:\n",
    "        \"\"\"Get all approved items for a job.\"\"\"\n",
    "        query = \"SELECT * FROM ReviewQueue WHERE job_id = ? AND status = 'Approved'\"\n",
    "        results = self.db.execute_query(query, (job_id,))\n",
    "        return [self._row_to_item(row) for row in results]\n",
    "    \n",
    "    def _row_to_item(self, row) -> ReviewItem:\n",
    "        \"\"\"Convert database row to ReviewItem.\"\"\"\n",
    "        return ReviewItem(\n",
    "            item_id=row['item_id'],\n",
    "            job_id=row['job_id'],\n",
    "            status=ReviewStatus(row['status']),\n",
    "            source_agent=row['source_agent'],\n",
    "            target_agent=row['target_agent'],\n",
    "            source_data=row['source_data'],\n",
    "            generated_content=row['generated_content'],\n",
    "            approved_content=row['approved_content'],\n",
    "            rejection_feedback=row['rejection_feedback'],\n",
    "            clarification_response=row['clarification_response']\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Core Agent Classes\n",
    "\n",
    "These agents form the processing pipeline for healthcare data documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class BaseAgent:\n    \"\"\"Base class for all agents with rate limiting and retry logic.\"\"\"\n    \n    def __init__(self, name: str, system_prompt: str, \n                 model_name: str = \"gemini-2.0-flash-exp\",\n                 requests_per_minute: int = 10):\n        self.name = name\n        self.system_prompt = system_prompt\n        self.model_name = model_name\n        self.model = genai.GenerativeModel(model_name)\n        self.active_toons: List[Toon] = []\n        self.requests_per_minute = requests_per_minute\n        self.min_delay = 60.0 / requests_per_minute  # Minimum delay between requests\n        self.last_request_time = 0\n        \n    def inject_toons(self, toons: List[Toon]):\n        \"\"\"Inject Toons into agent context.\"\"\"\n        self.active_toons = toons\n        \n    def build_prompt(self, user_input: str, additional_context: str = \"\") -> str:\n        \"\"\"Build the full prompt with system prompt, Toons, and user input.\"\"\"\n        prompt_parts = [self.system_prompt]\n        \n        # Add active Toons as context\n        if self.active_toons:\n            prompt_parts.append(\"\\n=== CONTEXT (Toons) ===\")\n            for toon in self.active_toons:\n                prompt_parts.append(f\"\\n[{toon.toon_type.value}: {toon.name}]\")\n                prompt_parts.append(toon.content)\n        \n        # Add additional context\n        if additional_context:\n            prompt_parts.append(\"\\n=== ADDITIONAL CONTEXT ===\")\n            prompt_parts.append(additional_context)\n        \n        # Add user input\n        prompt_parts.append(\"\\n=== INPUT ===\")\n        prompt_parts.append(user_input)\n        \n        return \"\\n\".join(prompt_parts)\n    \n    def _wait_for_rate_limit(self):\n        \"\"\"Implement rate limiting by waiting if necessary.\"\"\"\n        if self.last_request_time > 0:\n            elapsed = time.time() - self.last_request_time\n            if elapsed < self.min_delay:\n                wait_time = self.min_delay - elapsed\n                print(f\"⏱️  Rate limiting: waiting {wait_time:.1f}s...\")\n                time.sleep(wait_time)\n    \n    def generate(self, prompt: str, max_retries: int = 3) -> str:\n        \"\"\"Generate response using Gemini API with retry logic and rate limiting.\"\"\"\n        for attempt in range(max_retries):\n            try:\n                # Wait for rate limit before making request\n                self._wait_for_rate_limit()\n                \n                # Make API call\n                self.last_request_time = time.time()\n                response = self.model.generate_content(prompt)\n                return response.text\n                \n            except Exception as e:\n                error_str = str(e)\n                \n                # Check if it's a rate limit error\n                if \"ResourceExhausted\" in error_str or \"429\" in error_str:\n                    # Extract retry delay from error if available\n                    retry_delay = 6.0  # Default to 6 seconds\n                    \n                    # Try to extract the suggested retry delay\n                    if \"retry_delay\" in error_str:\n                        try:\n                            import re\n                            match = re.search(r'seconds: (\\d+)', error_str)\n                            if match:\n                                retry_delay = float(match.group(1))\n                        except:\n                            pass\n                    \n                    # Add exponential backoff\n                    wait_time = retry_delay * (2 ** attempt)\n                    \n                    if attempt < max_retries - 1:\n                        print(f\"⚠️  Rate limit hit. Waiting {wait_time:.1f}s before retry {attempt + 1}/{max_retries}...\")\n                        time.sleep(wait_time)\n                    else:\n                        print(f\"❌ Max retries reached. Error: {error_str}\")\n                        raise\n                else:\n                    # For non-rate-limit errors, raise immediately\n                    print(f\"❌ API Error: {error_str}\")\n                    raise\n        \n        raise Exception(f\"Failed after {max_retries} retries\")\n    \n    def process(self, input_data: str, additional_context: str = \"\") -> str:\n        \"\"\"Process input and return output.\"\"\"\n        full_prompt = self.build_prompt(input_data, additional_context)\n        return self.generate(full_prompt)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataParserAgent(BaseAgent):\n",
    "    \"\"\"Agent for parsing raw data into standardized JSON format.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        system_prompt = \"\"\"\n",
    "You are a DataParserAgent specialized in converting raw data specifications \n",
    "(CSV, JSON, XML) into a standardized JSON format.\n",
    "\n",
    "Your task:\n",
    "1. Parse the input data\n",
    "2. Preserve all original field names and values\n",
    "3. Output a JSON array where each element represents one variable/field\n",
    "4. Each element should include: original_name, original_type (if available), \n",
    "   original_description (if available), and any other metadata\n",
    "\n",
    "Output format:\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"original_name\": \"field_name\",\n",
    "    \"original_type\": \"type\",\n",
    "    \"original_description\": \"description\",\n",
    "    \"metadata\": {}\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "Only output valid JSON. No additional commentary.\n",
    "\"\"\"\n",
    "        super().__init__(\"DataParserAgent\", system_prompt)\n",
    "    \n",
    "    def parse_csv(self, csv_data: str) -> List[Dict]:\n",
    "        \"\"\"Parse CSV data dictionary.\"\"\"\n",
    "        result = self.process(csv_data)\n",
    "        # Extract JSON from markdown code block if present\n",
    "        if \"```json\" in result:\n",
    "            result = result.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in result:\n",
    "            result = result.split(\"```\")[1].split(\"```\")[0].strip()\n",
    "        return json.loads(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TechnicalAnalyzerAgent(BaseAgent):\n",
    "    \"\"\"Agent for analyzing technical properties and mapping to internal standards.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        system_prompt = \"\"\"\n",
    "You are a TechnicalAnalyzerAgent specialized in analyzing data fields and \n",
    "mapping them to internal standards.\n",
    "\n",
    "Your task:\n",
    "1. Analyze each field from the parsed data\n",
    "2. Infer technical properties (data_type, constraints, cardinality)\n",
    "3. Map to standardized field names following healthcare data conventions\n",
    "4. If mapping is unclear or confidence is low, flag for clarification\n",
    "\n",
    "Standard field mappings:\n",
    "- variable_name: standardized variable name\n",
    "- data_type: categorical, continuous, date, text, boolean\n",
    "- description: human-readable description\n",
    "- constraints: any validation rules\n",
    "- cardinality: required, optional, repeated\n",
    "- confidence: high, medium, low (for mapping quality)\n",
    "\n",
    "Output format:\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"original_name\": \"field_name\",\n",
    "    \"variable_name\": \"standardized_name\",\n",
    "    \"data_type\": \"categorical\",\n",
    "    \"description\": \"description\",\n",
    "    \"constraints\": {},\n",
    "    \"cardinality\": \"required\",\n",
    "    \"confidence\": \"high\",\n",
    "    \"needs_clarification\": false,\n",
    "    \"clarification_question\": \"\"\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "Only output valid JSON. No additional commentary.\n",
    "\"\"\"\n",
    "        super().__init__(\"TechnicalAnalyzerAgent\", system_prompt)\n",
    "    \n",
    "    def analyze(self, parsed_data: List[Dict], \n",
    "                clarifications: Optional[Dict[str, str]] = None) -> List[Dict]:\n",
    "        \"\"\"Analyze parsed data and map to internal standards.\"\"\"\n",
    "        additional_context = \"\"\n",
    "        if clarifications:\n",
    "            additional_context = \"\\n=== USER CLARIFICATIONS ===\\n\"\n",
    "            for field, clarification in clarifications.items():\n",
    "                additional_context += f\"{field}: {clarification}\\n\"\n",
    "        \n",
    "        result = self.process(json.dumps(parsed_data, indent=2), additional_context)\n",
    "        \n",
    "        # Extract JSON from markdown code block if present\n",
    "        if \"```json\" in result:\n",
    "            result = result.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in result:\n",
    "            result = result.split(\"```\")[1].split(\"```\")[0].strip()\n",
    "        \n",
    "        return json.loads(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DomainOntologyAgent(BaseAgent):\n",
    "    \"\"\"Agent for mapping to standard healthcare ontologies (OMOP, LOINC, etc.).\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        system_prompt = \"\"\"\n",
    "You are a DomainOntologyAgent specialized in mapping healthcare data fields \n",
    "to standard ontologies and terminologies.\n",
    "\n",
    "Your task:\n",
    "1. For each variable, identify appropriate standard ontology codes\n",
    "2. Primary ontologies to consider:\n",
    "   - OMOP CDM concepts\n",
    "   - LOINC codes (for lab/clinical observations)\n",
    "   - SNOMED CT (for clinical terms)\n",
    "   - RxNorm (for medications)\n",
    "3. Provide both the code and the standard term\n",
    "4. Include a confidence score for each mapping\n",
    "\n",
    "Output format:\n",
    "```json\n",
    "{\n",
    "  \"variable_name\": \"standardized_name\",\n",
    "  \"ontology_mappings\": [\n",
    "    {\n",
    "      \"system\": \"OMOP\",\n",
    "      \"code\": \"123456\",\n",
    "      \"display\": \"Standard Concept Name\",\n",
    "      \"confidence\": \"high\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "Only output valid JSON. No additional commentary.\n",
    "\"\"\"\n",
    "        super().__init__(\"DomainOntologyAgent\", system_prompt)\n",
    "    \n",
    "    def map_ontologies(self, variable_data: Dict) -> Dict:\n",
    "        \"\"\"Map a variable to standard ontologies.\"\"\"\n",
    "        result = self.process(json.dumps(variable_data, indent=2))\n",
    "        \n",
    "        # Extract JSON from markdown code block if present\n",
    "        if \"```json\" in result:\n",
    "            result = result.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in result:\n",
    "            result = result.split(\"```\")[1].split(\"```\")[0].strip()\n",
    "        \n",
    "        return json.loads(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainLanguageAgent(BaseAgent):\n",
    "    \"\"\"Agent for generating human-readable documentation.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        system_prompt = \"\"\"\n",
    "You are a PlainLanguageAgent specialized in creating clear, comprehensive, \n",
    "human-readable documentation for healthcare data variables.\n",
    "\n",
    "Your task:\n",
    "1. Convert technical variable specifications into plain language\n",
    "2. Explain what the variable represents in clinical/research context\n",
    "3. Describe data type, constraints, and valid values\n",
    "4. Include ontology mappings and their significance\n",
    "5. Write for an interdisciplinary audience (clinicians, researchers, data scientists)\n",
    "\n",
    "Output format (Markdown):\n",
    "```markdown\n",
    "## Variable: [Variable Name]\n",
    "\n",
    "**Description:** [Clear, concise description]\n",
    "\n",
    "**Technical Details:**\n",
    "- Data Type: [type]\n",
    "- Cardinality: [required/optional]\n",
    "- Valid Values: [constraints or ranges]\n",
    "\n",
    "**Standard Ontology Mappings:**\n",
    "- OMOP: [code] - [term]\n",
    "- LOINC: [code] - [term]\n",
    "\n",
    "**Clinical Context:** [Explanation of why this variable matters]\n",
    "```\n",
    "\n",
    "Only output Markdown documentation. No additional commentary.\n",
    "\"\"\"\n",
    "        super().__init__(\"PlainLanguageAgent\", system_prompt)\n",
    "    \n",
    "    def document_variable(self, enriched_data: Dict) -> str:\n",
    "        \"\"\"Generate plain language documentation for a variable.\"\"\"\n",
    "        result = self.process(json.dumps(enriched_data, indent=2))\n",
    "        \n",
    "        # Remove markdown code block markers if present\n",
    "        if \"```markdown\" in result:\n",
    "            result = result.split(\"```markdown\")[1].split(\"```\")[0].strip()\n",
    "        elif result.startswith(\"```\") and result.endswith(\"```\"):\n",
    "            result = result.split(\"```\")[1].split(\"```\")[0].strip()\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentationAssemblerAgent(BaseAgent):\n",
    "    \"\"\"Agent for assembling final documentation from approved items.\"\"\"\n",
    "    \n",
    "    def __init__(self, review_queue: ReviewQueueManager):\n",
    "        system_prompt = \"\"\"\n",
    "You are a DocumentationAssemblerAgent specialized in creating comprehensive, \n",
    "well-structured data documentation.\n",
    "\n",
    "Your task:\n",
    "1. Compile all approved variable documentation into a cohesive document\n",
    "2. Add a table of contents\n",
    "3. Include metadata (generation date, source file, etc.)\n",
    "4. Organize by logical groupings if applicable\n",
    "5. Ensure consistent formatting throughout\n",
    "\n",
    "Output: A complete Markdown document ready for publication.\n",
    "\"\"\"\n",
    "        super().__init__(\"DocumentationAssemblerAgent\", system_prompt)\n",
    "        self.review_queue = review_queue\n",
    "    \n",
    "    def assemble(self, job_id: str) -> str:\n",
    "        \"\"\"Assemble final documentation from approved review items.\"\"\"\n",
    "        approved_items = self.review_queue.get_approved_items(job_id)\n",
    "        \n",
    "        if not approved_items:\n",
    "            return \"# No approved documentation found for this job.\"\n",
    "        \n",
    "        # Build document\n",
    "        doc_parts = [\n",
    "            \"# Healthcare Data Documentation\",\n",
    "            f\"\\n**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "            f\"**Job ID:** {job_id}\",\n",
    "            \"\\n---\\n\"\n",
    "        ]\n",
    "        \n",
    "        # Add table of contents\n",
    "        doc_parts.append(\"## Table of Contents\\n\")\n",
    "        for i, item in enumerate(approved_items, 1):\n",
    "            # Extract variable name from content if possible\n",
    "            content = item.approved_content\n",
    "            if \"## Variable:\" in content:\n",
    "                var_name = content.split(\"## Variable:\")[1].split(\"\\n\")[0].strip()\n",
    "                doc_parts.append(f\"{i}. [{var_name}](#{var_name.lower().replace(' ', '-')})\")\n",
    "        \n",
    "        doc_parts.append(\"\\n---\\n\")\n",
    "        \n",
    "        # Add all approved content\n",
    "        for item in approved_items:\n",
    "            doc_parts.append(item.approved_content)\n",
    "            doc_parts.append(\"\\n---\\n\")\n",
    "        \n",
    "        return \"\\n\".join(doc_parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Orchestrator - Agent Workflow Management\n",
    "\n",
    "The Orchestrator manages the flow of data through the agent pipeline and handles the HITL workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Orchestrator:\n",
    "    \"\"\"Manages the workflow of agents and coordinates the documentation pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self, db_manager: DatabaseManager):\n",
    "        self.db = db_manager\n",
    "        self.toon_manager = ToonManager(db_manager)\n",
    "        self.review_queue = ReviewQueueManager(db_manager)\n",
    "        \n",
    "        # Initialize agents\n",
    "        self.data_parser = DataParserAgent()\n",
    "        self.technical_analyzer = TechnicalAnalyzerAgent()\n",
    "        self.domain_ontology = DomainOntologyAgent()\n",
    "        self.plain_language = PlainLanguageAgent()\n",
    "        self.assembler = DocumentationAssemblerAgent(self.review_queue)\n",
    "        \n",
    "    def create_job(self, source_file: str) -> str:\n",
    "        \"\"\"Create a new documentation job.\"\"\"\n",
    "        # Generate unique job ID\n",
    "        job_id = hashlib.md5(\n",
    "            f\"{source_file}_{datetime.now().isoformat()}\".encode()\n",
    "        ).hexdigest()[:12]\n",
    "        \n",
    "        query = \"\"\"\n",
    "        INSERT INTO Jobs (job_id, source_file, status)\n",
    "        VALUES (?, ?, 'Running')\n",
    "        \"\"\"\n",
    "        self.db.execute_update(query, (job_id, source_file))\n",
    "        print(f\"Created job {job_id} for source file: {source_file}\")\n",
    "        return job_id\n",
    "    \n",
    "    def process_data_dictionary(self, source_data: str, source_file: str = \"input.csv\",\n",
    "                                auto_approve: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        Main workflow: Process a data dictionary through the agent pipeline.\n",
    "        \n",
    "        Args:\n",
    "            source_data: The raw data dictionary content\n",
    "            source_file: Name of the source file\n",
    "            auto_approve: If True, automatically approve all generated content\n",
    "        \n",
    "        Returns:\n",
    "            job_id: The ID of the created job\n",
    "        \"\"\"\n",
    "        # Create job\n",
    "        job_id = self.create_job(source_file)\n",
    "        \n",
    "        print(\"\\n=== Step 1: Parsing Data ===\")\n",
    "        parsed_data = self.data_parser.parse_csv(source_data)\n",
    "        print(f\"Parsed {len(parsed_data)} variables\")\n",
    "        \n",
    "        print(\"\\n=== Step 2: Technical Analysis ===\")\n",
    "        analyzed_data = self.technical_analyzer.analyze(parsed_data)\n",
    "        print(f\"Analyzed {len(analyzed_data)} variables\")\n",
    "        \n",
    "        # Check for clarifications needed\n",
    "        needs_clarification = [v for v in analyzed_data if v.get('needs_clarification', False)]\n",
    "        if needs_clarification:\n",
    "            print(f\"\\n⚠️  {len(needs_clarification)} variables need clarification\")\n",
    "            for var in needs_clarification:\n",
    "                # Add to review queue\n",
    "                item = ReviewItem(\n",
    "                    job_id=job_id,\n",
    "                    source_agent=\"TechnicalAnalyzerAgent\",\n",
    "                    source_data=json.dumps(var),\n",
    "                    generated_content=var.get('clarification_question', 'Needs clarification'),\n",
    "                    status=ReviewStatus.NEEDS_CLARIFICATION\n",
    "                )\n",
    "                self.review_queue.add_item(item)\n",
    "        \n",
    "        print(\"\\n=== Step 3: Domain Ontology Mapping ===\")\n",
    "        enriched_data = []\n",
    "        for i, var in enumerate(analyzed_data):\n",
    "            if not var.get('needs_clarification', False):\n",
    "                print(f\"Mapping variable {i+1}/{len(analyzed_data)}: {var.get('variable_name', 'unknown')}\")\n",
    "                enriched = self.domain_ontology.map_ontologies(var)\n",
    "                enriched_data.append(enriched)\n",
    "        \n",
    "        print(f\"\\nEnriched {len(enriched_data)} variables with ontology mappings\")\n",
    "        \n",
    "        print(\"\\n=== Step 4: Plain Language Documentation ===\")\n",
    "        for i, var in enumerate(enriched_data):\n",
    "            print(f\"Documenting variable {i+1}/{len(enriched_data)}: {var.get('variable_name', 'unknown')}\")\n",
    "            documentation = self.plain_language.document_variable(var)\n",
    "            \n",
    "            # Add to review queue\n",
    "            item = ReviewItem(\n",
    "                job_id=job_id,\n",
    "                source_agent=\"PlainLanguageAgent\",\n",
    "                source_data=json.dumps(var),\n",
    "                generated_content=documentation,\n",
    "                status=ReviewStatus.PENDING\n",
    "            )\n",
    "            item_id = self.review_queue.add_item(item)\n",
    "            \n",
    "            # Auto-approve if requested\n",
    "            if auto_approve:\n",
    "                self.review_queue.approve_item(item_id)\n",
    "        \n",
    "        print(f\"\\n✓ Job {job_id} processing complete\")\n",
    "        print(f\"  - {len(enriched_data)} items ready for review\")\n",
    "        if needs_clarification:\n",
    "            print(f\"  - {len(needs_clarification)} items need clarification\")\n",
    "        \n",
    "        return job_id\n",
    "    \n",
    "    def finalize_documentation(self, job_id: str, output_file: str = \"documentation.md\") -> str:\n",
    "        \"\"\"Assemble and save final documentation.\"\"\"\n",
    "        print(f\"\\n=== Assembling Final Documentation ===\")\n",
    "        documentation = self.assembler.assemble(job_id)\n",
    "        \n",
    "        # Save to file\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(documentation)\n",
    "        \n",
    "        print(f\"✓ Documentation saved to {output_file}\")\n",
    "        \n",
    "        # Update job status\n",
    "        query = \"UPDATE Jobs SET status = 'Completed' WHERE job_id = ?\"\n",
    "        self.db.execute_update(query, (job_id,))\n",
    "        \n",
    "        return documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Example Usage and Demonstration\n",
    "\n",
    "Let's demonstrate the system with a sample healthcare data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample REDCap-style data dictionary\n",
    "sample_data_dictionary = \"\"\"Variable Name,Field Type,Field Label,Choices,Notes\n",
    "patient_id,text,Patient ID,,Unique identifier\n",
    "age,integer,Age (years),,Age at enrollment\n",
    "sex,radio,Biological Sex,\"1, Male | 2, Female | 3, Other\",\n",
    "bp_systolic,integer,Systolic Blood Pressure (mmHg),,\n",
    "bp_diastolic,integer,Diastolic Blood Pressure (mmHg),,\n",
    "diagnosis_date,date,Diagnosis Date,,Date of primary diagnosis\n",
    "hba1c,decimal,Hemoglobin A1c (%),,Glycated hemoglobin\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize orchestrator\n",
    "orchestrator = Orchestrator(db)\n",
    "\n",
    "# Create some useful Toons for context\n",
    "print(\"Creating Toons for context management...\")\n",
    "\n",
    "# Instruction Toon for OMOP mapping\n",
    "orchestrator.toon_manager.create_toon(\n",
    "    name=\"OMOP_Mapping_Instructions\",\n",
    "    toon_type=ToonType.INSTRUCTION,\n",
    "    content=\"\"\"When mapping to OMOP CDM:\n",
    "- Blood pressure measurements should map to OMOP concept_id 3004249 (Systolic) and 3012888 (Diastolic)\n",
    "- HbA1c should map to OMOP concept_id 3004410\n",
    "- Age should be stored as an integer in years\n",
    "- Sex should use standard OMOP gender concepts: 8507 (Male), 8532 (Female)\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Design decision Toon\n",
    "orchestrator.toon_manager.create_toon(\n",
    "    name=\"Project_Design_Notes\",\n",
    "    toon_type=ToonType.DESIGN,\n",
    "    content=\"\"\"This is a diabetes research study collecting baseline clinical measurements.\n",
    "All measurements follow standard clinical protocols. Blood pressure is measured in sitting position\n",
    "after 5 minutes rest. HbA1c measured using DCCT-aligned assay.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\"\\nToons created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inject Toons into relevant agents\n",
    "toons = orchestrator.toon_manager.list_toons()\n",
    "orchestrator.domain_ontology.inject_toons(toons)\n",
    "orchestrator.plain_language.inject_toons(toons)\n",
    "\n",
    "print(f\"Injected {len(toons)} Toons into agent context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the data dictionary\n",
    "job_id = orchestrator.process_data_dictionary(\n",
    "    source_data=sample_data_dictionary,\n",
    "    source_file=\"diabetes_study_data_dictionary.csv\",\n",
    "    auto_approve=True  # Set to False to enable manual review\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review pending items (if auto_approve=False)\n",
    "pending_items = orchestrator.review_queue.get_pending_items(job_id)\n",
    "print(f\"\\nPending review items: {len(pending_items)}\")\n",
    "\n",
    "if pending_items:\n",
    "    print(\"\\n=== Review Interface ===\")\n",
    "    for item in pending_items[:3]:  # Show first 3\n",
    "        print(f\"\\nItem {item.item_id}:\")\n",
    "        print(f\"Source: {item.source_agent}\")\n",
    "        print(f\"\\nGenerated Content:\\n{item.generated_content[:500]}...\")\n",
    "        print(\"\\nActions: [Approve], [Edit & Approve], [Reject]\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Approve an item with edits\n",
    "if pending_items:\n",
    "    item_id = pending_items[0].item_id\n",
    "    \n",
    "    # Get the generated content\n",
    "    original_content = pending_items[0].generated_content\n",
    "    \n",
    "    # Make an edit (example)\n",
    "    edited_content = original_content.replace(\n",
    "        \"Clinical Context:\",\n",
    "        \"Clinical Context: [EDITED BY REVIEWER]\"\n",
    "    )\n",
    "    \n",
    "    # Approve with edits\n",
    "    orchestrator.review_queue.approve_item(item_id, edited_content)\n",
    "    print(f\"Approved item {item_id} with edits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for clarification requests\n",
    "clarification_items = orchestrator.review_queue.get_clarification_items(job_id)\n",
    "print(f\"\\nItems needing clarification: {len(clarification_items)}\")\n",
    "\n",
    "if clarification_items:\n",
    "    print(\"\\n=== Clarification Interface ===\")\n",
    "    for item in clarification_items:\n",
    "        print(f\"\\nItem {item.item_id}:\")\n",
    "        print(f\"Question: {item.generated_content}\")\n",
    "        print(\"\\nYour Response: [Enter clarification here]\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Submit clarification\n",
    "if clarification_items:\n",
    "    item_id = clarification_items[0].item_id\n",
    "    clarification_response = \"This field represents the patient's biological sex as reported at enrollment.\"\n",
    "    \n",
    "    orchestrator.review_queue.submit_clarification(item_id, clarification_response)\n",
    "    print(f\"Submitted clarification for item {item_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize and generate documentation\n",
    "final_documentation = orchestrator.finalize_documentation(\n",
    "    job_id=job_id,\n",
    "    output_file=\"healthcare_data_documentation.md\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== Final Documentation Preview ===\")\n",
    "print(final_documentation[:1000] + \"\\n...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Context Management - Working vs Long-Term Memory\n",
    "\n",
    "Demonstration of the memory model and context compaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextManager:\n",
    "    \"\"\"Manages working and long-term memory for the ADE system.\"\"\"\n",
    "    \n",
    "    def __init__(self, db_manager: DatabaseManager, max_tokens: int = 100000):\n",
    "        self.db = db_manager\n",
    "        self.max_tokens = max_tokens\n",
    "        self.compaction_threshold = int(max_tokens * 0.8)\n",
    "        \n",
    "    def estimate_tokens(self, text: str) -> int:\n",
    "        \"\"\"Rough token estimation (1 token ≈ 4 characters).\"\"\"\n",
    "        return len(text) // 4\n",
    "    \n",
    "    def get_working_memory(self, job_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"Get current working memory for a job.\"\"\"\n",
    "        # Get active toons\n",
    "        query = \"SELECT state_value FROM SystemState WHERE state_key = ?\"\n",
    "        result = self.db.execute_query(query, (f\"active_toons_{job_id}\",))\n",
    "        active_toon_ids = json.loads(result[0]['state_value']) if result else []\n",
    "        \n",
    "        # Get session history\n",
    "        query = \"SELECT * FROM SessionHistory WHERE job_id = ? ORDER BY created_at\"\n",
    "        history_rows = self.db.execute_query(query, (job_id,))\n",
    "        \n",
    "        session_history = []\n",
    "        for row in history_rows:\n",
    "            session_history.append({\n",
    "                'role': row['role'],\n",
    "                'content': row['content'],\n",
    "                'timestamp': row['created_at']\n",
    "            })\n",
    "        \n",
    "        # Calculate total tokens\n",
    "        total_tokens = sum(self.estimate_tokens(msg['content']) for msg in session_history)\n",
    "        \n",
    "        return {\n",
    "            'active_toon_ids': active_toon_ids,\n",
    "            'session_history': session_history,\n",
    "            'total_tokens': total_tokens,\n",
    "            'needs_compaction': total_tokens > self.compaction_threshold\n",
    "        }\n",
    "    \n",
    "    def add_to_session_history(self, job_id: str, role: str, content: str):\n",
    "        \"\"\"Add a message to session history.\"\"\"\n",
    "        query = \"\"\"\n",
    "        INSERT INTO SessionHistory (job_id, role, content)\n",
    "        VALUES (?, ?, ?)\n",
    "        \"\"\"\n",
    "        self.db.execute_update(query, (job_id, role, content))\n",
    "    \n",
    "    def compact_context(self, job_id: str) -> str:\n",
    "        \"\"\"Compact session history using summarization.\"\"\"\n",
    "        print(\"\\n=== Context Compaction Triggered ===\")\n",
    "        \n",
    "        working_memory = self.get_working_memory(job_id)\n",
    "        session_history = working_memory['session_history']\n",
    "        \n",
    "        # Build conversation text\n",
    "        conversation = \"\\n\\n\".join([\n",
    "            f\"{msg['role'].upper()}: {msg['content']}\"\n",
    "            for msg in session_history\n",
    "        ])\n",
    "        \n",
    "        # Use Gemini to summarize\n",
    "        compactor_prompt = f\"\"\"\n",
    "You are a ContextCompactorAgent. Your task is to create a concise summary of this\n",
    "conversation that preserves all critical information, decisions, and clarifications.\n",
    "\n",
    "Conversation:\n",
    "{conversation}\n",
    "\n",
    "Provide a structured summary that includes:\n",
    "1. Key decisions made\n",
    "2. Important clarifications provided\n",
    "3. Current state of the work\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "        \n",
    "        model = genai.GenerativeModel(\"gemini-2.0-flash-exp\")\n",
    "        response = model.generate_content(compactor_prompt)\n",
    "        summary = response.text\n",
    "        \n",
    "        print(f\"Original tokens: {working_memory['total_tokens']}\")\n",
    "        print(f\"Summary tokens: {self.estimate_tokens(summary)}\")\n",
    "        print(f\"Reduction: {100 * (1 - self.estimate_tokens(summary) / working_memory['total_tokens']):.1f}%\")\n",
    "        \n",
    "        # Store summary as a Toon\n",
    "        toon_manager = ToonManager(self.db)\n",
    "        toon_manager.create_toon(\n",
    "            name=f\"Session_Summary_{job_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "            toon_type=ToonType.SUMMARY,\n",
    "            content=summary,\n",
    "            metadata={'job_id': job_id, 'original_tokens': working_memory['total_tokens']}\n",
    "        )\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def clear_context(self, job_id: str):\n",
    "        \"\"\"Clear working memory (flush session history and active toons).\"\"\"\n",
    "        # Note: This doesn't delete from database, just resets the \"active\" state\n",
    "        query = \"UPDATE SystemState SET state_value = '[]' WHERE state_key = ?\"\n",
    "        self.db.execute_update(query, (f\"active_toons_{job_id}\",))\n",
    "        \n",
    "        print(f\"Cleared working memory for job {job_id}\")\n",
    "        print(\"Note: Session history preserved in long-term memory (database)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate context management\n",
    "context_manager = ContextManager(db)\n",
    "\n",
    "# Simulate a conversation\n",
    "context_manager.add_to_session_history(job_id, \"user\", \"Please process the diabetes data dictionary\")\n",
    "context_manager.add_to_session_history(job_id, \"assistant\", \"I'll process the data dictionary through the agent pipeline.\")\n",
    "context_manager.add_to_session_history(\n",
    "    job_id, \n",
    "    \"user\", \n",
    "    \"For the HbA1c variable, please note that values above 6.5% indicate diabetes diagnosis.\"\n",
    ")\n",
    "\n",
    "# Check working memory\n",
    "working_memory = context_manager.get_working_memory(job_id)\n",
    "print(f\"\\nWorking Memory Status:\")\n",
    "print(f\"  Total tokens: {working_memory['total_tokens']}\")\n",
    "print(f\"  Needs compaction: {working_memory['needs_compaction']}\")\n",
    "print(f\"  Session messages: {len(working_memory['session_history'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. System Status and Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_system_status(db: DatabaseManager):\n",
    "    \"\"\"Display current system status.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ADE SYSTEM STATUS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Jobs\n",
    "    jobs = db.execute_query(\"SELECT * FROM Jobs\")\n",
    "    print(f\"\\nJobs: {len(jobs)}\")\n",
    "    for job in jobs:\n",
    "        print(f\"  [{job['job_id']}] {job['source_file']} - Status: {job['status']}\")\n",
    "    \n",
    "    # Toons\n",
    "    toons = db.execute_query(\"SELECT toon_type, COUNT(*) as count FROM Toons GROUP BY toon_type\")\n",
    "    print(f\"\\nToon Library:\")\n",
    "    for toon in toons:\n",
    "        print(f\"  {toon['toon_type']}: {toon['count']}\")\n",
    "    \n",
    "    # Review Queue\n",
    "    review_stats = db.execute_query(\n",
    "        \"SELECT status, COUNT(*) as count FROM ReviewQueue GROUP BY status\"\n",
    "    )\n",
    "    print(f\"\\nReview Queue:\")\n",
    "    for stat in review_stats:\n",
    "        print(f\"  {stat['status']}: {stat['count']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "display_system_status(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export and Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export database for backup\n",
    "import shutil\n",
    "\n",
    "def backup_database(db_path: str, backup_path: str):\n",
    "    \"\"\"Create a backup of the project database.\"\"\"\n",
    "    shutil.copy2(db_path, backup_path)\n",
    "    print(f\"Database backed up to {backup_path}\")\n",
    "\n",
    "# Uncomment to create backup\n",
    "# backup_database(\"project.db\", \"project_backup.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View generated documentation\n",
    "if os.path.exists(\"healthcare_data_documentation.md\"):\n",
    "    with open(\"healthcare_data_documentation.md\", 'r') as f:\n",
    "        print(\"\\n=== Generated Documentation ===\")\n",
    "        print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates a complete implementation of the Agent Development Environment (ADE) for Healthcare Data Documentation. Key features:\n",
    "\n",
    "### Implemented Components:\n",
    "1. **SQLite Database** - Full schema with all required tables\n",
    "2. **Toon System** - Context management with 6 Toon types\n",
    "3. **Review Queue** - Complete HITL workflow with status management\n",
    "4. **Core Agents**:\n",
    "   - DataParserAgent\n",
    "   - TechnicalAnalyzerAgent\n",
    "   - DomainOntologyAgent\n",
    "   - PlainLanguageAgent\n",
    "   - DocumentationAssemblerAgent\n",
    "5. **Orchestrator** - Agent workflow management\n",
    "6. **Context Manager** - Working vs long-term memory with compaction\n",
    "\n",
    "### Key Workflows:\n",
    "- Data ingestion and parsing\n",
    "- Technical analysis with clarification requests\n",
    "- Ontology mapping (OMOP, LOINC, SNOMED)\n",
    "- Human-readable documentation generation\n",
    "- Human-in-the-loop review and approval\n",
    "- Context compaction for large sessions\n",
    "\n",
    "### Next Steps:\n",
    "To use this in Kaggle:\n",
    "1. Add your Google Gemini API key as a Kaggle secret named 'GOOGLE_API_KEY'\n",
    "2. Upload your data dictionary files\n",
    "3. Run the notebook cells in order\n",
    "4. Review and approve generated documentation\n",
    "5. Export the final documentation\n",
    "\n",
    "### Extending the System:\n",
    "- Add custom agents for domain-specific processing\n",
    "- Create new Toon types for your use case\n",
    "- Implement additional ontology mappings\n",
    "- Build a web UI using Streamlit or Gradio\n",
    "- Add version control for documentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}