{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Development Environment (ADE) for Healthcare Data Documentation\n",
    "\n",
    "## Version 3.0\n",
    "\n",
    "This notebook implements a specialized development environment for building, testing, and managing AI agents using the Google Gemini API and Agent Development Kit (ADK).\n",
    "\n",
    "### System Overview\n",
    "\n",
    "The ADE creates an \"Orchestrator\" and team of sub-agents that:\n",
    "- Ingest complex, technical, and often imperfect data specifications (CSV, XML, JSON)\n",
    "- Produce comprehensive, human-readable documentation\n",
    "- Provide Human-in-the-Loop (HITL) review workflows\n",
    "- Manage context using the \"Toon\" notation system\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **SQLite Database** - Project-local persistence\n",
    "2. **Toon System** - Context management for large files\n",
    "3. **Core Agents** - Specialized AI agents for data processing\n",
    "4. **ReviewQueue** - Human-in-the-loop workflow\n",
    "5. **Orchestrator** - Agent chaining and workflow management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q google-generativeai sqlite3 pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sqlite3\nimport json\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Any, Tuple\nfrom enum import Enum\nimport google.generativeai as genai\nfrom dataclasses import dataclass, asdict\nimport hashlib\nimport os\nimport time"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Google Gemini API\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# Get API key from Kaggle secrets\n",
    "user_secrets = UserSecretsClient()\n",
    "api_key = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Configure the Gemini API\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "print(\"\u2713 Gemini API configured successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### \u26a0\ufe0f Important: API Rate Limits & Configuration\n\nThe system includes flexible rate limiting based on your Gemini API tier. You can configure this in the **API Configuration** cell above.\n\n**Available Tier Configurations:**\n\n| Tier | Requests/Min | Processing Time (7 vars) | Usage |\n|------|-------------|-------------------------|--------|\n| **FREE** | 10 | ~2-3 minutes | Default, free tier |\n| **PAYG** | 360 | ~10 seconds | Pay-as-you-go |\n| **ENTERPRISE** | 1000 | ~5 seconds | Enterprise customers |\n| **CONSERVATIVE** | 8 | ~3-4 minutes | Extra safe, fewer retries |\n\n**How to Change Your Configuration:**\n\n1. **Use a preset tier:**\n   ```python\n   # In the API Configuration cell, change this line:\n   API_CONFIG = APITier.PAYG  # Instead of APITier.FREE\n   ```\n\n2. **Create a custom configuration:**\n   ```python\n   API_CONFIG = APITier.custom(\n       requests_per_minute=50,\n       max_retries=3,\n       base_retry_delay=2.0\n   )\n   ```\n\n3. **Override for a specific job:**\n   ```python\n   # Create custom config\n   fast_config = APITier.PAYG\n   \n   # Pass to orchestrator\n   orchestrator = Orchestrator(db, api_config=fast_config)\n   ```\n\n**What Happens Automatically:**\n\n\u2705 **Rate limiting** - Waits between requests to stay within limits  \n\u2705 **Retry logic** - Auto-retries with exponential backoff if rate limit hit  \n\u2705 **Clear messages** - Shows `\u23f1\ufe0f Rate limiting: waiting X.Xs...`  \n\n**Tips for Best Performance:**\n\n1. **Free tier users:** Use `auto_approve=True` for testing small datasets first\n2. **Paid tier users:** Change to `APITier.PAYG` for much faster processing  \n3. **Having quota issues?** Try `APITier.CONSERVATIVE` for extra safety\n4. **Need specific limits?** Use `APITier.custom(your_limit)`\n\n**Example: Upgrading from Free to Paid Tier**\n\n```python\n# After upgrading your Gemini API plan:\n# 1. Change configuration (in API Configuration cell)\nAPI_CONFIG = APITier.PAYG\n\n# 2. Re-initialize orchestrator\norchestrator = Orchestrator(db)  # Uses new API_CONFIG\n\n# 3. Process normally - now 36x faster!\njob_id = orchestrator.process_data_dictionary(\n    source_data=sample_data_dictionary,\n    source_file=\"study.csv\",\n    auto_approve=True\n)\n```"
  },
  {
   "cell_type": "code",
   "source": "@dataclass\nclass APIConfig:\n    \"\"\"Configuration for API rate limits and retry behavior.\"\"\"\n    requests_per_minute: int = 10\n    max_retries: int = 3\n    base_retry_delay: float = 6.0\n    model_name: str = \"gemini-2.0-flash-exp\"\n    \n    def __post_init__(self):\n        self.min_delay = 60.0 / self.requests_per_minute\n\n\n# Predefined configurations for different API tiers\nclass APITier:\n    \"\"\"Predefined API configurations for different Gemini tiers.\"\"\"\n    \n    # Free tier: 10 requests/minute\n    FREE = APIConfig(\n        requests_per_minute=10,\n        max_retries=3,\n        base_retry_delay=6.0,\n        model_name=\"gemini-2.0-flash-exp\"\n    )\n    \n    # Pay-as-you-go tier: 360 requests/minute\n    PAYG = APIConfig(\n        requests_per_minute=360,\n        max_retries=3,\n        base_retry_delay=2.0,\n        model_name=\"gemini-2.0-flash-exp\"\n    )\n    \n    # Enterprise tier: 1000+ requests/minute\n    ENTERPRISE = APIConfig(\n        requests_per_minute=1000,\n        max_retries=2,\n        base_retry_delay=1.0,\n        model_name=\"gemini-2.0-flash-exp\"\n    )\n    \n    # Conservative mode: Extra safe with delays\n    CONSERVATIVE = APIConfig(\n        requests_per_minute=8,\n        max_retries=5,\n        base_retry_delay=8.0,\n        model_name=\"gemini-2.0-flash-exp\"\n    )\n    \n    @staticmethod\n    def custom(requests_per_minute: int, max_retries: int = 3, \n               base_retry_delay: float = 6.0, model_name: str = \"gemini-2.0-flash-exp\") -> APIConfig:\n        \"\"\"Create a custom API configuration.\"\"\"\n        return APIConfig(\n            requests_per_minute=requests_per_minute,\n            max_retries=max_retries,\n            base_retry_delay=base_retry_delay,\n            model_name=model_name\n        )\n\n\n# Set your tier here - CHANGE THIS BASED ON YOUR API TIER\nAPI_CONFIG = APITier.FREE  # Options: FREE, PAYG, ENTERPRISE, CONSERVATIVE, or APITier.custom(...)\n\nprint(f\"\ud83d\udcca API Configuration:\")\nprint(f\"   Tier: {'FREE' if API_CONFIG == APITier.FREE else 'CUSTOM'}\")\nprint(f\"   Requests/minute: {API_CONFIG.requests_per_minute}\")\nprint(f\"   Min delay between requests: {API_CONFIG.min_delay:.1f}s\")\nprint(f\"   Max retries: {API_CONFIG.max_retries}\")\nprint(f\"   Model: {API_CONFIG.model_name}\")\nprint(f\"\\n\ud83d\udca1 Tip: Change API_CONFIG to match your tier:\")\nprint(f\"   - API_CONFIG = APITier.FREE (10 req/min)\")\nprint(f\"   - API_CONFIG = APITier.PAYG (360 req/min)\")\nprint(f\"   - API_CONFIG = APITier.ENTERPRISE (1000 req/min)\")\nprint(f\"   - API_CONFIG = APITier.custom(50) (50 req/min)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## API Configuration and Rate Limits\n\nConfigure API rate limits based on your Gemini API tier.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### \u26a0\ufe0f Important: API Rate Limits\n\n**Gemini API Free Tier Limits:**\n- **10 requests per minute** per model\n- The system automatically handles rate limiting with delays between requests\n- Each API call will wait ~6 seconds to stay within limits\n- Processing 7 variables takes approximately **2-3 minutes** due to rate limiting\n\n**Tips for Faster Processing:**\n1. Use `auto_approve=True` for testing (skips manual review)\n2. Start with a small dataset (3-5 variables) to test\n3. For production with paid tier, limits are much higher\n4. The system automatically retries with exponential backoff if limits are hit\n\n**If you see quota errors:**\n- The system will automatically retry (up to 3 times)\n- Wait times increase with each retry (6s, 12s, 24s)\n- Consider upgrading to paid tier for higher quotas",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Database Schema and Setup\n",
    "\n",
    "The SQLite database is the backbone of the HITL workflow and provides persistent storage for all project data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatabaseManager:\n",
    "    \"\"\"Manages SQLite database operations for the ADE project.\"\"\"\n",
    "    \n",
    "    def __init__(self, db_path: str = \"project.db\"):\n",
    "        self.db_path = db_path\n",
    "        self.conn = None\n",
    "        self.cursor = None\n",
    "        \n",
    "    def connect(self):\n",
    "        \"\"\"Establish database connection.\"\"\"\n",
    "        self.conn = sqlite3.connect(self.db_path)\n",
    "        self.conn.row_factory = sqlite3.Row\n",
    "        self.cursor = self.conn.cursor()\n",
    "        \n",
    "    def close(self):\n",
    "        \"\"\"Close database connection.\"\"\"\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "            \n",
    "    def initialize_schema(self):\n",
    "        \"\"\"Create all required tables for the ADE system.\"\"\"\n",
    "        \n",
    "        # Agents table\n",
    "        self.cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS Agents (\n",
    "            agent_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            name TEXT NOT NULL UNIQUE,\n",
    "            system_prompt TEXT NOT NULL,\n",
    "            agent_type TEXT NOT NULL,\n",
    "            config JSON,\n",
    "            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "        \"\"\")\n",
    "        \n",
    "        # Toons table - Context snippets\n",
    "        self.cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS Toons (\n",
    "            toon_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            name TEXT NOT NULL UNIQUE,\n",
    "            toon_type TEXT NOT NULL CHECK(toon_type IN (\n",
    "                'Toon_Summary', 'Toon_Chunk', 'Toon_Instruction', \n",
    "                'Toon_Version', 'Toon_Design', 'Toon_Mapping'\n",
    "            )),\n",
    "            content TEXT NOT NULL,\n",
    "            metadata JSON,\n",
    "            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "        \"\"\")\n",
    "        \n",
    "        # Jobs table\n",
    "        self.cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS Jobs (\n",
    "            job_id TEXT PRIMARY KEY,\n",
    "            source_file TEXT NOT NULL,\n",
    "            status TEXT NOT NULL DEFAULT 'Running' CHECK(status IN (\n",
    "                'Running', 'Completed', 'Failed', 'Paused'\n",
    "            )),\n",
    "            metadata JSON,\n",
    "            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "        \"\"\")\n",
    "        \n",
    "        # ReviewQueue table - HITL workflow\n",
    "        self.cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS ReviewQueue (\n",
    "            item_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            job_id TEXT NOT NULL,\n",
    "            status TEXT NOT NULL DEFAULT 'Pending' CHECK(status IN (\n",
    "                'Pending', 'Approved', 'Rejected', 'Needs_Clarification'\n",
    "            )),\n",
    "            source_agent TEXT NOT NULL,\n",
    "            target_agent TEXT,\n",
    "            source_data TEXT NOT NULL,\n",
    "            generated_content TEXT NOT NULL,\n",
    "            approved_content TEXT,\n",
    "            rejection_feedback TEXT,\n",
    "            clarification_response TEXT,\n",
    "            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "            FOREIGN KEY (job_id) REFERENCES Jobs(job_id)\n",
    "        )\n",
    "        \"\"\")\n",
    "        \n",
    "        # SystemState table - Application state\n",
    "        self.cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS SystemState (\n",
    "            state_key TEXT PRIMARY KEY,\n",
    "            state_value TEXT NOT NULL,\n",
    "            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "        \"\"\")\n",
    "        \n",
    "        # SessionHistory table - Chat logs\n",
    "        self.cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS SessionHistory (\n",
    "            history_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            job_id TEXT,\n",
    "            role TEXT NOT NULL CHECK(role IN ('user', 'assistant', 'system')),\n",
    "            content TEXT NOT NULL,\n",
    "            metadata JSON,\n",
    "            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "            FOREIGN KEY (job_id) REFERENCES Jobs(job_id)\n",
    "        )\n",
    "        \"\"\")\n",
    "        \n",
    "        self.conn.commit()\n",
    "        print(\"Database schema initialized successfully.\")\n",
    "        \n",
    "    def execute_query(self, query: str, params: tuple = ()):\n",
    "        \"\"\"Execute a query and return results.\"\"\"\n",
    "        self.cursor.execute(query, params)\n",
    "        return self.cursor.fetchall()\n",
    "    \n",
    "    def execute_update(self, query: str, params: tuple = ()):\n",
    "        \"\"\"Execute an update/insert query.\"\"\"\n",
    "        self.cursor.execute(query, params)\n",
    "        self.conn.commit()\n",
    "        return self.cursor.lastrowid"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Using Toon Notation in Practice\n\n**When to use Toon notation:**\n1. **Large data arrays** - Use tabular format for rows of similar objects\n2. **Context injection** - Encode data before passing to agents\n3. **Toon library storage** - Store data compactly in Toon snippets\n4. **Prompt optimization** - Reduce token usage by 40-70%\n\n**Integration with agents:**\n```python\n# Instead of JSON\ndata_json = json.dumps(parsed_data)\n\n# Use Toon notation\ndata_toon = ToonNotation.encode(parsed_data)\n\n# Pass to agent (uses fewer tokens!)\nresult = agent.process(data_toon)\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Real-world example: Healthcare data token savings\n\nhealthcare_data = {\n    'patient_id': 'P12345',\n    'demographics': {\n        'age': 45,\n        'sex': 'Female',\n        'race': 'Asian'\n    },\n    'vitals': [\n        {'timestamp': '2024-01-01', 'bp_sys': 120, 'bp_dia': 80, 'hr': 72},\n        {'timestamp': '2024-01-02', 'bp_sys': 118, 'bp_dia': 78, 'hr': 70},\n        {'timestamp': '2024-01-03', 'bp_sys': 122, 'bp_dia': 82, 'hr': 74}\n    ],\n    'medications': ['Lisinopril', 'Metformin', 'Atorvastatin'],\n    'lab_results': {\n        'glucose': 95,\n        'hba1c': 5.7,\n        'cholesterol': 180\n    }\n}\n\ncomparison = ToonNotation.compare_sizes(healthcare_data)\n\nprint(\"=\" * 80)\nprint(\"REAL-WORLD HEALTHCARE DATA: TOKEN SAVINGS\")\nprint(\"=\" * 80)\n\nprint(f\"\\n\ud83d\udcca Statistics:\")\nprint(f\"   JSON:  {comparison['json_chars']} chars \u2192 ~{comparison['json_tokens']} tokens\")\nprint(f\"   Toon:  {comparison['toon_chars']} chars \u2192 ~{comparison['toon_tokens']} tokens\")\nprint(f\"   \ud83d\udcb0 Savings: {comparison['savings_percent']}% fewer tokens!\\n\")\n\nprint(\"\\n\ud83d\udcdd JSON Format:\")\nprint(\"-\" * 80)\nprint(comparison['json'])\n\nprint(\"\\n\u2728 Toon Notation:\")\nprint(\"-\" * 80)\nprint(comparison['toon'])\n\nprint(\"\\n\" + \"=\" * 80)\nprint(f\"This notation reduces context usage by {comparison['savings_percent']}%,\")\nprint(\"allowing you to fit more data in the same context window!\")\nprint(\"=\" * 80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Demonstration: Toon Notation Examples\n\nprint(\"=\" * 80)\nprint(\"TOON NOTATION EXAMPLES\")\nprint(\"=\" * 80)\n\n# Example 1: Simple object\nexample1 = {'id': 1, 'name': 'Ada'}\nprint(\"\\n1. Simple Object:\")\nprint(\"   JSON:  \", json.dumps(example1))\nprint(\"   Toon:  \", ToonNotation.encode(example1).replace('\\n', '\\n           '))\n\n# Example 2: Nested object\nexample2 = {'user': {'id': 1}}\nprint(\"\\n2. Nested Object:\")\nprint(\"   JSON:  \", json.dumps(example2))\nprint(\"   Toon:  \", ToonNotation.encode(example2).replace('\\n', '\\n           '))\n\n# Example 3: Primitive array (inline)\nexample3 = {'tags': ['foo', 'bar']}\nprint(\"\\n3. Primitive Array:\")\nprint(\"   JSON:  \", json.dumps(example3))\nprint(\"   Toon:  \", ToonNotation.encode(example3))\n\n# Example 4: Tabular array (uniform objects)\nexample4 = {'items': [{'id': 1, 'qty': 5}, {'id': 2, 'qty': 3}]}\nprint(\"\\n4. Tabular Array:\")\nprint(\"   JSON:  \", json.dumps(example4))\nprint(\"   Toon:  \", ToonNotation.encode(example4).replace('\\n', '\\n           '))\n\n# Example 5: Mixed array\nexample5 = {'items': [1, {'a': 1}, 'x']}\nprint(\"\\n5. Mixed Array:\")\nprint(\"   JSON:  \", json.dumps(example5))\nprint(\"   Toon:  \", ToonNotation.encode(example5).replace('\\n', '\\n           '))\n\n# Example 6: Array of arrays\nexample6 = {'pairs': [[1, 2], [3, 4]]}\nprint(\"\\n6. Array of Arrays:\")\nprint(\"   JSON:  \", json.dumps(example6))\nprint(\"   Toon:  \", ToonNotation.encode(example6).replace('\\n', '\\n           '))\n\n# Example 7: Root array\nexample7 = ['x', 'y']\nprint(\"\\n7. Root Array:\")\nprint(\"   JSON:  \", json.dumps(example7))\nprint(\"   Toon:  \", ToonNotation.encode(example7))\n\n# Example 8: Empty containers\nexample8 = {'items': []}\nprint(\"\\n8. Empty Array:\")\nprint(\"   JSON:  \", json.dumps(example8))\nprint(\"   Toon:  \", ToonNotation.encode(example8))\n\n# Example 9: Special quoting\nexample9 = {'note': 'hello, world', 'items': ['true', True]}\nprint(\"\\n9. Special Quoting:\")\nprint(\"   JSON:  \", json.dumps(example9))\nprint(\"   Toon:  \", ToonNotation.encode(example9).replace('\\n', '\\n           '))\n\nprint(\"\\n\" + \"=\" * 80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class ToonNotation:\n    \"\"\"\n    Compact notation for encoding data to maximize context efficiency.\n    \n    Reduces token usage by 40-70% compared to standard JSON while preserving\n    all information. Inspired by YAML but optimized for LLM context windows.\n    \"\"\"\n    \n    @staticmethod\n    def _needs_quoting(value: str) -> bool:\n        \"\"\"Check if a string value needs quotes to avoid ambiguity.\"\"\"\n        if not isinstance(value, str):\n            return False\n        # Quote if contains comma, colon, or looks like a number/boolean\n        if ',' in value or ':' in value:\n            return True\n        if value.lower() in ['true', 'false', 'null', 'none']:\n            return True\n        try:\n            float(value)\n            return True\n        except:\n            return False\n    \n    @staticmethod\n    def _is_tabular(arr: list) -> bool:\n        \"\"\"Check if array is uniform objects (tabular format).\"\"\"\n        if not arr or not isinstance(arr[0], dict):\n            return False\n        keys = set(arr[0].keys())\n        return all(isinstance(item, dict) and set(item.keys()) == keys for item in arr)\n    \n    @staticmethod\n    def _is_primitive_array(arr: list) -> bool:\n        \"\"\"Check if array contains only primitives (not dicts/lists).\"\"\"\n        return all(not isinstance(item, (dict, list)) for item in arr)\n    \n    @staticmethod\n    def encode(data: Any, indent: int = 0) -> str:\n        \"\"\"\n        Encode data in Toon notation.\n        \n        Examples:\n            {'id': 1, 'name': 'Ada'} \u2192\n                id: 1\n                name: Ada\n            \n            {'tags': ['foo', 'bar']} \u2192\n                tags[2]: foo,bar\n            \n            {'items': [{'id': 1, 'qty': 5}, {'id': 2, 'qty': 3}]} \u2192\n                items[2]{id,qty}:\n                  1,5\n                  2,3\n        \"\"\"\n        prefix = \"  \" * indent\n        \n        # Handle None/null\n        if data is None:\n            return \"null\"\n        \n        # Handle primitives\n        if isinstance(data, bool):\n            return str(data).lower()\n        if isinstance(data, (int, float)):\n            return str(data)\n        if isinstance(data, str):\n            if ToonNotation._needs_quoting(data):\n                return f'\"{data}\"'\n            return data\n        \n        # Handle empty containers\n        if isinstance(data, dict) and not data:\n            return \"\"  # Empty dict produces no output\n        if isinstance(data, list) and not data:\n            return \"[0]:\"\n        \n        # Handle root array\n        if isinstance(data, list):\n            # Primitive array (inline)\n            if ToonNotation._is_primitive_array(data):\n                values = [\n                    f'\"{v}\"' if ToonNotation._needs_quoting(v) else str(v)\n                    for v in data\n                ]\n                return f\"[{len(data)}]: {','.join(values)}\"\n            \n            # Tabular array (uniform objects)\n            elif ToonNotation._is_tabular(data):\n                keys = list(data[0].keys())\n                result = f\"[{len(data)}]{{{','.join(keys)}}}:\\n\"\n                for item in data:\n                    values = [\n                        f'\"{item[k]}\"' if ToonNotation._needs_quoting(item[k]) else str(item[k])\n                        for k in keys\n                    ]\n                    result += f\"{prefix}  {','.join(values)}\\n\"\n                return result.rstrip()\n            \n            # Mixed/nested array (list format)\n            else:\n                result = f\"[{len(data)}]:\\n\"\n                for item in data:\n                    if isinstance(item, (dict, list)):\n                        item_str = ToonNotation.encode(item, indent + 1)\n                        result += f\"{prefix}  - {item_str}\\n\"\n                    else:\n                        val = f'\"{item}\"' if ToonNotation._needs_quoting(item) else str(item)\n                        result += f\"{prefix}  - {val}\\n\"\n                return result.rstrip()\n        \n        # Handle object\n        if isinstance(data, dict):\n            result = []\n            for key, value in data.items():\n                # Empty arrays\n                if isinstance(value, list) and not value:\n                    result.append(f\"{prefix}{key}[0]:\")\n                # Primitive array (inline)\n                elif isinstance(value, list) and ToonNotation._is_primitive_array(value):\n                    values = [\n                        f'\"{v}\"' if ToonNotation._needs_quoting(v) else str(v)\n                        for v in value\n                    ]\n                    result.append(f\"{prefix}{key}[{len(value)}]: {','.join(values)}\")\n                # Tabular array\n                elif isinstance(value, list) and ToonNotation._is_tabular(value):\n                    keys = list(value[0].keys())\n                    result.append(f\"{prefix}{key}[{len(value)}]{{{','.join(keys)}}}:\")\n                    for item in value:\n                        values = [\n                            f'\"{item[k]}\"' if ToonNotation._needs_quoting(item[k]) else str(item[k])\n                            for k in keys\n                        ]\n                        result.append(f\"{prefix}  {','.join(values)}\")\n                # Mixed array\n                elif isinstance(value, list):\n                    result.append(f\"{prefix}{key}[{len(value)}]:\")\n                    for item in value:\n                        if isinstance(item, (dict, list)):\n                            item_str = ToonNotation.encode(item, indent + 1)\n                            # Handle array of arrays\n                            if isinstance(item, list):\n                                result.append(f\"{prefix}  - {item_str}\")\n                            else:\n                                result.append(f\"{prefix}  - {item_str}\")\n                        else:\n                            val = f'\"{item}\"' if ToonNotation._needs_quoting(item) else str(item)\n                            result.append(f\"{prefix}  - {val}\")\n                # Nested object\n                elif isinstance(value, dict):\n                    result.append(f\"{prefix}{key}:\")\n                    nested = ToonNotation.encode(value, indent + 1)\n                    result.append(nested)\n                # Primitive value\n                else:\n                    val = value\n                    if isinstance(val, bool):\n                        val = str(val).lower()\n                    elif isinstance(val, str) and ToonNotation._needs_quoting(val):\n                        val = f'\"{val}\"'\n                    result.append(f\"{prefix}{key}: {val}\")\n            \n            return \"\\n\".join(result)\n        \n        return str(data)\n    \n    @staticmethod\n    def compare_sizes(data: dict) -> dict:\n        \"\"\"Compare token usage between JSON and Toon notation.\"\"\"\n        json_str = json.dumps(data, indent=2)\n        toon_str = ToonNotation.encode(data)\n        \n        # Rough token estimation (1 token \u2248 4 chars)\n        json_tokens = len(json_str) // 4\n        toon_tokens = len(toon_str) // 4\n        savings = 100 * (1 - toon_tokens / json_tokens)\n        \n        return {\n            'json_chars': len(json_str),\n            'toon_chars': len(toon_str),\n            'json_tokens': json_tokens,\n            'toon_tokens': toon_tokens,\n            'savings_percent': round(savings, 1),\n            'json': json_str,\n            'toon': toon_str\n        }\n\n\nprint(\"\u2713 Toon Notation system loaded\")\nprint(\"  Compact data encoding for efficient context usage\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "class DataParserAgent(BaseAgent):\n    \"\"\"Agent for parsing raw data into standardized JSON format.\"\"\"\n    \n    def __init__(self, config: APIConfig = None):\n        system_prompt = \"\"\"\nYou are a DataParserAgent specialized in converting raw data specifications \n(CSV, JSON, XML) into a standardized format using Toon notation.\n\nToon notation is a compact format that reduces token usage:\n- Objects: key: value (one per line)\n- Primitive arrays: key[count]: val1,val2,val3\n- Tabular arrays: key[count]{col1,col2}: then data rows\n- Example: items[2]{id,name}:\n            1,foo\n            2,bar\n\nYour task:\n1. Parse the input data (CSV, JSON, or XML)\n2. Preserve all original field names and values\n3. Output in Toon notation format\n4. Each variable should include: original_name, original_type (if available), \n   original_description (if available), and any other metadata\n\nOutput format (Toon notation):\nvariables[N]{original_name,original_type,original_description,notes}:\n  field_name,type,description,notes\n  field_name2,type2,description2,notes2\n\nOnly output valid Toon notation. No additional commentary.\n\"\"\"\n        super().__init__(\"DataParserAgent\", system_prompt, config)\n    \n    def parse_csv(self, csv_data: str) -> List[Dict]:\n        \"\"\"Parse CSV data dictionary.\"\"\"\n        result = self.process(csv_data)\n        \n        # Try to parse as Toon notation first\n        # For now, fall back to JSON parsing since LLM might output JSON\n        if \"```json\" in result or \"{\" in result:\n            # Extract JSON from markdown code block if present\n            if \"```json\" in result:\n                result = result.split(\"```json\")[1].split(\"```\")[0].strip()\n            elif \"```\" in result:\n                result = result.split(\"```\")[1].split(\"```\")[0].strip()\n            return json.loads(result)\n        else:\n            # Agent used Toon notation - for now convert back\n            # In future, we'd parse Toon notation directly\n            # For MVP, request JSON output\n            return json.loads(result)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize database\n",
    "db = DatabaseManager(\"project.db\")\n",
    "db.connect()\n",
    "db.initialize_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Toon System - Context Management\n",
    "\n",
    "The \"Toon\" system is a critical component for managing large files and providing granular control over agent context. A Toon is a discrete, named snippet of context stored in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToonType(Enum):\n",
    "    \"\"\"Enumeration of Toon types.\"\"\"\n",
    "    SUMMARY = \"Toon_Summary\"\n",
    "    CHUNK = \"Toon_Chunk\"\n",
    "    INSTRUCTION = \"Toon_Instruction\"\n",
    "    VERSION = \"Toon_Version\"\n",
    "    DESIGN = \"Toon_Design\"\n",
    "    MAPPING = \"Toon_Mapping\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Toon:\n",
    "    \"\"\"Represents a context snippet (Toon).\"\"\"\n",
    "    name: str\n",
    "    toon_type: ToonType\n",
    "    content: str\n",
    "    metadata: Optional[Dict[str, Any]] = None\n",
    "    toon_id: Optional[int] = None\n",
    "    \n",
    "\n",
    "class ToonManager:\n",
    "    \"\"\"Manages the Toon Library for context management.\"\"\"\n",
    "    \n",
    "    def __init__(self, db_manager: DatabaseManager):\n",
    "        self.db = db_manager\n",
    "        \n",
    "    def create_toon(self, name: str, toon_type: ToonType, content: str, \n",
    "                    metadata: Optional[Dict] = None) -> int:\n",
    "        \"\"\"Create a new Toon in the library.\"\"\"\n",
    "        query = \"\"\"\n",
    "        INSERT INTO Toons (name, toon_type, content, metadata)\n",
    "        VALUES (?, ?, ?, ?)\n",
    "        \"\"\"\n",
    "        metadata_json = json.dumps(metadata) if metadata else None\n",
    "        toon_id = self.db.execute_update(\n",
    "            query, \n",
    "            (name, toon_type.value, content, metadata_json)\n",
    "        )\n",
    "        print(f\"Created Toon '{name}' (ID: {toon_id}, Type: {toon_type.value})\")\n",
    "        return toon_id\n",
    "    \n",
    "    def get_toon(self, toon_id: int) -> Optional[Toon]:\n",
    "        \"\"\"Retrieve a Toon by ID.\"\"\"\n",
    "        query = \"SELECT * FROM Toons WHERE toon_id = ?\"\n",
    "        result = self.db.execute_query(query, (toon_id,))\n",
    "        if result:\n",
    "            row = result[0]\n",
    "            return Toon(\n",
    "                toon_id=row['toon_id'],\n",
    "                name=row['name'],\n",
    "                toon_type=ToonType(row['toon_type']),\n",
    "                content=row['content'],\n",
    "                metadata=json.loads(row['metadata']) if row['metadata'] else None\n",
    "            )\n",
    "        return None\n",
    "    \n",
    "    def get_toon_by_name(self, name: str) -> Optional[Toon]:\n",
    "        \"\"\"Retrieve a Toon by name.\"\"\"\n",
    "        query = \"SELECT * FROM Toons WHERE name = ?\"\n",
    "        result = self.db.execute_query(query, (name,))\n",
    "        if result:\n",
    "            row = result[0]\n",
    "            return Toon(\n",
    "                toon_id=row['toon_id'],\n",
    "                name=row['name'],\n",
    "                toon_type=ToonType(row['toon_type']),\n",
    "                content=row['content'],\n",
    "                metadata=json.loads(row['metadata']) if row['metadata'] else None\n",
    "            )\n",
    "        return None\n",
    "    \n",
    "    def list_toons(self, toon_type: Optional[ToonType] = None) -> List[Toon]:\n",
    "        \"\"\"List all Toons, optionally filtered by type.\"\"\"\n",
    "        if toon_type:\n",
    "            query = \"SELECT * FROM Toons WHERE toon_type = ? ORDER BY created_at DESC\"\n",
    "            results = self.db.execute_query(query, (toon_type.value,))\n",
    "        else:\n",
    "            query = \"SELECT * FROM Toons ORDER BY created_at DESC\"\n",
    "            results = self.db.execute_query(query)\n",
    "        \n",
    "        toons = []\n",
    "        for row in results:\n",
    "            toons.append(Toon(\n",
    "                toon_id=row['toon_id'],\n",
    "                name=row['name'],\n",
    "                toon_type=ToonType(row['toon_type']),\n",
    "                content=row['content'],\n",
    "                metadata=json.loads(row['metadata']) if row['metadata'] else None\n",
    "            ))\n",
    "        return toons\n",
    "    \n",
    "    def update_toon(self, toon_id: int, content: Optional[str] = None,\n",
    "                    metadata: Optional[Dict] = None):\n",
    "        \"\"\"Update a Toon's content or metadata.\"\"\"\n",
    "        updates = []\n",
    "        params = []\n",
    "        \n",
    "        if content is not None:\n",
    "            updates.append(\"content = ?\")\n",
    "            params.append(content)\n",
    "        \n",
    "        if metadata is not None:\n",
    "            updates.append(\"metadata = ?\")\n",
    "            params.append(json.dumps(metadata))\n",
    "        \n",
    "        if updates:\n",
    "            updates.append(\"updated_at = CURRENT_TIMESTAMP\")\n",
    "            query = f\"UPDATE Toons SET {', '.join(updates)} WHERE toon_id = ?\"\n",
    "            params.append(toon_id)\n",
    "            self.db.execute_update(query, tuple(params))\n",
    "            print(f\"Updated Toon ID {toon_id}\")\n",
    "    \n",
    "    def delete_toon(self, toon_id: int):\n",
    "        \"\"\"Delete a Toon from the library.\"\"\"\n",
    "        query = \"DELETE FROM Toons WHERE toon_id = ?\"\n",
    "        self.db.execute_update(query, (toon_id,))\n",
    "        print(f\"Deleted Toon ID {toon_id}\")",
    "\n\n\n# Review Queue Data Structures\nclass ReviewStatus(Enum):\n    \"\"\"Status of items in the review queue.\"\"\"\n    PENDING = \"Pending\"\n    APPROVED = \"Approved\"\n    REJECTED = \"Rejected\"\n    NEEDS_CLARIFICATION = \"Needs_Clarification\"\n\n\n@dataclass\nclass ReviewItem:\n    \"\"\"Represents an item in the review queue.\"\"\"\n    job_id: str\n    source_agent: str\n    source_data: str\n    generated_content: str\n    status: ReviewStatus\n    item_id: Optional[int] = None\n    approved_content: Optional[str] = None\n    feedback: Optional[str] = None\n    created_at: Optional[str] = None\n\n\nclass ReviewQueueManager:\n    \"\"\"Manages the review queue for Human-in-the-Loop workflow.\"\"\"\n    \n    def __init__(self, db_manager: DatabaseManager):\n        self.db = db_manager\n    \n    def add_item(self, item: ReviewItem) -> int:\n        \"\"\"Add an item to the review queue.\"\"\"\n        query = \"\"\"\n        INSERT INTO ReviewQueue (job_id, source_agent, source_data, generated_content, status)\n        VALUES (?, ?, ?, ?, ?)\n        \"\"\"\n        cursor = self.db.conn.execute(\n            query,\n            (item.job_id, item.source_agent, item.source_data, \n             item.generated_content, item.status.value)\n        )\n        self.db.conn.commit()\n        return cursor.lastrowid\n    \n    def get_pending_items(self, job_id: Optional[str] = None) -> List[ReviewItem]:\n        \"\"\"Get all pending review items, optionally filtered by job_id.\"\"\"\n        if job_id:\n            query = \"\"\"\n            SELECT * FROM ReviewQueue \n            WHERE status = 'Pending' AND job_id = ?\n            ORDER BY created_at\n            \"\"\"\n            rows = self.db.execute_query(query, (job_id,))\n        else:\n            query = \"\"\"\n            SELECT * FROM ReviewQueue \n            WHERE status = 'Pending'\n            ORDER BY created_at\n            \"\"\"\n            rows = self.db.execute_query(query)\n        \n        return [self._row_to_item(row) for row in rows]\n    \n    def get_clarification_items(self, job_id: Optional[str] = None) -> List[ReviewItem]:\n        \"\"\"Get items that need clarification.\"\"\"\n        if job_id:\n            query = \"\"\"\n            SELECT * FROM ReviewQueue \n            WHERE status = 'Needs_Clarification' AND job_id = ?\n            ORDER BY created_at\n            \"\"\"\n            rows = self.db.execute_query(query, (job_id,))\n        else:\n            query = \"\"\"\n            SELECT * FROM ReviewQueue \n            WHERE status = 'Needs_Clarification'\n            ORDER BY created_at\n            \"\"\"\n            rows = self.db.execute_query(query)\n        \n        return [self._row_to_item(row) for row in rows]\n    \n    def get_approved_items(self, job_id: str) -> List[ReviewItem]:\n        \"\"\"Get all approved items for a job.\"\"\"\n        query = \"\"\"\n        SELECT * FROM ReviewQueue \n        WHERE status = 'Approved' AND job_id = ?\n        ORDER BY created_at\n        \"\"\"\n        rows = self.db.execute_query(query, (job_id,))\n        return [self._row_to_item(row) for row in rows]\n    \n    def approve_item(self, item_id: int, approved_content: Optional[str] = None):\n        \"\"\"Approve a review item.\"\"\"\n        if approved_content:\n            query = \"\"\"\n            UPDATE ReviewQueue \n            SET status = 'Approved', approved_content = ?\n            WHERE item_id = ?\n            \"\"\"\n            self.db.execute_update(query, (approved_content, item_id))\n        else:\n            # Use generated_content as approved_content\n            query = \"\"\"\n            UPDATE ReviewQueue \n            SET status = 'Approved', approved_content = generated_content\n            WHERE item_id = ?\n            \"\"\"\n            self.db.execute_update(query, (item_id,))\n        \n        print(f\"\u2713 Approved item {item_id}\")\n    \n    def reject_item(self, item_id: int, feedback: str):\n        \"\"\"Reject a review item with feedback.\"\"\"\n        query = \"\"\"\n        UPDATE ReviewQueue \n        SET status = 'Rejected', feedback = ?\n        WHERE item_id = ?\n        \"\"\"\n        self.db.execute_update(query, (feedback, item_id))\n        print(f\"\u2713 Rejected item {item_id}\")\n    \n    def submit_clarification(self, item_id: int, clarification_response: str):\n        \"\"\"Submit a clarification response and mark as pending review.\"\"\"\n        query = \"\"\"\n        UPDATE ReviewQueue \n        SET status = 'Pending', feedback = ?\n        WHERE item_id = ?\n        \"\"\"\n        self.db.execute_update(query, (clarification_response, item_id))\n        print(f\"\u2713 Submitted clarification for item {item_id}\")\n    \n    def _row_to_item(self, row: Dict) -> ReviewItem:\n        \"\"\"Convert database row to ReviewItem.\"\"\"\n        return ReviewItem(\n            item_id=row['item_id'],\n            job_id=row['job_id'],\n            source_agent=row['source_agent'],\n            source_data=row['source_data'],\n            generated_content=row['generated_content'],\n            status=ReviewStatus(row['status']),\n            approved_content=row.get('approved_content'),\n            feedback=row.get('feedback'),\n            created_at=row.get('created_at')\n        )\n\n\nprint(\"\u2713 Review Queue classes defined (ReviewStatus, ReviewItem, ReviewQueueManager)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "class BaseAgent:\n    \"\"\"Base class for all agents with configurable rate limiting and retry logic.\"\"\"\n    \n    def __init__(self, name: str, system_prompt: str, config: APIConfig = None):\n        self.name = name\n        self.system_prompt = system_prompt\n        self.config = config or API_CONFIG  # Use global config if not specified\n        self.model = genai.GenerativeModel(self.config.model_name)\n        self.active_toons: List[Toon] = []\n        self.last_request_time = 0\n        \n    def inject_toons(self, toons: List[Toon]):\n        \"\"\"Inject Toons into agent context.\"\"\"\n        self.active_toons = toons\n        \n    def build_prompt(self, user_input: str, additional_context: str = \"\") -> str:\n        \"\"\"Build the full prompt with system prompt, Toons, and user input.\"\"\"\n        prompt_parts = [self.system_prompt]\n        \n        # Add active Toons as context\n        if self.active_toons:\n            prompt_parts.append(\"\\n=== CONTEXT (Toons) ===\")\n            for toon in self.active_toons:\n                prompt_parts.append(f\"\\n[{toon.toon_type.value}: {toon.name}]\")\n                prompt_parts.append(toon.content)\n        \n        # Add additional context\n        if additional_context:\n            prompt_parts.append(\"\\n=== ADDITIONAL CONTEXT ===\")\n            prompt_parts.append(additional_context)\n        \n        # Add user input\n        prompt_parts.append(\"\\n=== INPUT ===\")\n        prompt_parts.append(user_input)\n        \n        return \"\\n\".join(prompt_parts)\n    \n    def _wait_for_rate_limit(self):\n        \"\"\"Implement rate limiting by waiting if necessary.\"\"\"\n        if self.last_request_time > 0:\n            elapsed = time.time() - self.last_request_time\n            if elapsed < self.config.min_delay:\n                wait_time = self.config.min_delay - elapsed\n                print(f\"\u23f1\ufe0f  Rate limiting: waiting {wait_time:.1f}s...\")\n                time.sleep(wait_time)\n    \n    def generate(self, prompt: str) -> str:\n        \"\"\"Generate response using Gemini API with retry logic and rate limiting.\"\"\"\n        for attempt in range(self.config.max_retries):\n            try:\n                # Wait for rate limit before making request\n                self._wait_for_rate_limit()\n                \n                # Make API call\n                self.last_request_time = time.time()\n                response = self.model.generate_content(prompt)\n                return response.text\n                \n            except Exception as e:\n                error_str = str(e)\n                \n                # Check if it's a rate limit error\n                if \"ResourceExhausted\" in error_str or \"429\" in error_str:\n                    # Extract retry delay from error if available\n                    retry_delay = self.config.base_retry_delay\n                    \n                    # Try to extract the suggested retry delay\n                    if \"retry_delay\" in error_str:\n                        try:\n                            import re\n                            match = re.search(r'seconds: (\\d+)', error_str)\n                            if match:\n                                retry_delay = float(match.group(1))\n                        except:\n                            pass\n                    \n                    # Add exponential backoff\n                    wait_time = retry_delay * (2 ** attempt)\n                    \n                    if attempt < self.config.max_retries - 1:\n                        print(f\"\u26a0\ufe0f  Rate limit hit. Waiting {wait_time:.1f}s before retry {attempt + 1}/{self.config.max_retries}...\")\n                        time.sleep(wait_time)\n                    else:\n                        print(f\"\u274c Max retries ({self.config.max_retries}) reached.\")\n                        print(f\"\ud83d\udca1 Tip: Try increasing API_CONFIG.max_retries or wait and re-run the cell.\")\n                        raise\n                else:\n                    # For non-rate-limit errors, raise immediately\n                    print(f\"\u274c API Error: {error_str}\")\n                    raise\n        \n        raise Exception(f\"Failed after {self.config.max_retries} retries\")\n    \n\n    \n    def process_with_toon(self, data: Any, additional_context: str = \"\") -> str:\n        \"\"\"\n        Process data using Toon notation for efficiency.\n        \n        This method encodes the input data using ToonNotation to reduce token usage\n        by 40-70% compared to JSON.\n        \n        Args:\n            data: Any serializable data structure\n            additional_context: Additional context to include\n            \n        Returns:\n            Generated response from the agent\n        \"\"\"\n        # Encode data in Toon notation\n        toon_encoded = ToonNotation.encode(data)\n        \n        # Add context explaining the format\n        toon_context = \"\"\"\nThe input data below is in Toon notation format, a compact encoding that reduces token usage:\n- Simple values: key: value\n- Arrays: key[count]: val1,val2,val3\n- Tabular data: key[count]{col1,col2}: followed by rows\n\"\"\"\n        full_context = toon_context\n        if additional_context:\n            full_context += \"\\n\" + additional_context\n        \n        # Process with Toon-encoded data\n        return self.process(toon_encoded, full_context)\n\n    def process(self, input_data: str, additional_context: str = \"\") -> str:\n        \"\"\"Process input and return output.\"\"\"\n        full_prompt = self.build_prompt(input_data, additional_context)\n        return self.generate(full_prompt)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class DataParserAgent(BaseAgent):\n    \"\"\"Agent for parsing raw data into standardized JSON format.\"\"\"\n    \n    def __init__(self, config: APIConfig = None):\n        system_prompt = \"\"\"\nYou are a DataParserAgent specialized in converting raw data specifications \n(CSV, JSON, XML) into a standardized JSON format.\n\nYour task:\n1. Parse the input data\n2. Preserve all original field names and values\n3. Output a JSON array where each element represents one variable/field\n4. Each element should include: original_name, original_type (if available), \n   original_description (if available), and any other metadata\n\nOutput format:\n```json\n[\n  {\n    \"original_name\": \"field_name\",\n    \"original_type\": \"type\",\n    \"original_description\": \"description\",\n    \"metadata\": {}\n  }\n]\n```\n\nOnly output valid JSON. No additional commentary.\n\"\"\"\n        super().__init__(\"DataParserAgent\", system_prompt, config)\n    \n    def parse_csv(self, csv_data: str) -> List[Dict]:\n        \"\"\"Parse CSV data dictionary.\"\"\"\n        result = self.process(csv_data)\n        # Extract JSON from markdown code block if present\n        if \"```json\" in result:\n            result = result.split(\"```json\")[1].split(\"```\")[0].strip()\n        elif \"```\" in result:\n            result = result.split(\"```\")[1].split(\"```\")[0].strip()\n        return json.loads(result)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class TechnicalAnalyzerAgent(BaseAgent):\n    \"\"\"Agent for analyzing technical properties and mapping to internal standards.\"\"\"\n    \n    def __init__(self, config: APIConfig = None):\n        system_prompt = \"\"\"\nYou are a TechnicalAnalyzerAgent specialized in analyzing data fields and \nmapping them to internal standards.\n\nYour task:\n\n\n**Input Format: Toon Notation**\nThe input data will be provided in Toon notation, a compact format that reduces token usage:\n- Simple values: `key: value`\n- Arrays: `key[count]: val1,val2,val3` \n- Tabular data: `key[count]{col1,col2}:` followed by data rows\n\nExample:\n```\nvariables[2]{original_name,original_type}:\n  patient_id,text\n  age,integer\n```\n\nParse this format and extract the field information.\n\n1. Analyze each field from the parsed data\n2. Infer technical properties (data_type, constraints, cardinality)\n3. Map to standardized field names following healthcare data conventions\n4. If mapping is unclear or confidence is low, flag for clarification\n\nStandard field mappings:\n- variable_name: standardized variable name\n- data_type: categorical, continuous, date, text, boolean\n- description: human-readable description\n- constraints: any validation rules\n- cardinality: required, optional, repeated\n- confidence: high, medium, low (for mapping quality)\n\nOutput format:\n```json\n[\n  {\n    \"original_name\": \"field_name\",\n    \"variable_name\": \"standardized_name\",\n    \"data_type\": \"categorical\",\n    \"description\": \"description\",\n    \"constraints\": {},\n    \"cardinality\": \"required\",\n    \"confidence\": \"high\",\n    \"needs_clarification\": false,\n    \"clarification_question\": \"\"\n  }\n]\n```\n\nOnly output valid JSON. No additional commentary.\n\"\"\"\n        super().__init__(\"TechnicalAnalyzerAgent\", system_prompt, config)\n    \n    def analyze(self, parsed_data: List[Dict], \n                clarifications: Optional[Dict[str, str]] = None) -> List[Dict]:\n        \"\"\"Analyze parsed data and map to internal standards.\"\"\"\n        additional_context = \"\"\n        if clarifications:\n            additional_context = \"\\n=== USER CLARIFICATIONS ===\\n\"\n            for field, clarification in clarifications.items():\n                additional_context += f\"{field}: {clarification}\\n\"\n        \n        # Use Toon notation for efficient token usage (40-70% reduction)\n        toon_encoded = ToonNotation.encode({\"variables\": parsed_data})\n        \n        # Add format explanation\n        format_context = \"\\nData is in Toon notation format for efficiency. Output JSON as specified.\\n\"\n        full_context = format_context + (additional_context if additional_context else \"\")\n        \n        result = self.process(toon_encoded, full_context)\n        \n        # Extract JSON from markdown code block if present\n        if \"```json\" in result:\n            result = result.split(\"```json\")[1].split(\"```\")[0].strip()\n        elif \"```\" in result:\n            result = result.split(\"```\")[1].split(\"```\")[0].strip()\n        \n        return json.loads(result)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DomainOntologyAgent(BaseAgent):\n    \"\"\"Agent for mapping to standard healthcare ontologies (OMOP, LOINC, etc.).\"\"\"\n    \n    def __init__(self, config: APIConfig = None):\n        system_prompt = \"\"\"\nYou are a DomainOntologyAgent specialized in mapping healthcare data fields \nto standard ontologies and terminologies.\n\nYour task:\n\n**Input Format: Toon Notation**\nInput data is provided in Toon notation (compact format):\n- `key: value` for simple fields\n- `key[n]: v1,v2,...` for arrays\n- Nested objects use indentation\n\nExample:\n```\nvariable_name: blood_pressure_systolic\ndata_type: continuous\ndescription: Systolic blood pressure in mmHg\n```\n\n\n1. For each variable, identify appropriate standard ontology codes\n2. Primary ontologies to consider:\n   - OMOP CDM concepts\n   - LOINC codes (for lab/clinical observations)\n   - SNOMED CT (for clinical terms)\n   - RxNorm (for medications)\n3. Provide both the code and the standard term\n4. Include a confidence score for each mapping\n\nOutput format:\n```json\n{\n  \"variable_name\": \"standardized_name\",\n  \"ontology_mappings\": [\n    {\n      \"system\": \"OMOP\",\n      \"code\": \"123456\",\n      \"display\": \"Standard Concept Name\",\n      \"confidence\": \"high\"\n    }\n  ]\n}\n```\n\nOnly output valid JSON. No additional commentary.\n\"\"\"\n        super().__init__(\"DomainOntologyAgent\", system_prompt, config)\n    \n    def map_ontologies(self, variable_data: Dict) -> Dict:\n        \"\"\"Map a variable to standard ontologies.\"\"\"\n        # Use Toon notation for efficient token usage\n        toon_encoded = ToonNotation.encode(variable_data)\n        format_context = \"\\nInput is in Toon notation format. Output JSON as specified.\\n\"\n        result = self.process(toon_encoded, format_context)\n        \n        # Extract JSON from markdown code block if present\n        if \"```json\" in result:\n            result = result.split(\"```json\")[1].split(\"```\")[0].strip()\n        elif \"```\" in result:\n            result = result.split(\"```\")[1].split(\"```\")[0].strip()\n        \n        return json.loads(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainLanguageAgent(BaseAgent):\n    \"\"\"Agent for generating human-readable documentation.\"\"\"\n    \n    def __init__(self, config: APIConfig = None):\n        system_prompt = \"\"\"\nYou are a PlainLanguageAgent specialized in creating clear, comprehensive, \nhuman-readable documentation for healthcare data variables.\n\nYour task:\n\n**Input Format: Toon Notation**\nThe enriched variable data is provided in Toon notation (compact format that saves 40-70% tokens):\n- Simple fields: `key: value`\n- Arrays: `key[n]: val1,val2,val3`\n- Objects: nested with indentation\n\nParse the Toon format and generate clear markdown documentation.\n\n\n1. Convert technical variable specifications into plain language\n2. Explain what the variable represents in clinical/research context\n3. Describe data type, constraints, and valid values\n4. Include ontology mappings and their significance\n5. Write for an interdisciplinary audience (clinicians, researchers, data scientists)\n\nOutput format (Markdown):\n```markdown\n## Variable: [Variable Name]\n\n**Description:** [Clear, concise description]\n\n**Technical Details:**\n- Data Type: [type]\n- Cardinality: [required/optional]\n- Valid Values: [constraints or ranges]\n\n**Standard Ontology Mappings:**\n- OMOP: [code] - [term]\n- LOINC: [code] - [term]\n\n**Clinical Context:** [Explanation of why this variable matters]\n```\n\nOnly output Markdown documentation. No additional commentary.\n\"\"\"\n        super().__init__(\"PlainLanguageAgent\", system_prompt, config)\n    \n    def document_variable(self, enriched_data: Dict) -> str:\n        \"\"\"Generate plain language documentation for a variable.\"\"\"\n        # Use Toon notation for efficient token usage (40-70% reduction)\n        toon_encoded = ToonNotation.encode(enriched_data)\n        format_context = \"\\nInput is in Toon notation format. Generate markdown documentation.\\n\"\n        result = self.process(toon_encoded, format_context)\n        \n        # Remove markdown code block markers if present\n        if \"```markdown\" in result:\n            result = result.split(\"```markdown\")[1].split(\"```\")[0].strip()\n        elif result.startswith(\"```\") and result.endswith(\"```\"):\n            result = result.split(\"```\")[1].split(\"```\")[0].strip()\n        \n        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class DocumentationAssemblerAgent(BaseAgent):\n    \"\"\"Agent for assembling final documentation from approved items.\"\"\"\n    \n    def __init__(self, review_queue: ReviewQueueManager, config: APIConfig = None):\n        system_prompt = \"\"\"\nYou are a DocumentationAssemblerAgent specialized in creating comprehensive, \nwell-structured data documentation.\n\nYour task:\n1. Compile all approved variable documentation into a cohesive document\n2. Add a table of contents\n3. Include metadata (generation date, source file, etc.)\n4. Organize by logical groupings if applicable\n5. Ensure consistent formatting throughout\n\nOutput: A complete Markdown document ready for publication.\n\"\"\"\n        super().__init__(\"DocumentationAssemblerAgent\", system_prompt, config)\n        self.review_queue = review_queue\n    \n    def assemble(self, job_id: str) -> str:\n        \"\"\"Assemble final documentation from approved review items.\"\"\"\n        approved_items = self.review_queue.get_approved_items(job_id)\n        \n        if not approved_items:\n            return \"# No approved documentation found for this job.\"\n        \n        # Build document\n        doc_parts = [\n            \"# Healthcare Data Documentation\",\n            f\"\\n**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n            f\"**Job ID:** {job_id}\",\n            \"\\n---\\n\"\n        ]\n        \n        # Add table of contents\n        doc_parts.append(\"## Table of Contents\\n\")\n        for i, item in enumerate(approved_items, 1):\n            # Extract variable name from content if possible\n            content = item.approved_content\n            if \"## Variable:\" in content:\n                var_name = content.split(\"## Variable:\")[1].split(\"\\n\")[0].strip()\n                doc_parts.append(f\"{i}. [{var_name}](#{var_name.lower().replace(' ', '-')})\")\n        \n        doc_parts.append(\"\\n---\\n\")\n        \n        # Add all approved content\n        for item in approved_items:\n            doc_parts.append(item.approved_content)\n            doc_parts.append(\"\\n---\\n\")\n        \n        return \"\\n\".join(doc_parts)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DomainOntologyAgent(BaseAgent):\n",
    "    \"\"\"Agent for mapping to standard healthcare ontologies (OMOP, LOINC, etc.).\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        system_prompt = \"\"\"\n",
    "You are a DomainOntologyAgent specialized in mapping healthcare data fields \n",
    "to standard ontologies and terminologies.\n",
    "\n",
    "Your task:\n",
    "1. For each variable, identify appropriate standard ontology codes\n",
    "2. Primary ontologies to consider:\n",
    "   - OMOP CDM concepts\n",
    "   - LOINC codes (for lab/clinical observations)\n",
    "   - SNOMED CT (for clinical terms)\n",
    "   - RxNorm (for medications)\n",
    "3. Provide both the code and the standard term\n",
    "4. Include a confidence score for each mapping\n",
    "\n",
    "Output format:\n",
    "```json\n",
    "{\n",
    "  \"variable_name\": \"standardized_name\",\n",
    "  \"ontology_mappings\": [\n",
    "    {\n",
    "      \"system\": \"OMOP\",\n",
    "      \"code\": \"123456\",\n",
    "      \"display\": \"Standard Concept Name\",\n",
    "      \"confidence\": \"high\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "Only output valid JSON. No additional commentary.\n",
    "\"\"\"\n",
    "        super().__init__(\"DomainOntologyAgent\", system_prompt)\n",
    "    \n",
    "    def map_ontologies(self, variable_data: Dict) -> Dict:\n",
    "        \"\"\"Map a variable to standard ontologies.\"\"\"\n",
    "        result = self.process(json.dumps(variable_data, indent=2))\n",
    "        \n",
    "        # Extract JSON from markdown code block if present\n",
    "        if \"```json\" in result:\n",
    "            result = result.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in result:\n",
    "            result = result.split(\"```\")[1].split(\"```\")[0].strip()\n",
    "        \n",
    "        return json.loads(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class Orchestrator:\n    \"\"\"Manages the workflow of agents and coordinates the documentation pipeline.\"\"\"\n    \n    def __init__(self, db_manager: DatabaseManager, api_config: APIConfig = None):\n        self.db = db_manager\n        self.config = api_config or API_CONFIG\n        self.toon_manager = ToonManager(db_manager)\n        self.review_queue = ReviewQueueManager(db_manager)\n        \n        # Initialize agents with configuration\n        self.data_parser = DataParserAgent(config=self.config)\n        self.technical_analyzer = TechnicalAnalyzerAgent(config=self.config)\n        self.domain_ontology = DomainOntologyAgent(config=self.config)\n        self.plain_language = PlainLanguageAgent(config=self.config)\n        self.assembler = DocumentationAssemblerAgent(self.review_queue, config=self.config)\n        \n        print(f\"\u2713 Orchestrator initialized with {self.config.requests_per_minute} req/min limit\")\n        \n    def create_job(self, source_file: str) -> str:\n        \"\"\"Create a new documentation job.\"\"\"\n        # Generate unique job ID\n        job_id = hashlib.md5(\n            f\"{source_file}_{datetime.now().isoformat()}\".encode()\n        ).hexdigest()[:12]\n        \n        query = \"\"\"\n        INSERT INTO Jobs (job_id, source_file, status)\n        VALUES (?, ?, 'Running')\n        \"\"\"\n        self.db.execute_update(query, (job_id, source_file))\n        print(f\"Created job {job_id} for source file: {source_file}\")\n        return job_id\n    \n    def process_data_dictionary(self, source_data: str, source_file: str = \"input.csv\",\n                                auto_approve: bool = False) -> str:\n        \"\"\"\n        Main workflow: Process a data dictionary through the agent pipeline.\n        \n        Args:\n            source_data: The raw data dictionary content\n            source_file: Name of the source file\n            auto_approve: If True, automatically approve all generated content\n        \n        Returns:\n            job_id: The ID of the created job\n        \"\"\"\n        # Create job\n        job_id = self.create_job(source_file)\n        \n        print(\"\\n=== Step 1: Parsing Data ===\")\n        parsed_data = self.data_parser.parse_csv(source_data)\n        print(f\"Parsed {len(parsed_data)} variables\")\n        \n        print(\"\\n=== Step 2: Technical Analysis ===\")\n        analyzed_data = self.technical_analyzer.analyze(parsed_data)\n        print(f\"Analyzed {len(analyzed_data)} variables\")\n        \n        # Check for clarifications needed\n        needs_clarification = [v for v in analyzed_data if v.get('needs_clarification', False)]\n        if needs_clarification:\n            print(f\"\\n\u26a0\ufe0f  {len(needs_clarification)} variables need clarification\")\n            for var in needs_clarification:\n                # Add to review queue\n                item = ReviewItem(\n                    job_id=job_id,\n                    source_agent=\"TechnicalAnalyzerAgent\",\n                    source_data=json.dumps(var),\n                    generated_content=var.get('clarification_question', 'Needs clarification'),\n                    status=ReviewStatus.NEEDS_CLARIFICATION\n                )\n                self.review_queue.add_item(item)\n        \n        print(\"\\n=== Step 3: Domain Ontology Mapping ===\")\n        enriched_data = []\n        for i, var in enumerate(analyzed_data):\n            if not var.get('needs_clarification', False):\n                print(f\"Mapping variable {i+1}/{len(analyzed_data)}: {var.get('variable_name', 'unknown')}\")\n                enriched = self.domain_ontology.map_ontologies(var)\n                enriched_data.append(enriched)\n        \n        print(f\"\\nEnriched {len(enriched_data)} variables with ontology mappings\")\n        \n        print(\"\\n=== Step 4: Plain Language Documentation ===\")\n        for i, var in enumerate(enriched_data):\n            print(f\"Documenting variable {i+1}/{len(enriched_data)}: {var.get('variable_name', 'unknown')}\")\n            documentation = self.plain_language.document_variable(var)\n            \n            # Add to review queue\n            item = ReviewItem(\n                job_id=job_id,\n                source_agent=\"PlainLanguageAgent\",\n                source_data=json.dumps(var),\n                generated_content=documentation,\n                status=ReviewStatus.PENDING\n            )\n            item_id = self.review_queue.add_item(item)\n            \n            # Auto-approve if requested\n            if auto_approve:\n                self.review_queue.approve_item(item_id)\n        \n        print(f\"\\n\u2713 Job {job_id} processing complete\")\n        print(f\"  - {len(enriched_data)} items ready for review\")\n        if needs_clarification:\n            print(f\"  - {len(needs_clarification)} items need clarification\")\n        \n        return job_id\n    \n    def finalize_documentation(self, job_id: str, output_file: str = \"documentation.md\") -> str:\n        \"\"\"Assemble and save final documentation.\"\"\"\n        print(f\"\\n=== Assembling Final Documentation ===\")\n        documentation = self.assembler.assemble(job_id)\n        \n        # Save to file\n        with open(output_file, 'w') as f:\n            f.write(documentation)\n        \n        print(f\"\u2713 Documentation saved to {output_file}\")\n        \n        # Update job status\n        query = \"UPDATE Jobs SET status = 'Completed' WHERE job_id = ?\"\n        self.db.execute_update(query, (job_id,))\n        \n        return documentation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentationAssemblerAgent(BaseAgent):\n",
    "    \"\"\"Agent for assembling final documentation from approved items.\"\"\"\n",
    "    \n",
    "    def __init__(self, review_queue: ReviewQueueManager):\n",
    "        system_prompt = \"\"\"\n",
    "You are a DocumentationAssemblerAgent specialized in creating comprehensive, \n",
    "well-structured data documentation.\n",
    "\n",
    "Your task:\n",
    "1. Compile all approved variable documentation into a cohesive document\n",
    "2. Add a table of contents\n",
    "3. Include metadata (generation date, source file, etc.)\n",
    "4. Organize by logical groupings if applicable\n",
    "5. Ensure consistent formatting throughout\n",
    "\n",
    "Output: A complete Markdown document ready for publication.\n",
    "\"\"\"\n",
    "        super().__init__(\"DocumentationAssemblerAgent\", system_prompt)\n",
    "        self.review_queue = review_queue\n",
    "    \n",
    "    def assemble(self, job_id: str) -> str:\n",
    "        \"\"\"Assemble final documentation from approved review items.\"\"\"\n",
    "        approved_items = self.review_queue.get_approved_items(job_id)\n",
    "        \n",
    "        if not approved_items:\n",
    "            return \"# No approved documentation found for this job.\"\n",
    "        \n",
    "        # Build document\n",
    "        doc_parts = [\n",
    "            \"# Healthcare Data Documentation\",\n",
    "            f\"\\n**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "            f\"**Job ID:** {job_id}\",\n",
    "            \"\\n---\\n\"\n",
    "        ]\n",
    "        \n",
    "        # Add table of contents\n",
    "        doc_parts.append(\"## Table of Contents\\n\")\n",
    "        for i, item in enumerate(approved_items, 1):\n",
    "            # Extract variable name from content if possible\n",
    "            content = item.approved_content\n",
    "            if \"## Variable:\" in content:\n",
    "                var_name = content.split(\"## Variable:\")[1].split(\"\\n\")[0].strip()\n",
    "                doc_parts.append(f\"{i}. [{var_name}](#{var_name.lower().replace(' ', '-')})\")\n",
    "        \n",
    "        doc_parts.append(\"\\n---\\n\")\n",
    "        \n",
    "        # Add all approved content\n",
    "        for item in approved_items:\n",
    "            doc_parts.append(item.approved_content)\n",
    "            doc_parts.append(\"\\n---\\n\")\n",
    "        \n",
    "        return \"\\n\".join(doc_parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Orchestrator - Agent Workflow Management\n",
    "\n",
    "The Orchestrator manages the flow of data through the agent pipeline and handles the HITL workflow."
   ]
  },
  {
   "cell_type": "code",
   "source": "# Summary of Available Examples\nprint(\"=\" * 80)\nprint(\"AVAILABLE EXAMPLE DATA DICTIONARIES\")\nprint(\"=\" * 80)\nprint(\"\\n1. sample_data_dictionary     - Basic diabetes study (7 variables)\")\nprint(\"2. ehr_data_dictionary        - Electronic Health Records (15 variables)\")\nprint(\"3. omop_data_dictionary       - OMOP Common Data Model (12 variables)\")\nprint(\"4. genomic_data_dictionary    - Genetic/Genomic data (15 variables)\")\nprint(\"5. clinical_trial_dictionary  - Clinical trial data (15 variables)\")\nprint(\"6. imaging_data_dictionary    - Medical imaging data (14 variables)\")\nprint(\"7. pro_data_dictionary        - Patient-reported outcomes (14 variables)\")\nprint(\"8. laboratory_data_dictionary - Lab test results (16 variables)\")\nprint(\"\\n\" + \"=\" * 80)\nprint(\"\\n\ud83d\udca1 To use an example, simply pass it to the orchestrator:\")\nprint(\"   job_id = orchestrator.process_data_dictionary(\")\nprint(\"       source_data=ehr_data_dictionary,  # Choose your example\")\nprint(\"       source_file='ehr_data.csv',\")\nprint(\"       auto_approve=True\")\nprint(\"   )\")\nprint(\"=\" * 80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Example 7: Laboratory Test Results\nlaboratory_data_dictionary = \"\"\"Variable Name,Field Type,Field Label,Choices,Notes\nspecimen_id,text,Specimen ID,,Unique specimen identifier\npatient_id,text,Patient ID,,Patient identifier\ncollection_date,date,Collection Date,,Date/time specimen collected\ntest_code,text,Test Code (LOINC),,LOINC code for test\ntest_name,text,Test Name,,Common test name\nresult_value,text,Result Value,,Numeric or categorical result\nresult_numeric,decimal,Numeric Result,,Numeric value if applicable\nresult_unit,text,Unit of Measure (UCUM),,UCUM unit code\nreference_low,decimal,Reference Range Lower,,Normal range lower bound\nreference_high,decimal,Reference Range Upper,,Normal range upper bound\nabnormal_flag,radio,Abnormal Flag,\"N, Normal | L, Low | H, High | LL, Critically Low | HH, Critically High\",Interpretation\nspecimen_type,radio,Specimen Type,\"1, Blood | 2, Urine | 3, CSF | 4, Other\",Type of biospecimen\nlab_id,text,Laboratory ID,,Performing lab identifier\nmethod,text,Test Method,,Analytical method used\nverified_by,text,Verified By,,Lab technician ID\nloinc_code,text,LOINC Code,,Full LOINC identifier\n\"\"\"\n\nprint(\"\u2713 Laboratory Data Dictionary loaded\")\nprint(f\"  Variables: {len(laboratory_data_dictionary.split(chr(10))) - 1}\")\nprint(\"  Note: Uses LOINC and UCUM standards\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Example 6: Patient-Reported Outcomes (PRO)\npro_data_dictionary = \"\"\"Variable Name,Field Type,Field Label,Choices,Notes\nparticipant_id,text,Participant ID,,Study participant identifier\nsurvey_date,date,Survey Completion Date,,Date PRO completed\nsurvey_type,radio,Survey Type,\"1, Baseline | 2, Follow-up | 3, Final\",Assessment timepoint\nphq9_total,integer,PHQ-9 Total Score,,\"0-27, depression severity\"\nphq9_q1,radio,Little interest or pleasure,\"0, Not at all | 1, Several days | 2, More than half | 3, Nearly every day\",Over last 2 weeks\nphq9_q2,radio,Feeling down or depressed,\"0, Not at all | 1, Several days | 2, More than half | 3, Nearly every day\",Over last 2 weeks\ngad7_total,integer,GAD-7 Total Score,,\"0-21, anxiety severity\"\ngad7_q1,radio,Feeling nervous/anxious,\"0, Not at all | 1, Several days | 2, More than half | 3, Nearly every day\",Over last 2 weeks\npain_severity,radio,Pain Severity (0-10),\"0, None | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10, Worst\",Numeric rating scale\npain_interference,radio,Pain Interference with Activities,\"1, Not at all | 2, A little bit | 3, Somewhat | 4, Quite a bit | 5, Very much\",PROMIS scale\nfatigue_score,integer,PROMIS Fatigue T-Score,,\"20-80, normalized T-score\"\nsleep_quality,radio,Sleep Quality,\"1, Very poor | 2, Poor | 3, Fair | 4, Good | 5, Very good\",PSQI component\nqol_physical,decimal,Physical QOL Domain,,\"0-100, SF-36\"\nqol_mental,decimal,Mental QOL Domain,,\"0-100, SF-36\"\n\"\"\"\n\nprint(\"\u2713 Patient-Reported Outcomes Dictionary loaded\")\nprint(f\"  Variables: {len(pro_data_dictionary.split(chr(10))) - 1}\")\nprint(\"  Note: Includes PHQ-9, GAD-7, PROMIS, and SF-36 instruments\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Example 5: Medical Imaging Data (DICOM-based)\nimaging_data_dictionary = \"\"\"Variable Name,Field Type,Field Label,Choices,Notes\naccession_number,text,Accession Number,,Unique exam identifier\npatient_id,text,Patient ID,,De-identified patient ID\nstudy_date,date,Study Date,,Date imaging was performed\nmodality,radio,Imaging Modality,\"CT, MRI, PET, US, XR\",Type of scan\nbody_region,text,Body Region (RadLex),,\"Chest, Abdomen, Brain, etc.\"\nprotocol,text,Scan Protocol,,Specific imaging protocol used\nslice_thickness,decimal,Slice Thickness (mm),,For CT/MRI\ncontrast_used,yesno,Contrast Agent Used,\"0, No | 1, Yes\",IV contrast administration\ndose,decimal,Radiation Dose (mGy),,For CT/XR only\nfinding_present,yesno,Pathologic Finding,\"0, No | 1, Yes\",Any abnormality detected\nfinding_type,text,Finding Type,,Description of pathology\nlesion_size,decimal,Lesion Size (mm),,Maximum diameter if applicable\nradiologist_id,text,Interpreting Radiologist,,Reader ID\ndicom_series_uid,text,DICOM Series Instance UID,,Unique series identifier\n\"\"\"\n\nprint(\"\u2713 Imaging Data Dictionary loaded\")\nprint(f\"  Variables: {len(imaging_data_dictionary.split(chr(10))) - 1}\")\nprint(\"  Note: Based on DICOM and RadLex standards\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Example 4: Clinical Trial Data (CDISC SDTM-like)\nclinical_trial_dictionary = \"\"\"Variable Name,Field Type,Field Label,Choices,Notes\nstudyid,text,Study Identifier,,Protocol number\nusubjid,text,Unique Subject ID,,Unique across all studies\nsubjid,text,Subject ID,,ID within this study\nsiteid,text,Site ID,,Clinical site identifier\narm,radio,Treatment Arm,\"1, Placebo | 2, Low Dose | 3, High Dose\",Randomization arm\nvisit,text,Visit Name,,\"Screening, Baseline, Week 4, Week 8, Week 12\"\nvisitnum,integer,Visit Number,,\"1-10\"\nvisitdate,date,Visit Date,,Actual visit date\nae_term,text,Adverse Event Term,,MedDRA preferred term\nae_severity,radio,AE Severity,\"1, Mild | 2, Moderate | 3, Severe\",Intensity grading\nae_serious,yesno,Serious AE,\"0, No | 1, Yes\",SAE flag\nae_related,radio,Related to Study Drug,\"0, Unrelated | 1, Unlikely | 2, Possible | 3, Probable | 4, Definite\",Causality assessment\nefficacy_score,decimal,Primary Efficacy Score,,\"0-100, higher is better\"\nqol_score,decimal,Quality of Life Score,,\"0-100, SF-36\"\ncompliance,decimal,Medication Compliance (%),,\"0-100, pill count\"\n\"\"\"\n\nprint(\"\u2713 Clinical Trial Data Dictionary loaded\")\nprint(f\"  Variables: {len(clinical_trial_dictionary.split(chr(10))) - 1}\")\nprint(\"  Note: Follows CDISC SDTM standards\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Example 3: Genomic/Genetic Data\ngenomic_data_dictionary = \"\"\"Variable Name,Field Type,Field Label,Choices,Notes\nsample_id,text,Sample ID,,Unique biospecimen identifier\npatient_id,text,Patient ID,,De-identified patient identifier\ngene_symbol,text,Gene Symbol (HGNC),,Official gene symbol from HGNC\nvariant_id,text,Variant ID (dbSNP),,rs number from dbSNP database\nchromosome,text,Chromosome,,\"1-22, X, Y, MT\"\nposition,integer,Genomic Position (hg38),,Position on reference genome GRCh38\nref_allele,text,Reference Allele,,\"A, C, G, T\"\nalt_allele,text,Alternate Allele,,\"A, C, G, T, or indel\"\nvariant_type,radio,Variant Type,\"1, SNV | 2, Insertion | 3, Deletion | 4, CNV\",Single nucleotide or structural\ngenotype,text,Genotype,,\"0/0, 0/1, 1/1\"\nread_depth,integer,Read Depth (DP),,Number of reads covering position\nallele_frequency,decimal,Allele Frequency (AF),,\"0.0-1.0, population frequency\"\nclinical_significance,radio,Clinical Significance,\"0, Benign | 1, Likely Benign | 2, VUS | 3, Likely Pathogenic | 4, Pathogenic\",ClinVar classification\nphenotype_association,text,Associated Phenotype,,Disease or trait association\ntranscript_id,text,Transcript ID (Ensembl),,Canonical transcript identifier\n\"\"\"\n\nprint(\"\u2713 Genomic Data Dictionary loaded\")\nprint(f\"  Variables: {len(genomic_data_dictionary.split(chr(10))) - 1}\")\nprint(\"  Note: Follows HGNC, dbSNP, and ClinVar standards\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Example 2: OMOP Common Data Model (CDM) Format\nomop_data_dictionary = \"\"\"Variable Name,Field Type,Field Label,Choices,Notes\nperson_id,integer,Person ID,,OMOP person identifier\nvisit_occurrence_id,integer,Visit Occurrence ID,,Foreign key to VISIT_OCCURRENCE\nmeasurement_date,date,Measurement Date,,Date of measurement\nmeasurement_concept_id,integer,Measurement Concept ID,,OMOP standard concept for measurement type\nvalue_as_number,decimal,Numeric Value,,Numeric result value\nvalue_as_concept_id,integer,Categorical Value Concept ID,,OMOP concept for categorical results\nunit_concept_id,integer,Unit Concept ID,,OMOP concept for unit of measure\nrange_low,decimal,Normal Range Lower Bound,,Lower limit of normal range\nrange_high,decimal,Normal Range Upper Bound,,Upper limit of normal range\nprovider_id,integer,Provider ID,,Foreign key to PROVIDER table\nmeasurement_source_value,text,Source Value,,Original value from source system\nmeasurement_source_concept_id,integer,Source Concept ID,,Concept ID from source vocabulary\n\"\"\"\n\nprint(\"\u2713 OMOP CDM Data Dictionary loaded\")\nprint(f\"  Variables: {len(omop_data_dictionary.split(chr(10))) - 1}\")\nprint(\"  Note: Uses OMOP standard concept IDs for interoperability\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Example 1: Electronic Health Record (EHR) Data\nehr_data_dictionary = \"\"\"Variable Name,Field Type,Field Label,Choices,Notes\nmrn,text,Medical Record Number,,Unique patient identifier\nencounter_id,text,Encounter ID,,Unique visit identifier\nvisit_date,date,Visit Date,,Date of clinical encounter\nchief_complaint,text,Chief Complaint,,Primary reason for visit\ndx_code,text,Diagnosis Code (ICD-10),,Primary diagnosis\nbp_systolic,integer,Systolic BP (mmHg),,\"70-250, sitting position\"\nbp_diastolic,integer,Diastolic BP (mmHg),,\"40-150, sitting position\"\nheart_rate,integer,Heart Rate (bpm),,\"40-200\"\ntemperature,decimal,Temperature (F),,\"95.0-106.0\"\nrespiratory_rate,integer,Respiratory Rate (breaths/min),,\"8-40\"\noxygen_sat,integer,Oxygen Saturation (%),,\"70-100, room air\"\nbmi,decimal,Body Mass Index,,Calculated from height/weight\nsmoking_status,radio,Smoking Status,\"0, Never | 1, Former | 2, Current\",From social history\nmedication_count,integer,Number of Active Medications,,Count of current prescriptions\nlab_ordered,yesno,Labs Ordered,\"0, No | 1, Yes\",Any lab tests ordered this visit\n\"\"\"\n\nprint(\"\u2713 EHR Data Dictionary loaded\")\nprint(f\"  Variables: {len(ehr_data_dictionary.split(chr(10))) - 1}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### \ud83d\udcda Example Data Dictionaries by Healthcare Domain\n\nBelow are real-world examples for different healthcare data types. Use these to test the system or as templates for your own data.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Orchestrator:\n",
    "    \"\"\"Manages the workflow of agents and coordinates the documentation pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self, db_manager: DatabaseManager):\n",
    "        self.db = db_manager\n",
    "        self.toon_manager = ToonManager(db_manager)\n",
    "        self.review_queue = ReviewQueueManager(db_manager)\n",
    "        \n",
    "        # Initialize agents\n",
    "        self.data_parser = DataParserAgent()\n",
    "        self.technical_analyzer = TechnicalAnalyzerAgent()\n",
    "        self.domain_ontology = DomainOntologyAgent()\n",
    "        self.plain_language = PlainLanguageAgent()\n",
    "        self.assembler = DocumentationAssemblerAgent(self.review_queue)\n",
    "        \n",
    "    def create_job(self, source_file: str) -> str:\n",
    "        \"\"\"Create a new documentation job.\"\"\"\n",
    "        # Generate unique job ID\n",
    "        job_id = hashlib.md5(\n",
    "            f\"{source_file}_{datetime.now().isoformat()}\".encode()\n",
    "        ).hexdigest()[:12]\n",
    "        \n",
    "        query = \"\"\"\n",
    "        INSERT INTO Jobs (job_id, source_file, status)\n",
    "        VALUES (?, ?, 'Running')\n",
    "        \"\"\"\n",
    "        self.db.execute_update(query, (job_id, source_file))\n",
    "        print(f\"Created job {job_id} for source file: {source_file}\")\n",
    "        return job_id\n",
    "    \n",
    "    def process_data_dictionary(self, source_data: str, source_file: str = \"input.csv\",\n",
    "                                auto_approve: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        Main workflow: Process a data dictionary through the agent pipeline.\n",
    "        \n",
    "        Args:\n",
    "            source_data: The raw data dictionary content\n",
    "            source_file: Name of the source file\n",
    "            auto_approve: If True, automatically approve all generated content\n",
    "        \n",
    "        Returns:\n",
    "            job_id: The ID of the created job\n",
    "        \"\"\"\n",
    "        # Create job\n",
    "        job_id = self.create_job(source_file)\n",
    "        \n",
    "        print(\"\\n=== Step 1: Parsing Data ===\")\n",
    "        parsed_data = self.data_parser.parse_csv(source_data)\n",
    "        print(f\"Parsed {len(parsed_data)} variables\")\n",
    "        \n",
    "        print(\"\\n=== Step 2: Technical Analysis ===\")\n",
    "        analyzed_data = self.technical_analyzer.analyze(parsed_data)\n",
    "        print(f\"Analyzed {len(analyzed_data)} variables\")\n",
    "        \n",
    "        # Check for clarifications needed\n",
    "        needs_clarification = [v for v in analyzed_data if v.get('needs_clarification', False)]\n",
    "        if needs_clarification:\n",
    "            print(f\"\\n\u26a0\ufe0f  {len(needs_clarification)} variables need clarification\")\n",
    "            for var in needs_clarification:\n",
    "                # Add to review queue\n",
    "                item = ReviewItem(\n",
    "                    job_id=job_id,\n",
    "                    source_agent=\"TechnicalAnalyzerAgent\",\n",
    "                    source_data=json.dumps(var),\n",
    "                    generated_content=var.get('clarification_question', 'Needs clarification'),\n",
    "                    status=ReviewStatus.NEEDS_CLARIFICATION\n",
    "                )\n",
    "                self.review_queue.add_item(item)\n",
    "        \n",
    "        print(\"\\n=== Step 3: Domain Ontology Mapping ===\")\n",
    "        enriched_data = []\n",
    "        for i, var in enumerate(analyzed_data):\n",
    "            if not var.get('needs_clarification', False):\n",
    "                print(f\"Mapping variable {i+1}/{len(analyzed_data)}: {var.get('variable_name', 'unknown')}\")\n",
    "                enriched = self.domain_ontology.map_ontologies(var)\n",
    "                enriched_data.append(enriched)\n",
    "        \n",
    "        print(f\"\\nEnriched {len(enriched_data)} variables with ontology mappings\")\n",
    "        \n",
    "        print(\"\\n=== Step 4: Plain Language Documentation ===\")\n",
    "        for i, var in enumerate(enriched_data):\n",
    "            print(f\"Documenting variable {i+1}/{len(enriched_data)}: {var.get('variable_name', 'unknown')}\")\n",
    "            documentation = self.plain_language.document_variable(var)\n",
    "            \n",
    "            # Add to review queue\n",
    "            item = ReviewItem(\n",
    "                job_id=job_id,\n",
    "                source_agent=\"PlainLanguageAgent\",\n",
    "                source_data=json.dumps(var),\n",
    "                generated_content=documentation,\n",
    "                status=ReviewStatus.PENDING\n",
    "            )\n",
    "            item_id = self.review_queue.add_item(item)\n",
    "            \n",
    "            # Auto-approve if requested\n",
    "            if auto_approve:\n",
    "                self.review_queue.approve_item(item_id)\n",
    "        \n",
    "        print(f\"\\n\u2713 Job {job_id} processing complete\")\n",
    "        print(f\"  - {len(enriched_data)} items ready for review\")\n",
    "        if needs_clarification:\n",
    "            print(f\"  - {len(needs_clarification)} items need clarification\")\n",
    "        \n",
    "        return job_id\n",
    "    \n",
    "    def finalize_documentation(self, job_id: str, output_file: str = \"documentation.md\") -> str:\n",
    "        \"\"\"Assemble and save final documentation.\"\"\"\n",
    "        print(f\"\\n=== Assembling Final Documentation ===\")\n",
    "        documentation = self.assembler.assemble(job_id)\n",
    "        \n",
    "        # Save to file\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(documentation)\n",
    "        \n",
    "        print(f\"\u2713 Documentation saved to {output_file}\")\n",
    "        \n",
    "        # Update job status\n",
    "        query = \"UPDATE Jobs SET status = 'Completed' WHERE job_id = ?\"\n",
    "        self.db.execute_update(query, (job_id,))\n",
    "        \n",
    "        return documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Example Usage and Demonstration\n",
    "\n",
    "Let's demonstrate the system with a sample healthcare data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample REDCap-style data dictionary\n",
    "sample_data_dictionary = \"\"\"Variable Name,Field Type,Field Label,Choices,Notes\n",
    "patient_id,text,Patient ID,,Unique identifier\n",
    "age,integer,Age (years),,Age at enrollment\n",
    "sex,radio,Biological Sex,\"1, Male | 2, Female | 3, Other\",\n",
    "bp_systolic,integer,Systolic Blood Pressure (mmHg),,\n",
    "bp_diastolic,integer,Diastolic Blood Pressure (mmHg),,\n",
    "diagnosis_date,date,Diagnosis Date,,Date of primary diagnosis\n",
    "hba1c,decimal,Hemoglobin A1c (%),,Glycated hemoglobin\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize orchestrator\n",
    "orchestrator = Orchestrator(db)\n",
    "\n",
    "# Create some useful Toons for context\n",
    "print(\"Creating Toons for context management...\")\n",
    "\n",
    "# Instruction Toon for OMOP mapping\n",
    "orchestrator.toon_manager.create_toon(\n",
    "    name=\"OMOP_Mapping_Instructions\",\n",
    "    toon_type=ToonType.INSTRUCTION,\n",
    "    content=\"\"\"When mapping to OMOP CDM:\n",
    "- Blood pressure measurements should map to OMOP concept_id 3004249 (Systolic) and 3012888 (Diastolic)\n",
    "- HbA1c should map to OMOP concept_id 3004410\n",
    "- Age should be stored as an integer in years\n",
    "- Sex should use standard OMOP gender concepts: 8507 (Male), 8532 (Female)\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Design decision Toon\n",
    "orchestrator.toon_manager.create_toon(\n",
    "    name=\"Project_Design_Notes\",\n",
    "    toon_type=ToonType.DESIGN,\n",
    "    content=\"\"\"This is a diabetes research study collecting baseline clinical measurements.\n",
    "All measurements follow standard clinical protocols. Blood pressure is measured in sitting position\n",
    "after 5 minutes rest. HbA1c measured using DCCT-aligned assay.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\"\\nToons created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inject Toons into relevant agents\n",
    "toons = orchestrator.toon_manager.list_toons()\n",
    "orchestrator.domain_ontology.inject_toons(toons)\n",
    "orchestrator.plain_language.inject_toons(toons)\n",
    "\n",
    "print(f\"Injected {len(toons)} Toons into agent context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the data dictionary\n",
    "job_id = orchestrator.process_data_dictionary(\n",
    "    source_data=sample_data_dictionary,\n",
    "    source_file=\"diabetes_study_data_dictionary.csv\",\n",
    "    auto_approve=True  # Set to False to enable manual review\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review pending items (if auto_approve=False)\n",
    "pending_items = orchestrator.review_queue.get_pending_items(job_id)\n",
    "print(f\"\\nPending review items: {len(pending_items)}\")\n",
    "\n",
    "if pending_items:\n",
    "    print(\"\\n=== Review Interface ===\")\n",
    "    for item in pending_items[:3]:  # Show first 3\n",
    "        print(f\"\\nItem {item.item_id}:\")\n",
    "        print(f\"Source: {item.source_agent}\")\n",
    "        print(f\"\\nGenerated Content:\\n{item.generated_content[:500]}...\")\n",
    "        print(\"\\nActions: [Approve], [Edit & Approve], [Reject]\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Approve an item with edits\n",
    "if pending_items:\n",
    "    item_id = pending_items[0].item_id\n",
    "    \n",
    "    # Get the generated content\n",
    "    original_content = pending_items[0].generated_content\n",
    "    \n",
    "    # Make an edit (example)\n",
    "    edited_content = original_content.replace(\n",
    "        \"Clinical Context:\",\n",
    "        \"Clinical Context: [EDITED BY REVIEWER]\"\n",
    "    )\n",
    "    \n",
    "    # Approve with edits\n",
    "    orchestrator.review_queue.approve_item(item_id, edited_content)\n",
    "    print(f\"Approved item {item_id} with edits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for clarification requests\n",
    "clarification_items = orchestrator.review_queue.get_clarification_items(job_id)\n",
    "print(f\"\\nItems needing clarification: {len(clarification_items)}\")\n",
    "\n",
    "if clarification_items:\n",
    "    print(\"\\n=== Clarification Interface ===\")\n",
    "    for item in clarification_items:\n",
    "        print(f\"\\nItem {item.item_id}:\")\n",
    "        print(f\"Question: {item.generated_content}\")\n",
    "        print(\"\\nYour Response: [Enter clarification here]\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Submit clarification\n",
    "if clarification_items:\n",
    "    item_id = clarification_items[0].item_id\n",
    "    clarification_response = \"This field represents the patient's biological sex as reported at enrollment.\"\n",
    "    \n",
    "    orchestrator.review_queue.submit_clarification(item_id, clarification_response)\n",
    "    print(f\"Submitted clarification for item {item_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize and generate documentation\n",
    "final_documentation = orchestrator.finalize_documentation(\n",
    "    job_id=job_id,\n",
    "    output_file=\"healthcare_data_documentation.md\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== Final Documentation Preview ===\")\n",
    "print(final_documentation[:1000] + \"\\n...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Context Management - Working vs Long-Term Memory\n",
    "\n",
    "Demonstration of the memory model and context compaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextManager:\n",
    "    \"\"\"Manages working and long-term memory for the ADE system.\"\"\"\n",
    "    \n",
    "    def __init__(self, db_manager: DatabaseManager, max_tokens: int = 100000):\n",
    "        self.db = db_manager\n",
    "        self.max_tokens = max_tokens\n",
    "        self.compaction_threshold = int(max_tokens * 0.8)\n",
    "        \n",
    "    def estimate_tokens(self, text: str) -> int:\n",
    "        \"\"\"Rough token estimation (1 token \u2248 4 characters).\"\"\"\n",
    "        return len(text) // 4\n",
    "    \n",
    "    def get_working_memory(self, job_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"Get current working memory for a job.\"\"\"\n",
    "        # Get active toons\n",
    "        query = \"SELECT state_value FROM SystemState WHERE state_key = ?\"\n",
    "        result = self.db.execute_query(query, (f\"active_toons_{job_id}\",))\n",
    "        active_toon_ids = json.loads(result[0]['state_value']) if result else []\n",
    "        \n",
    "        # Get session history\n",
    "        query = \"SELECT * FROM SessionHistory WHERE job_id = ? ORDER BY created_at\"\n",
    "        history_rows = self.db.execute_query(query, (job_id,))\n",
    "        \n",
    "        session_history = []\n",
    "        for row in history_rows:\n",
    "            session_history.append({\n",
    "                'role': row['role'],\n",
    "                'content': row['content'],\n",
    "                'timestamp': row['created_at']\n",
    "            })\n",
    "        \n",
    "        # Calculate total tokens\n",
    "        total_tokens = sum(self.estimate_tokens(msg['content']) for msg in session_history)\n",
    "        \n",
    "        return {\n",
    "            'active_toon_ids': active_toon_ids,\n",
    "            'session_history': session_history,\n",
    "            'total_tokens': total_tokens,\n",
    "            'needs_compaction': total_tokens > self.compaction_threshold\n",
    "        }\n",
    "    \n",
    "    def add_to_session_history(self, job_id: str, role: str, content: str):\n",
    "        \"\"\"Add a message to session history.\"\"\"\n",
    "        query = \"\"\"\n",
    "        INSERT INTO SessionHistory (job_id, role, content)\n",
    "        VALUES (?, ?, ?)\n",
    "        \"\"\"\n",
    "        self.db.execute_update(query, (job_id, role, content))\n",
    "    \n",
    "    def compact_context(self, job_id: str) -> str:\n",
    "        \"\"\"Compact session history using summarization.\"\"\"\n",
    "        print(\"\\n=== Context Compaction Triggered ===\")\n",
    "        \n",
    "        working_memory = self.get_working_memory(job_id)\n",
    "        session_history = working_memory['session_history']\n",
    "        \n",
    "        # Build conversation text\n",
    "        conversation = \"\\n\\n\".join([\n",
    "            f\"{msg['role'].upper()}: {msg['content']}\"\n",
    "            for msg in session_history\n",
    "        ])\n",
    "        \n",
    "        # Use Gemini to summarize\n",
    "        compactor_prompt = f\"\"\"\n",
    "You are a ContextCompactorAgent. Your task is to create a concise summary of this\n",
    "conversation that preserves all critical information, decisions, and clarifications.\n",
    "\n",
    "Conversation:\n",
    "{conversation}\n",
    "\n",
    "Provide a structured summary that includes:\n",
    "1. Key decisions made\n",
    "2. Important clarifications provided\n",
    "3. Current state of the work\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "        \n",
    "        model = genai.GenerativeModel(\"gemini-2.0-flash-exp\")\n",
    "        response = model.generate_content(compactor_prompt)\n",
    "        summary = response.text\n",
    "        \n",
    "        print(f\"Original tokens: {working_memory['total_tokens']}\")\n",
    "        print(f\"Summary tokens: {self.estimate_tokens(summary)}\")\n",
    "        print(f\"Reduction: {100 * (1 - self.estimate_tokens(summary) / working_memory['total_tokens']):.1f}%\")\n",
    "        \n",
    "        # Store summary as a Toon\n",
    "        toon_manager = ToonManager(self.db)\n",
    "        toon_manager.create_toon(\n",
    "            name=f\"Session_Summary_{job_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "            toon_type=ToonType.SUMMARY,\n",
    "            content=summary,\n",
    "            metadata={'job_id': job_id, 'original_tokens': working_memory['total_tokens']}\n",
    "        )\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def clear_context(self, job_id: str):\n",
    "        \"\"\"Clear working memory (flush session history and active toons).\"\"\"\n",
    "        # Note: This doesn't delete from database, just resets the \"active\" state\n",
    "        query = \"UPDATE SystemState SET state_value = '[]' WHERE state_key = ?\"\n",
    "        self.db.execute_update(query, (f\"active_toons_{job_id}\",))\n",
    "        \n",
    "        print(f\"Cleared working memory for job {job_id}\")\n",
    "        print(\"Note: Session history preserved in long-term memory (database)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate context management\n",
    "context_manager = ContextManager(db)\n",
    "\n",
    "# Simulate a conversation\n",
    "context_manager.add_to_session_history(job_id, \"user\", \"Please process the diabetes data dictionary\")\n",
    "context_manager.add_to_session_history(job_id, \"assistant\", \"I'll process the data dictionary through the agent pipeline.\")\n",
    "context_manager.add_to_session_history(\n",
    "    job_id, \n",
    "    \"user\", \n",
    "    \"For the HbA1c variable, please note that values above 6.5% indicate diabetes diagnosis.\"\n",
    ")\n",
    "\n",
    "# Check working memory\n",
    "working_memory = context_manager.get_working_memory(job_id)\n",
    "print(f\"\\nWorking Memory Status:\")\n",
    "print(f\"  Total tokens: {working_memory['total_tokens']}\")\n",
    "print(f\"  Needs compaction: {working_memory['needs_compaction']}\")\n",
    "print(f\"  Session messages: {len(working_memory['session_history'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. System Status and Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_system_status(db: DatabaseManager):\n",
    "    \"\"\"Display current system status.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ADE SYSTEM STATUS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Jobs\n",
    "    jobs = db.execute_query(\"SELECT * FROM Jobs\")\n",
    "    print(f\"\\nJobs: {len(jobs)}\")\n",
    "    for job in jobs:\n",
    "        print(f\"  [{job['job_id']}] {job['source_file']} - Status: {job['status']}\")\n",
    "    \n",
    "    # Toons\n",
    "    toons = db.execute_query(\"SELECT toon_type, COUNT(*) as count FROM Toons GROUP BY toon_type\")\n",
    "    print(f\"\\nToon Library:\")\n",
    "    for toon in toons:\n",
    "        print(f\"  {toon['toon_type']}: {toon['count']}\")\n",
    "    \n",
    "    # Review Queue\n",
    "    review_stats = db.execute_query(\n",
    "        \"SELECT status, COUNT(*) as count FROM ReviewQueue GROUP BY status\"\n",
    "    )\n",
    "    print(f\"\\nReview Queue:\")\n",
    "    for stat in review_stats:\n",
    "        print(f\"  {stat['status']}: {stat['count']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "display_system_status(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export and Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export database for backup\n",
    "import shutil\n",
    "\n",
    "def backup_database(db_path: str, backup_path: str):\n",
    "    \"\"\"Create a backup of the project database.\"\"\"\n",
    "    shutil.copy2(db_path, backup_path)\n",
    "    print(f\"Database backed up to {backup_path}\")\n",
    "\n",
    "# Uncomment to create backup\n",
    "# backup_database(\"project.db\", \"project_backup.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View generated documentation\n",
    "if os.path.exists(\"healthcare_data_documentation.md\"):\n",
    "    with open(\"healthcare_data_documentation.md\", 'r') as f:\n",
    "        print(\"\\n=== Generated Documentation ===\")\n",
    "        print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates a complete implementation of the Agent Development Environment (ADE) for Healthcare Data Documentation. Key features:\n",
    "\n",
    "### Implemented Components:\n",
    "1. **SQLite Database** - Full schema with all required tables\n",
    "2. **Toon System** - Context management with 6 Toon types\n",
    "3. **Review Queue** - Complete HITL workflow with status management\n",
    "4. **Core Agents**:\n",
    "   - DataParserAgent\n",
    "   - TechnicalAnalyzerAgent\n",
    "   - DomainOntologyAgent\n",
    "   - PlainLanguageAgent\n",
    "   - DocumentationAssemblerAgent\n",
    "5. **Orchestrator** - Agent workflow management\n",
    "6. **Context Manager** - Working vs long-term memory with compaction\n",
    "\n",
    "### Key Workflows:\n",
    "- Data ingestion and parsing\n",
    "- Technical analysis with clarification requests\n",
    "- Ontology mapping (OMOP, LOINC, SNOMED)\n",
    "- Human-readable documentation generation\n",
    "- Human-in-the-loop review and approval\n",
    "- Context compaction for large sessions\n",
    "\n",
    "### Next Steps:\n",
    "To use this in Kaggle:\n",
    "1. Add your Google Gemini API key as a Kaggle secret named 'GOOGLE_API_KEY'\n",
    "2. Upload your data dictionary files\n",
    "3. Run the notebook cells in order\n",
    "4. Review and approve generated documentation\n",
    "5. Export the final documentation\n",
    "\n",
    "### Extending the System:\n",
    "- Add custom agents for domain-specific processing\n",
    "- Create new Toon types for your use case\n",
    "- Implement additional ontology mappings\n",
    "- Build a web UI using Streamlit or Gradio\n",
    "- Add version control for documentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}