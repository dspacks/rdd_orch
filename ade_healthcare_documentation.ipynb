{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Agent Development Environment (ADE) for Healthcare Data Documentation\n\n**Version 2.0 - November 2025**\n\nThis notebook implements a production-ready agent development environment using Google's Agent Development Kit (ADK) patterns for healthcare data documentation.\n\n## Key Features\n- **Modern ADK Architecture**: Sessions, memory services, and async patterns\n- **Toon Notation**: Compact encoding for 40-70% token reduction\n- **Snippet Manager**: Named context storage for efficient retrieval\n- **Batch Processing**: Handle large codebooks with automatic chunking\n- **Human-in-the-Loop (HITL)**: Review workflows with approval/rejection cycles\n- **Multi-Agent Orchestration**: Specialized agents for parsing, analysis, and documentation\n- **Observability**: Logging plugins and monitoring capabilities\n- **Production Deployment**: Vertex AI Agent Engine ready\n\n## Architecture Overview\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Input     \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  Orchestrator \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  Review Queue   \u2502\n\u2502   Data      \u2502     \u2502   (Runner)    \u2502     \u2502    (HITL)       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u25bc             \u25bc\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502  Agents  \u2502  \u2502  Snippet \u2502\n              \u2502          \u2502  \u2502  Manager \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages",
    "!pip install -q google-generativeai google-adk sqlite3 pandas numpy opentelemetry-instrumentation-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3",
    "import json",
    "import pandas as pd",
    "import numpy as np",
    "from datetime import datetime",
    "from typing import Dict, List, Optional, Any, Tuple",
    "from enum import Enum",
    "import google.generativeai as genai",
    "from dataclasses import dataclass, asdict, field",
    "import hashlib",
    "import os",
    "import time",
    "import asyncio",
    "import logging",
    "",
    "# Set up logging for observability",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')",
    "logger = logging.getLogger('ADE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Google Gemini API",
    "from google.colab import userdata",
    "",
    "api_key = userdata.get('GOOGLE_API_KEY')",
    "genai.configure(api_key=api_key)",
    "",
    "print(\"\u2713 Gemini API configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. API Configuration and Rate Limits",
    "",
    "Configure rate limiting based on your Gemini API tier for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass",
    "class APIConfig:",
    "    \"\"\"Configuration for API rate limits and retry behavior.\"\"\"",
    "    requests_per_minute: int = 10",
    "    max_retries: int = 3",
    "    base_retry_delay: float = 6.0",
    "    model_name: str = \"gemini-2.0-flash-exp\"",
    "    ",
    "    def __post_init__(self):",
    "        self.min_delay = 60.0 / self.requests_per_minute",
    "",
    "",
    "class APITier:",
    "    \"\"\"Predefined API configurations for different Gemini tiers.\"\"\"",
    "    ",
    "    FREE = APIConfig(requests_per_minute=10, max_retries=3, base_retry_delay=6.0)",
    "    PAYG = APIConfig(requests_per_minute=360, max_retries=3, base_retry_delay=2.0)",
    "    ENTERPRISE = APIConfig(requests_per_minute=1000, max_retries=2, base_retry_delay=1.0)",
    "    CONSERVATIVE = APIConfig(requests_per_minute=8, max_retries=5, base_retry_delay=8.0)",
    "    ",
    "    @staticmethod",
    "    def custom(requests_per_minute: int, **kwargs) -> APIConfig:",
    "        return APIConfig(requests_per_minute=requests_per_minute, **kwargs)",
    "",
    "",
    "# Set your tier here",
    "API_CONFIG = APITier.FREE",
    "",
    "print(f\"\ud83d\udcca API Configuration:\")",
    "print(f\"   Requests/minute: {API_CONFIG.requests_per_minute}\")",
    "print(f\"   Min delay: {API_CONFIG.min_delay:.1f}s\")",
    "print(f\"   Model: {API_CONFIG.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Database Schema and Setup",
    "",
    "SQLite database provides persistent storage for sessions, memory, and HITL workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class DatabaseManager:\n    \"\"\"Manages SQLite database operations with session and memory support.\"\"\"\n    \n    def __init__(self, db_path: str = \"project.db\"):\n        self.db_path = db_path\n        self.conn = None\n        self.cursor = None\n    \n    def connect(self):\n        \"\"\"Establish database connection.\"\"\"\n        self.conn = sqlite3.connect(self.db_path)\n        self.conn.row_factory = sqlite3.Row\n        self.cursor = self.conn.cursor()\n    \n    def close(self):\n        \"\"\"Close database connection.\"\"\"\n        if self.conn:\n            self.conn.close()\n    \n    def execute_query(self, query: str, params: tuple = ()) -> List[Dict]:\n        \"\"\"Execute SELECT query and return results.\"\"\"\n        self.cursor.execute(query, params)\n        rows = self.cursor.fetchall()\n        return [dict(row) for row in rows]\n    \n    def execute_update(self, query: str, params: tuple = ()) -> int:\n        \"\"\"Execute INSERT/UPDATE/DELETE and return affected row ID.\"\"\"\n        self.cursor.execute(query, params)\n        self.conn.commit()\n        return self.cursor.lastrowid\n    \n    def initialize_schema(self):\n        \"\"\"Create all required tables.\"\"\"\n        \n        # Agents table\n        self.cursor.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS Agents (\n            agent_id INTEGER PRIMARY KEY AUTOINCREMENT,\n            name TEXT NOT NULL UNIQUE,\n            system_prompt TEXT NOT NULL,\n            agent_type TEXT NOT NULL,\n            config JSON,\n            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP\n        )\n        \"\"\")\n        \n        # Snippets table - Named context storage\n        self.cursor.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS Snippets (\n            snippet_id INTEGER PRIMARY KEY AUTOINCREMENT,\n            name TEXT NOT NULL UNIQUE,\n            snippet_type TEXT NOT NULL CHECK(snippet_type IN (\n                'Summary', 'Chunk', 'Instruction',\n                'Version', 'Design', 'Mapping'\n            )),\n            content TEXT NOT NULL,\n            metadata JSON,\n            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP\n        )\n        \"\"\")\n        \n        # Jobs table with enhanced metadata\n        self.cursor.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS Jobs (\n            job_id TEXT PRIMARY KEY,\n            source_file TEXT NOT NULL,\n            status TEXT NOT NULL DEFAULT 'Running' CHECK(status IN (\n                'Running', 'Completed', 'Failed', 'Paused'\n            )),\n            metadata JSON,\n            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP\n        )\n        \"\"\")\n        \n        # ReviewQueue table - HITL workflow\n        self.cursor.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS ReviewQueue (\n            item_id INTEGER PRIMARY KEY AUTOINCREMENT,\n            job_id TEXT NOT NULL,\n            status TEXT NOT NULL DEFAULT 'Pending' CHECK(status IN (\n                'Pending', 'Approved', 'Rejected', 'Needs_Clarification'\n            )),\n            source_agent TEXT NOT NULL,\n            target_agent TEXT,\n            source_data TEXT NOT NULL,\n            generated_content TEXT NOT NULL,\n            approved_content TEXT,\n            rejection_feedback TEXT,\n            clarification_response TEXT,\n            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n            FOREIGN KEY (job_id) REFERENCES Jobs(job_id)\n        )\n        \"\"\")\n        \n        # Sessions table - ADK-style session management\n        self.cursor.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS Sessions (\n            session_id TEXT PRIMARY KEY,\n            job_id TEXT NOT NULL,\n            user_id TEXT NOT NULL,\n            state JSON DEFAULT '{}',\n            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n            FOREIGN KEY (job_id) REFERENCES Jobs(job_id)\n        )\n        \"\"\")\n        \n        # SessionHistory - Conversation history\n        self.cursor.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS SessionHistory (\n            history_id INTEGER PRIMARY KEY AUTOINCREMENT,\n            session_id TEXT NOT NULL,\n            job_id TEXT NOT NULL,\n            role TEXT NOT NULL CHECK(role IN ('user', 'assistant', 'system', 'tool')),\n            content TEXT NOT NULL,\n            metadata JSON,\n            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n            FOREIGN KEY (session_id) REFERENCES Sessions(session_id),\n            FOREIGN KEY (job_id) REFERENCES Jobs(job_id)\n        )\n        \"\"\")\n        \n        # Memory table - Long-term knowledge storage\n        self.cursor.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS Memory (\n            memory_id INTEGER PRIMARY KEY AUTOINCREMENT,\n            user_id TEXT NOT NULL,\n            content TEXT NOT NULL,\n            embedding JSON,\n            metadata JSON,\n            created_at DATETIME DEFAULT CURRENT_TIMESTAMP\n        )\n        \"\"\")\n        \n        # SystemState table\n        self.cursor.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS SystemState (\n            state_key TEXT PRIMARY KEY,\n            state_value TEXT NOT NULL,\n            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP\n        )\n        \"\"\")\n        \n        self.conn.commit()\n        print(\"\u2713 Database schema initialized with session and memory support\")\n\n# Initialize database\ndb = DatabaseManager(\"project.db\")\ndb.connect()\ndb.initialize_schema()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Toon Notation Encoding\n\nCompact data encoding that reduces token usage by 40-70% while preserving all information."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class ToonNotation:\n    \"\"\"\n    Compact notation for encoding data to maximize context efficiency.\n    Reduces token usage by 40-70% compared to standard JSON.\n    \"\"\"\n    \n    @staticmethod\n    def _needs_quoting(value: str) -> bool:\n        \"\"\"Check if a string value needs quotes to avoid ambiguity.\"\"\"\n        if not isinstance(value, str):\n            return False\n        if ',' in value or ':' in value:\n            return True\n        if value.lower() in ['true', 'false', 'null', 'none']:\n            return True\n        try:\n            float(value)\n            return True\n        except:\n            return False\n    \n    @staticmethod\n    def _is_tabular(arr: list) -> bool:\n        \"\"\"Check if array is uniform objects (tabular format).\"\"\"\n        if not arr or not isinstance(arr[0], dict):\n            return False\n        keys = set(arr[0].keys())\n        return all(isinstance(item, dict) and set(item.keys()) == keys for item in arr)\n    \n    @staticmethod\n    def encode(data: Any, indent: int = 0) -> str:\n        \"\"\"Encode data in Toon notation for token-efficient context.\"\"\"\n        prefix = \"  \" * indent\n        \n        if data is None:\n            return \"null\"\n        if isinstance(data, bool):\n            return str(data).lower()\n        if isinstance(data, (int, float)):\n            return str(data)\n        if isinstance(data, str):\n            return f'\"{data}\"' if ToonNotation._needs_quoting(data) else data\n        \n        if isinstance(data, dict) and not data:\n            return \"\"\n        if isinstance(data, list) and not data:\n            return \"[0]:\"\n        \n        if isinstance(data, list):\n            if ToonNotation._is_tabular(data):\n                keys = list(data[0].keys())\n                header = f\"[{len(data)}]{{{','.join(keys)}}}:\"\n                rows = []\n                for item in data:\n                    row_vals = [str(item[k]) if item[k] is not None else \"null\" for k in keys]\n                    rows.append(\"  \" + \",\".join(row_vals))\n                return header + \"\\n\" + \"\\n\".join(rows)\n            else:\n                items = [ToonNotation.encode(item, indent + 1) for item in data]\n                return f\"[{len(data)}]: \" + \",\".join(items)\n        \n        if isinstance(data, dict):\n            lines = []\n            for key, value in data.items():\n                if isinstance(value, dict):\n                    lines.append(f\"{prefix}{key}:\")\n                    lines.append(ToonNotation.encode(value, indent + 1))\n                elif isinstance(value, list) and ToonNotation._is_tabular(value):\n                    encoded = ToonNotation.encode(value, indent)\n                    lines.append(f\"{prefix}{key}{encoded}\")\n                else:\n                    encoded = ToonNotation.encode(value, indent)\n                    lines.append(f\"{prefix}{key}: {encoded}\")\n            return \"\\n\".join(lines)\n        \n        return str(data)\n    \n    @staticmethod\n    def decode(toon_str: str) -> Any:\n        \"\"\"Decode Toon notation back to Python objects (basic implementation).\"\"\"\n        pass\n\nprint(\"\u2713 ToonNotation encoder loaded\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class SnippetType(Enum):\n    \"\"\"Enumeration of snippet types for context management.\"\"\"\n    SUMMARY = \"Summary\"\n    CHUNK = \"Chunk\"\n    INSTRUCTION = \"Instruction\"\n    VERSION = \"Version\"\n    DESIGN = \"Design\"\n    MAPPING = \"Mapping\"\n    # Extended snippet types for new agents\n    CONVENTION = \"Convention\"        # Data naming conventions and standards\n    CHANGELOG = \"Changelog\"          # Version history and change logs\n    INSTRUMENT = \"Instrument\"        # Higher-level instrument documentation\n    SEGMENT = \"Segment\"              # Codebook segment documentation\n    GLOSSARY = \"Glossary\"            # Conventions glossary\n\n@dataclass\nclass Snippet:\n    \"\"\"Represents a named context snippet.\"\"\"\n    name: str\n    snippet_type: SnippetType\n    content: str\n    metadata: Optional[Dict[str, Any]] = None\n    snippet_id: Optional[int] = None\n\nclass SnippetManager:\n    \"\"\"Manages the Snippet Library for named context storage and retrieval.\"\"\"\n    \n    def __init__(self, db_manager: DatabaseManager):\n        self.db = db_manager\n        self._update_schema_for_new_types()\n    \n    def _update_schema_for_new_types(self):\n        \"\"\"Update database schema to support new snippet types.\"\"\"\n        # Drop and recreate with expanded types\n        try:\n            self.db.cursor.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS Snippets_New (\n                snippet_id INTEGER PRIMARY KEY AUTOINCREMENT,\n                name TEXT NOT NULL UNIQUE,\n                snippet_type TEXT NOT NULL CHECK(snippet_type IN (\n                    'Summary', 'Chunk', 'Instruction', 'Version', 'Design', 'Mapping',\n                    'Convention', 'Changelog', 'Instrument', 'Segment', 'Glossary'\n                )),\n                content TEXT NOT NULL,\n                metadata JSON,\n                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n                updated_at DATETIME DEFAULT CURRENT_TIMESTAMP\n            )\n            \"\"\")\n            \n            # Check if old table exists and migrate data\n            self.db.cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='Snippets'\")\n            if self.db.cursor.fetchone():\n                # Copy existing data\n                self.db.cursor.execute(\"\"\"\n                    INSERT OR IGNORE INTO Snippets_New \n                    SELECT * FROM Snippets\n                \"\"\")\n                # Drop old table\n                self.db.cursor.execute(\"DROP TABLE Snippets\")\n                # Rename new table\n                self.db.cursor.execute(\"ALTER TABLE Snippets_New RENAME TO Snippets\")\n            else:\n                # Just rename if no old table\n                self.db.cursor.execute(\"ALTER TABLE Snippets_New RENAME TO Snippets\")\n            \n            self.db.conn.commit()\n        except Exception as e:\n            # Table might already have the new schema\n            logger.debug(f\"Schema update note: {e}\")\n    \n    def create_snippet(self, name: str, snippet_type: SnippetType, content: str,\n                      metadata: Optional[Dict] = None) -> int:\n        \"\"\"Create a new snippet in the library.\"\"\"\n        query = \"\"\"\n        INSERT INTO Snippets (name, snippet_type, content, metadata)\n        VALUES (?, ?, ?, ?)\n        \"\"\"\n        metadata_json = json.dumps(metadata) if metadata else None\n        snippet_id = self.db.execute_update(query, (name, snippet_type.value, content, metadata_json))\n        logger.info(f\"Created Snippet '{name}' (ID: {snippet_id})\")\n        return snippet_id\n    \n    def get_snippet_by_name(self, name: str) -> Optional[Snippet]:\n        \"\"\"Retrieve a snippet by name.\"\"\"\n        query = \"SELECT * FROM Snippets WHERE name = ?\"\n        result = self.db.execute_query(query, (name,))\n        if result:\n            row = result[0]\n            return Snippet(\n                snippet_id=row['snippet_id'],\n                name=row['name'],\n                snippet_type=SnippetType(row['snippet_type']),\n                content=row['content'],\n                metadata=json.loads(row['metadata']) if row['metadata'] else None\n            )\n        return None\n    \n    def update_snippet(self, snippet_id: int, content: str = None, metadata: Dict = None):\n        \"\"\"Update an existing snippet.\"\"\"\n        if content:\n            self.db.execute_update(\n                \"UPDATE Snippets SET content = ?, updated_at = CURRENT_TIMESTAMP WHERE snippet_id = ?\",\n                (content, snippet_id)\n            )\n        if metadata:\n            self.db.execute_update(\n                \"UPDATE Snippets SET metadata = ?, updated_at = CURRENT_TIMESTAMP WHERE snippet_id = ?\",\n                (json.dumps(metadata), snippet_id)\n            )\n    \n    def list_snippets(self, snippet_type: Optional[SnippetType] = None) -> List[Snippet]:\n        \"\"\"List all snippets, optionally filtered by type.\"\"\"\n        if snippet_type:\n            query = \"SELECT * FROM Snippets WHERE snippet_type = ?\"\n            results = self.db.execute_query(query, (snippet_type.value,))\n        else:\n            query = \"SELECT * FROM Snippets\"\n            results = self.db.execute_query(query)\n        \n        return [\n            Snippet(\n                snippet_id=row['snippet_id'],\n                name=row['name'],\n                snippet_type=SnippetType(row['snippet_type']),\n                content=row['content'],\n                metadata=json.loads(row['metadata']) if row['metadata'] else None\n            )\n            for row in results\n        ]\n    \n    def delete_snippet(self, snippet_id: int):\n        \"\"\"Delete a snippet from the library.\"\"\"\n        self.db.execute_update(\"DELETE FROM Snippets WHERE snippet_id = ?\", (snippet_id,))\n        logger.info(f\"Deleted Snippet ID: {snippet_id}\")\n    \n    def create_convention_snippet(self, name: str, convention_rules: Dict) -> int:\n        \"\"\"Create a snippet specifically for data conventions.\"\"\"\n        content = ToonNotation.encode(convention_rules)\n        return self.create_snippet(\n            name=name,\n            snippet_type=SnippetType.CONVENTION,\n            content=content,\n            metadata={\"type\": \"naming_conventions\", \"auto_generated\": False}\n        )\n    \n    def create_changelog_snippet(self, name: str, changes: List[Dict]) -> int:\n        \"\"\"Create a snippet for version changelog.\"\"\"\n        content = ToonNotation.encode({\"changes\": changes})\n        return self.create_snippet(\n            name=name,\n            snippet_type=SnippetType.CHANGELOG,\n            content=content,\n            metadata={\"type\": \"version_history\", \"entries\": len(changes)}\n        )\n    \n    def create_instrument_snippet(self, name: str, instrument_data: Dict) -> int:\n        \"\"\"Create a snippet for instrument documentation.\"\"\"\n        content = ToonNotation.encode(instrument_data)\n        return self.create_snippet(\n            name=name,\n            snippet_type=SnippetType.INSTRUMENT,\n            content=content,\n            metadata={\"type\": \"instrument\", \"variable_count\": len(instrument_data.get(\"variables\", []))}\n        )\n\nprint(\"\u2713 SnippetManager loaded with extended snippet types:\")\nprint(\"   Core types: Summary, Chunk, Instruction, Version, Design, Mapping\")\nprint(\"   Extended types: Convention, Changelog, Instrument, Segment, Glossary\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Human-in-the-Loop Review Queue",
    "",
    "The ReviewQueue manages approval workflows for generated content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass",
    "class ReviewItem:",
    "    \"\"\"Represents an item in the review queue.\"\"\"",
    "    item_id: int",
    "    job_id: str",
    "    status: str",
    "    source_agent: str",
    "    target_agent: Optional[str]",
    "    source_data: str",
    "    generated_content: str",
    "    approved_content: Optional[str] = None",
    "    rejection_feedback: Optional[str] = None",
    "",
    "",
    "class ReviewQueueManager:",
    "    \"\"\"Manages the HITL review workflow.\"\"\"",
    "    ",
    "    def __init__(self, db_manager: DatabaseManager):",
    "        self.db = db_manager",
    "    ",
    "    def add_item(self, job_id: str, source_agent: str, source_data: str,",
    "                 generated_content: str, target_agent: Optional[str] = None) -> int:",
    "        \"\"\"Add an item to the review queue.\"\"\"",
    "        query = \"\"\"",
    "        INSERT INTO ReviewQueue (job_id, source_agent, target_agent, source_data, generated_content)",
    "        VALUES (?, ?, ?, ?, ?)",
    "        \"\"\"",
    "        item_id = self.db.execute_update(",
    "            query, (job_id, source_agent, target_agent, source_data, generated_content)",
    "        )",
    "        logger.info(f\"Added review item {item_id} from {source_agent}\")",
    "        return item_id",
    "    ",
    "    def get_pending_items(self, job_id: str) -> List[ReviewItem]:",
    "        \"\"\"Get all pending review items for a job.\"\"\"",
    "        query = \"SELECT * FROM ReviewQueue WHERE job_id = ? AND status = 'Pending'\"",
    "        results = self.db.execute_query(query, (job_id,))",
    "        return [",
    "            ReviewItem(",
    "                item_id=row['item_id'],",
    "                job_id=row['job_id'],",
    "                status=row['status'],",
    "                source_agent=row['source_agent'],",
    "                target_agent=row['target_agent'],",
    "                source_data=row['source_data'],",
    "                generated_content=row['generated_content'],",
    "                approved_content=row['approved_content'],",
    "                rejection_feedback=row['rejection_feedback']",
    "            )",
    "            for row in results",
    "        ]",
    "    ",
    "    def approve_item(self, item_id: int, approved_content: Optional[str] = None):",
    "        \"\"\"Approve a review item.\"\"\"",
    "        if approved_content:",
    "            query = \"\"\"",
    "            UPDATE ReviewQueue ",
    "            SET status = 'Approved', approved_content = ?, updated_at = CURRENT_TIMESTAMP",
    "            WHERE item_id = ?",
    "            \"\"\"",
    "            self.db.execute_update(query, (approved_content, item_id))",
    "        else:",
    "            query = \"\"\"",
    "            UPDATE ReviewQueue ",
    "            SET status = 'Approved', approved_content = generated_content, updated_at = CURRENT_TIMESTAMP",
    "            WHERE item_id = ?",
    "            \"\"\"",
    "            self.db.execute_update(query, (item_id,))",
    "        logger.info(f\"Approved review item {item_id}\")",
    "    ",
    "    def reject_item(self, item_id: int, feedback: str):",
    "        \"\"\"Reject a review item with feedback.\"\"\"",
    "        query = \"\"\"",
    "        UPDATE ReviewQueue ",
    "        SET status = 'Rejected', rejection_feedback = ?, updated_at = CURRENT_TIMESTAMP",
    "        WHERE item_id = ?",
    "        \"\"\"",
    "        self.db.execute_update(query, (feedback, item_id))",
    "        logger.info(f\"Rejected review item {item_id}\")",
    "    ",
    "    def get_approved_items(self, job_id: str) -> List[ReviewItem]:",
    "        \"\"\"Get all approved items for a job.\"\"\"",
    "        query = \"SELECT * FROM ReviewQueue WHERE job_id = ? AND status = 'Approved'\"",
    "        results = self.db.execute_query(query, (job_id,))",
    "        return [",
    "            ReviewItem(",
    "                item_id=row['item_id'],",
    "                job_id=row['job_id'],",
    "                status=row['status'],",
    "                source_agent=row['source_agent'],",
    "                target_agent=row['target_agent'],",
    "                source_data=row['source_data'],",
    "                generated_content=row['generated_content'],",
    "                approved_content=row['approved_content'],",
    "                rejection_feedback=row['rejection_feedback']",
    "            )",
    "            for row in results",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Core Agent Classes",
    "",
    "Specialized agents with retry logic, rate limiting, and Toon context injection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class BaseAgent:\n    \"\"\"Base class for all agents with rate limiting, retry logic, and observability.\"\"\"\n    \n    def __init__(self, name: str, system_prompt: str, config: APIConfig = None):\n        self.name = name\n        self.system_prompt = system_prompt\n        self.config = config or API_CONFIG\n        self.model = genai.GenerativeModel(self.config.model_name)\n        self.active_snippets: List[Snippet] = []\n        self.last_request_time = 0\n        self.request_count = 0\n        self.logger = logging.getLogger(f'ADE.{name}')\n    \n    def inject_snippets(self, snippets: List[Snippet]):\n        \"\"\"Inject context snippets into agent.\"\"\"\n        self.active_snippets = snippets\n        self.logger.info(f\"Injected {len(snippets)} snippets\")\n    \n    def build_prompt(self, user_input: str, additional_context: str = \"\") -> str:\n        \"\"\"Build the full prompt with system prompt, snippets, and user input.\"\"\"\n        prompt_parts = [self.system_prompt]\n        \n        if self.active_snippets:\n            prompt_parts.append(\"\\n=== CONTEXT (Snippets) ===\")\n            for snippet in self.active_snippets:\n                prompt_parts.append(f\"\\n[{snippet.snippet_type.value}: {snippet.name}]\")\n                prompt_parts.append(snippet.content)\n        \n        if additional_context:\n            prompt_parts.append(\"\\n=== ADDITIONAL CONTEXT ===\")\n            prompt_parts.append(additional_context)\n        \n        prompt_parts.append(\"\\n=== INPUT ===\")\n        prompt_parts.append(user_input)\n        \n        return \"\\n\".join(prompt_parts)\n    \n    def _wait_for_rate_limit(self):\n        \"\"\"Implement rate limiting by waiting if necessary.\"\"\"\n        if self.last_request_time > 0:\n            elapsed = time.time() - self.last_request_time\n            if elapsed < self.config.min_delay:\n                wait_time = self.config.min_delay - elapsed\n                print(f\"\u23f1\ufe0f  Rate limiting: waiting {wait_time:.1f}s...\")\n                time.sleep(wait_time)\n    \n    def generate(self, prompt: str) -> str:\n        \"\"\"Generate response with retry logic and rate limiting.\"\"\"\n        for attempt in range(self.config.max_retries):\n            try:\n                self._wait_for_rate_limit()\n                self.last_request_time = time.time()\n                self.request_count += 1\n                \n                response = self.model.generate_content(prompt)\n                self.logger.info(f\"Request {self.request_count} successful\")\n                return response.text\n                \n            except Exception as e:\n                error_str = str(e)\n                if \"429\" in error_str or \"quota\" in error_str.lower():\n                    wait_time = self.config.base_retry_delay * (2 ** attempt)\n                    self.logger.warning(f\"Rate limit hit, retrying in {wait_time}s (attempt {attempt + 1})\")\n                    print(f\"\u26a0\ufe0f  Rate limit hit, waiting {wait_time}s before retry {attempt + 1}/{self.config.max_retries}\")\n                    time.sleep(wait_time)\n                else:\n                    self.logger.error(f\"API error: {error_str}\")\n                    raise\n        \n        raise Exception(f\"Max retries ({self.config.max_retries}) exceeded\")\n    \n    def process(self, user_input: str, additional_context: str = \"\") -> str:\n        \"\"\"Process input through the agent.\"\"\"\n        prompt = self.build_prompt(user_input, additional_context)\n        return self.generate(prompt)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataParserAgent(BaseAgent):",
    "    \"\"\"Agent for parsing raw data into standardized JSON format.\"\"\"",
    "    ",
    "    def __init__(self, config: APIConfig = None):",
    "        system_prompt = \"\"\"You are a DataParserAgent specialized in converting raw data specifications into standardized JSON format.",
    "",
    "Your task:",
    "1. Parse the input data (CSV, JSON, or XML)",
    "2. Preserve all original field names and values",
    "3. Output a JSON array where each element represents one variable/field",
    "4. Include: original_name, original_type, original_description, and any metadata",
    "",
    "Output format:",
    "```json",
    "[",
    "  {",
    "    \"original_name\": \"field_name\",",
    "    \"original_type\": \"type\",",
    "    \"original_description\": \"description\",",
    "    \"metadata\": {}",
    "  }",
    "]",
    "```",
    "",
    "Only output valid JSON. No additional commentary.\"\"\"",
    "        super().__init__(\"DataParserAgent\", system_prompt, config)",
    "    ",
    "    def parse_csv(self, csv_data: str) -> List[Dict]:",
    "        \"\"\"Parse CSV data dictionary.\"\"\"",
    "        result = self.process(csv_data)",
    "        if \"```json\" in result:",
    "            result = result.split(\"```json\")[1].split(\"```\")[0].strip()",
    "        elif \"```\" in result:",
    "            result = result.split(\"```\")[1].split(\"```\")[0].strip()",
    "        return json.loads(result)",
    "",
    "",
    "class TechnicalAnalyzerAgent(BaseAgent):",
    "    \"\"\"Agent for analyzing technical properties and mapping to internal standards.\"\"\"",
    "    ",
    "    def __init__(self, config: APIConfig = None):",
    "        system_prompt = \"\"\"You are a TechnicalAnalyzerAgent specialized in analyzing data fields and mapping them to internal standards.",
    "",
    "**Input Format: Toon Notation**",
    "Input data is provided in Toon notation (compact format):",
    "- `key: value` for simple fields",
    "- `key[n]{col1,col2}:` followed by data rows for tabular data",
    "",
    "Your task:",
    "1. Analyze each field from the parsed data",
    "2. Infer technical properties (data_type, constraints, cardinality)",
    "3. Map to standardized field names following healthcare data conventions",
    "4. Flag unclear mappings for clarification",
    "",
    "Output format:",
    "```json",
    "[",
    "  {",
    "    \"original_name\": \"field_name\",",
    "    \"variable_name\": \"standardized_name\",",
    "    \"data_type\": \"categorical|continuous|date|text|boolean\",",
    "    \"description\": \"description\",",
    "    \"constraints\": {},",
    "    \"cardinality\": \"required|optional|repeated\",",
    "    \"confidence\": \"high|medium|low\",",
    "    \"needs_clarification\": false,",
    "    \"clarification_question\": \"\"",
    "  }",
    "]",
    "```",
    "",
    "Only output valid JSON. No additional commentary.\"\"\"",
    "        super().__init__(\"TechnicalAnalyzerAgent\", system_prompt, config)",
    "    ",
    "    def analyze(self, parsed_data: List[Dict], clarifications: Optional[Dict[str, str]] = None) -> List[Dict]:",
    "        \"\"\"Analyze parsed data and map to internal standards.\"\"\"",
    "        additional_context = \"\"",
    "        if clarifications:",
    "            additional_context = \"\\n=== USER CLARIFICATIONS ===\\n\"",
    "            for field, clarification in clarifications.items():",
    "                additional_context += f\"{field}: {clarification}\\n\"",
    "        ",
    "        toon_encoded = ToonNotation.encode({\"variables\": parsed_data})",
    "        format_context = \"\\nData is in Toon notation format. Output JSON as specified.\\n\"",
    "        result = self.process(toon_encoded, format_context + additional_context)",
    "        ",
    "        if \"```json\" in result:",
    "            result = result.split(\"```json\")[1].split(\"```\")[0].strip()",
    "        elif \"```\" in result:",
    "            result = result.split(\"```\")[1].split(\"```\")[0].strip()",
    "        return json.loads(result)",
    "",
    "",
    "class DomainOntologyAgent(BaseAgent):",
    "    \"\"\"Agent for mapping to standard healthcare ontologies.\"\"\"",
    "    ",
    "    def __init__(self, config: APIConfig = None):",
    "        system_prompt = \"\"\"You are a DomainOntologyAgent specialized in mapping healthcare data fields to standard ontologies.",
    "",
    "Your task:",
    "1. For each variable, identify appropriate standard ontology codes",
    "2. Primary ontologies: OMOP CDM, LOINC, SNOMED CT, RxNorm",
    "3. Provide code and standard term",
    "4. Include confidence score for each mapping",
    "",
    "Output format:",
    "```json",
    "{",
    "  \"variable_name\": \"standardized_name\",",
    "  \"ontology_mappings\": [",
    "    {",
    "      \"system\": \"OMOP\",",
    "      \"code\": \"123456\",",
    "      \"display\": \"Standard Concept Name\",",
    "      \"confidence\": \"high\"",
    "    }",
    "  ]",
    "}",
    "```",
    "",
    "Only output valid JSON. No additional commentary.\"\"\"",
    "        super().__init__(\"DomainOntologyAgent\", system_prompt, config)",
    "    ",
    "    def map_ontologies(self, variable_data: Dict) -> Dict:",
    "        \"\"\"Map a variable to standard ontologies.\"\"\"",
    "        toon_encoded = ToonNotation.encode(variable_data)",
    "        result = self.process(toon_encoded, \"\\nInput is in Toon notation. Output JSON.\\n\")",
    "        ",
    "        if \"```json\" in result:",
    "            result = result.split(\"```json\")[1].split(\"```\")[0].strip()",
    "        elif \"```\" in result:",
    "            result = result.split(\"```\")[1].split(\"```\")[0].strip()",
    "        return json.loads(result)",
    "",
    "",
    "class PlainLanguageAgent(BaseAgent):",
    "    \"\"\"Agent for generating human-readable documentation.\"\"\"",
    "    ",
    "    def __init__(self, config: APIConfig = None):",
    "        system_prompt = \"\"\"You are a PlainLanguageAgent specialized in creating clear, comprehensive documentation for healthcare data variables.",
    "",
    "Your task:",
    "1. Convert technical variable specifications into plain language",
    "2. Explain clinical/research context",
    "3. Describe data type, constraints, and valid values",
    "4. Include ontology mappings and significance",
    "5. Write for interdisciplinary audience (clinicians, researchers, data scientists)",
    "",
    "Output format (Markdown):",
    "```markdown",
    "## Variable: [Variable Name]",
    "",
    "**Description:** [Clear, concise description]",
    "",
    "**Technical Details:**",
    "- Data Type: [type]",
    "- Cardinality: [required/optional]",
    "- Valid Values: [constraints or ranges]",
    "",
    "**Standard Ontology Mappings:**",
    "- OMOP: [code] - [term]",
    "- LOINC: [code] - [term]",
    "",
    "**Clinical Context:** [Explanation of why this variable matters]",
    "```",
    "",
    "Only output Markdown documentation. No additional commentary.\"\"\"",
    "        super().__init__(\"PlainLanguageAgent\", system_prompt, config)",
    "    ",
    "    def document_variable(self, enriched_data: Dict) -> str:",
    "        \"\"\"Generate plain language documentation for a variable.\"\"\"",
    "        toon_encoded = ToonNotation.encode(enriched_data)",
    "        result = self.process(toon_encoded, \"\\nInput is in Toon notation. Generate markdown.\\n\")",
    "        ",
    "        if \"```markdown\" in result:",
    "            result = result.split(\"```markdown\")[1].split(\"```\")[0].strip()",
    "        elif result.startswith(\"```\") and result.endswith(\"```\"):",
    "            result = result.split(\"```\")[1].split(\"```\")[0].strip()",
    "        return result",
    "",
    "",
    "class DocumentationAssemblerAgent(BaseAgent):",
    "    \"\"\"Agent for assembling final documentation from approved items.\"\"\"",
    "    ",
    "    def __init__(self, review_queue: ReviewQueueManager, config: APIConfig = None):",
    "        system_prompt = \"\"\"You are a DocumentationAssemblerAgent specialized in creating comprehensive, well-structured data documentation.",
    "",
    "Your task:",
    "1. Compile all approved variable documentation into a cohesive document",
    "2. Add a table of contents",
    "3. Include metadata (generation date, source file, etc.)",
    "4. Organize by logical groupings if applicable",
    "5. Ensure consistent formatting throughout",
    "",
    "Output: A complete Markdown document ready for publication.\"\"\"",
    "        super().__init__(\"DocumentationAssemblerAgent\", system_prompt, config)",
    "        self.review_queue = review_queue",
    "    ",
    "    def assemble(self, job_id: str) -> str:",
    "        \"\"\"Assemble final documentation from approved review items.\"\"\"",
    "        approved_items = self.review_queue.get_approved_items(job_id)",
    "        ",
    "        if not approved_items:",
    "            return \"# No approved documentation found for this job.\"",
    "        ",
    "        doc_parts = [",
    "            \"# Healthcare Data Documentation\",",
    "            f\"\\n**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",",
    "            f\"**Job ID:** {job_id}\",",
    "            \"\\n---\\n\"",
    "        ]",
    "        ",
    "        doc_parts.append(\"## Table of Contents\\n\")",
    "        for i, item in enumerate(approved_items, 1):",
    "            content = item.approved_content",
    "            if \"## Variable:\" in content:",
    "                var_name = content.split(\"## Variable:\")[1].split(\"\\n\")[0].strip()",
    "                doc_parts.append(f\"{i}. [{var_name}](#{var_name.lower().replace(' ', '-')})\")",
    "        ",
    "        doc_parts.append(\"\\n---\\n\")",
    "        ",
    "        for item in approved_items:",
    "            doc_parts.append(item.approved_content)",
    "            doc_parts.append(\"\\n---\\n\")",
    "        ",
    "        return \"\\n\".join(doc_parts)",
    "",
    "",
    "print(\"\u2713 All agent classes defined with Toon support and observability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 6.1 Extended Agent Classes\n\nAdditional specialized agents for design improvement, data conventions compliance, version control, and higher-level documentation.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DesignImprovementAgent(BaseAgent):\n",
    "    \"\"\"Agent for enhancing design documentation and improving clarity.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: APIConfig = None):\n",
    "        system_prompt = \"\"\"You are a DesignImprovementAgent specialized in enhancing \n",
    "        documentation design and clarity.\n",
    "        \n",
    "        Your task:\n",
    "        1. Review the provided documentation\n",
    "        2. Identify areas for improvement in structure, clarity, and completeness\n",
    "        3. Suggest and apply design enhancements\n",
    "        4. Score the design before and after improvements\n",
    "        \n",
    "        Output format:\n",
    "        ```json\n",
    "        {\n",
    "          \"improved_content\": \"enhanced documentation text\",\n",
    "          \"design_score\": {\n",
    "            \"before\": 70,\n",
    "            \"after\": 85\n",
    "          },\n",
    "          \"improvements_made\": [\"list of improvements\"]\n",
    "        }\n",
    "        ```\n",
    "        Only output valid JSON. No additional commentary.\"\"\"\n",
    "        \n",
    "        super().__init__(\"DesignImprovementAgent\", system_prompt, config)\n",
    "    \n",
    "    def improve_design(self, documentation: str) -> Dict:\n",
    "        \"\"\"Improve the design of documentation.\"\"\"\n",
    "        result = self.process(documentation)\n",
    "        if \"```json\" in result:\n",
    "            result = result.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        try:\n",
    "            return json.loads(result)\n",
    "        except json.JSONDecodeError:\n",
    "            return {\"improved_content\": documentation, \"design_score\": {\"before\": 0, \"after\": 0}}\n",
    "\n",
    "\n",
    "class DataConventionsAgent(BaseAgent):\n",
    "    \"\"\"Agent for analyzing and enforcing data naming conventions.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: APIConfig = None):\n",
    "        system_prompt = \"\"\"You are a DataConventionsAgent specialized in analyzing \n",
    "        data naming conventions and standards compliance.\n",
    "        \n",
    "        Your task:\n",
    "        1. Analyze variable naming patterns\n",
    "        2. Check compliance with common standards (snake_case, camelCase, etc.)\n",
    "        3. Identify convention violations and warnings\n",
    "        4. Suggest standardized names\n",
    "        \n",
    "        Output format:\n",
    "        ```json\n",
    "        {\n",
    "          \"naming_pattern\": \"detected pattern\",\n",
    "          \"convention_compliance\": 85,\n",
    "          \"convention_warnings\": [\"list of warnings\"],\n",
    "          \"suggested_name\": \"standardized_name\"\n",
    "        }\n",
    "        ```\n",
    "        Only output valid JSON. No additional commentary.\"\"\"\n",
    "        \n",
    "        super().__init__(\"DataConventionsAgent\", system_prompt, config)\n",
    "    \n",
    "    def analyze_conventions(self, var_data: Dict) -> Dict:\n",
    "        \"\"\"Analyze naming conventions for a variable.\"\"\"\n",
    "        result = self.process(json.dumps(var_data))\n",
    "        if \"```json\" in result:\n",
    "            result = result.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        try:\n",
    "            return json.loads(result)\n",
    "        except json.JSONDecodeError:\n",
    "            return {\"naming_pattern\": \"unknown\", \"convention_compliance\": 0, \"convention_warnings\": []}\n",
    "    \n",
    "    def generate_conventions_glossary(self, all_vars: List[Dict]) -> Dict:\n",
    "        \"\"\"Generate a glossary of conventions used.\"\"\"\n",
    "        patterns = {}\n",
    "        for var in all_vars:\n",
    "            conv = var.get('conventions', {})\n",
    "            pattern = conv.get('naming_pattern', 'unknown')\n",
    "            patterns[pattern] = patterns.get(pattern, 0) + 1\n",
    "        \n",
    "        dominant = max(patterns.keys(), key=lambda k: patterns[k]) if patterns else 'mixed'\n",
    "        return {\n",
    "            \"dominant_pattern\": dominant,\n",
    "            \"pattern_distribution\": patterns,\n",
    "            \"total_variables\": len(all_vars)\n",
    "        }\n",
    "\n",
    "\n",
    "class VersionControlAgent(BaseAgent):\n",
    "    \"\"\"Agent for tracking documentation versions and changes.\"\"\"\n",
    "    \n",
    "    def __init__(self, db_manager: DatabaseManager, config: APIConfig = None):\n",
    "        system_prompt = \"\"\"You are a VersionControlAgent specialized in tracking \n",
    "        documentation versions and managing change history.\"\"\"\n",
    "        \n",
    "        super().__init__(\"VersionControlAgent\", system_prompt, config)\n",
    "        self.db = db_manager\n",
    "    \n",
    "    def create_version(self, element_id: str, element_type: str, content: str, author: str = \"system\") -> Dict:\n",
    "        \"\"\"Create a new version for a documentation element.\"\"\"\n",
    "        # Get current version\n",
    "        query = \"\"\"SELECT version FROM SystemState \n",
    "                   WHERE key = ? ORDER BY updated_at DESC LIMIT 1\"\"\"\n",
    "        current = self.db.execute_query(query, (f\"{element_type}:{element_id}:version\",))\n",
    "        \n",
    "        if current:\n",
    "            current_version = current[0]['version']\n",
    "            # Increment version\n",
    "            parts = current_version.split('.')\n",
    "            parts[-1] = str(int(parts[-1]) + 1)\n",
    "            new_version = '.'.join(parts)\n",
    "        else:\n",
    "            new_version = \"1.0.0\"\n",
    "        \n",
    "        # Store version\n",
    "        self.db.execute_update(\n",
    "            \"\"\"INSERT OR REPLACE INTO SystemState (key, value, version, updated_at)\n",
    "               VALUES (?, ?, ?, CURRENT_TIMESTAMP)\"\"\",\n",
    "            (f\"{element_type}:{element_id}:version\", content, new_version)\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"element_id\": element_id,\n",
    "            \"element_type\": element_type,\n",
    "            \"new_version\": new_version,\n",
    "            \"author\": author\n",
    "        }\n",
    "    \n",
    "    def get_version_history(self, element_id: str) -> List[Dict]:\n",
    "        \"\"\"Get version history for an element.\"\"\"\n",
    "        query = \"\"\"SELECT * FROM SystemState \n",
    "                   WHERE key LIKE ? ORDER BY updated_at DESC\"\"\"\n",
    "        return self.db.execute_query(query, (f\"%{element_id}%\",))\n",
    "    \n",
    "    def rollback_to_version(self, element_id: str, target_version: str) -> Dict:\n",
    "        \"\"\"Rollback to a specific version.\"\"\"\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"element_id\": element_id,\n",
    "            \"rolled_back_to\": target_version\n",
    "        }\n",
    "\n",
    "\n",
    "class HigherLevelDocumentationAgent(BaseAgent):\n",
    "    \"\"\"Agent for generating higher-level documentation (instruments, segments, codebooks).\"\"\"\n",
    "    \n",
    "    def __init__(self, config: APIConfig = None):\n",
    "        system_prompt = \"\"\"You are a HigherLevelDocumentationAgent specialized in \n",
    "        generating higher-level documentation for instruments, segments, and codebooks.\n",
    "        \n",
    "        Your task:\n",
    "        1. Group related variables into logical instruments/segments\n",
    "        2. Generate comprehensive documentation for these groupings\n",
    "        3. Create codebook overviews\n",
    "        \n",
    "        Output format for instrument documentation:\n",
    "        ```json\n",
    "        {\n",
    "          \"instrument_name\": \"name\",\n",
    "          \"description\": \"description\",\n",
    "          \"variables\": [\"list of variable names\"],\n",
    "          \"documentation_markdown\": \"markdown documentation\"\n",
    "        }\n",
    "        ```\n",
    "        Only output valid JSON. No additional commentary.\"\"\"\n",
    "        \n",
    "        super().__init__(\"HigherLevelDocumentationAgent\", system_prompt, config)\n",
    "    \n",
    "    def identify_instruments(self, all_vars: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Identify potential instruments/segments from variables.\"\"\"\n",
    "        # Group by common prefixes or patterns\n",
    "        groups = {}\n",
    "        for var in all_vars:\n",
    "            name = var.get('original_name', var.get('variable_name', 'unknown'))\n",
    "            # Simple grouping by prefix\n",
    "            prefix = name.split('_')[0] if '_' in name else name[:3]\n",
    "            if prefix not in groups:\n",
    "                groups[prefix] = []\n",
    "            groups[prefix].append(var)\n",
    "        \n",
    "        instruments = []\n",
    "        for prefix, vars in groups.items():\n",
    "            if len(vars) >= 2:  # Only group if 2+ variables\n",
    "                instruments.append({\n",
    "                    \"suggested_name\": f\"{prefix}_instrument\",\n",
    "                    \"variable_count\": len(vars),\n",
    "                    \"variables\": vars\n",
    "                })\n",
    "        \n",
    "        return instruments\n",
    "    \n",
    "    def document_instrument(self, variables: List[Dict]) -> Dict:\n",
    "        \"\"\"Generate documentation for an instrument.\"\"\"\n",
    "        var_summary = json.dumps(variables[:10])  # Limit for context\n",
    "        result = self.process(f\"Document this instrument with variables: {var_summary}\")\n",
    "        if \"```json\" in result:\n",
    "            result = result.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        try:\n",
    "            return json.loads(result)\n",
    "        except json.JSONDecodeError:\n",
    "            return {\n",
    "                \"instrument_name\": \"Unknown\",\n",
    "                \"description\": \"Auto-generated instrument\",\n",
    "                \"variables\": [v.get('original_name', 'unknown') for v in variables],\n",
    "                \"documentation_markdown\": \"Documentation pending\"\n",
    "            }\n",
    "    \n",
    "    def generate_codebook_overview(self, all_vars: List[Dict], instruments: List[Dict] = None) -> Dict:\n",
    "        \"\"\"Generate an overview for the entire codebook.\"\"\"\n",
    "        return {\n",
    "            \"total_variables\": len(all_vars),\n",
    "            \"instruments\": len(instruments) if instruments else 0,\n",
    "            \"overview\": f\"Codebook containing {len(all_vars)} variables\"\n",
    "        }\n",
    "\n",
    "\n",
    "class ValidationAgent(BaseAgent):\n",
    "    \"\"\"Agent for validating outputs from other agents and ensuring quality and consistency.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: APIConfig = None):\n",
    "        system_prompt = \"\"\"You are a ValidationAgent specialized in validating and \n",
    "        quality-checking outputs from other agents in the documentation pipeline.\n",
    "        \n",
    "        Your task:\n",
    "        1. Review outputs from various agents for correctness and completeness\n",
    "        2. Check for consistency across different agent outputs\n",
    "        3. Identify potential errors, inconsistencies, or missing information\n",
    "        4. Validate data types, formats, and standards compliance\n",
    "        5. Ensure ontology mappings are accurate and appropriate\n",
    "        6. Verify that documentation is clear, accurate, and complete\n",
    "        \n",
    "        Output format:\n",
    "        ```json\n",
    "        {\n",
    "          \"validation_passed\": true/false,\n",
    "          \"overall_score\": 0-100,\n",
    "          \"issues_found\": [\n",
    "            {\n",
    "              \"severity\": \"critical/warning/info\",\n",
    "              \"category\": \"category_name\",\n",
    "              \"description\": \"issue description\",\n",
    "              \"affected_field\": \"field_name\",\n",
    "              \"suggestion\": \"how to fix\"\n",
    "            }\n",
    "          ],\n",
    "          \"consistency_checks\": {\n",
    "            \"naming_consistent\": true/false,\n",
    "            \"types_valid\": true/false,\n",
    "            \"ontologies_appropriate\": true/false,\n",
    "            \"documentation_complete\": true/false\n",
    "          },\n",
    "          \"recommendations\": [\"list of improvement recommendations\"],\n",
    "          \"validated_at\": \"timestamp\"\n",
    "        }\n",
    "        ```\n",
    "        Only output valid JSON. No additional commentary.\n",
    "        \n",
    "        Be thorough but fair in your validation. Focus on:\n",
    "        - Data integrity and correctness\n",
    "        - Consistency across all outputs\n",
    "        - Completeness of documentation\n",
    "        - Appropriateness of ontology mappings\n",
    "        - Clarity of plain language descriptions\"\"\"\n",
    "        \n",
    "        super().__init__(\"ValidationAgent\", system_prompt, config)\n",
    "    \n",
    "    def validate_parsed_data(self, parsed_data: List[Dict]) -> Dict:\n",
    "        \"\"\"Validate the output from DataParserAgent.\"\"\"\n",
    "        validation_input = f\"Validate this parsed data output: {json.dumps(parsed_data[:20])}\"\n",
    "        result = self.process(validation_input)\n",
    "        return self._parse_validation_result(result)\n",
    "    \n",
    "    def validate_technical_analysis(self, analyzed_data: List[Dict]) -> Dict:\n",
    "        \"\"\"Validate the output from TechnicalAnalyzerAgent.\"\"\"\n",
    "        validation_input = f\"Validate this technical analysis output: {json.dumps(analyzed_data[:10])}\"\n",
    "        result = self.process(validation_input)\n",
    "        return self._parse_validation_result(result)\n",
    "    \n",
    "    def validate_ontology_mappings(self, enriched_data: List[Dict]) -> Dict:\n",
    "        \"\"\"Validate ontology mappings from DomainOntologyAgent.\"\"\"\n",
    "        validation_input = f\"Validate these ontology mappings: {json.dumps(enriched_data[:10])}\"\n",
    "        result = self.process(validation_input)\n",
    "        return self._parse_validation_result(result)\n",
    "    \n",
    "    def validate_documentation(self, documentation: str) -> Dict:\n",
    "        \"\"\"Validate plain language documentation from PlainLanguageAgent.\"\"\"\n",
    "        validation_input = f\"Validate this documentation for clarity and completeness: {documentation[:2000]}\"\n",
    "        result = self.process(validation_input)\n",
    "        return self._parse_validation_result(result)\n",
    "    \n",
    "    def validate_full_pipeline_output(self, pipeline_results: Dict) -> Dict:\n",
    "        \"\"\"Validate the complete output from the entire agent pipeline.\"\"\"\n",
    "        validation_input = f\"\"\"Validate this complete pipeline output for consistency and quality:\n",
    "        \n",
    "        Parsed Data Summary: {len(pipeline_results.get('parsed_data', []))} variables\n",
    "        Technical Analysis: {len(pipeline_results.get('analyzed_data', []))} analyzed\n",
    "        Ontology Mappings: {len(pipeline_results.get('enriched_data', []))} mapped\n",
    "        Documentation: {len(pipeline_results.get('documentation', []))} documents\n",
    "        \n",
    "        Sample Data: {json.dumps(pipeline_results, default=str)[:3000]}\n",
    "        \"\"\"\n",
    "        result = self.process(validation_input)\n",
    "        return self._parse_validation_result(result)\n",
    "    \n",
    "    def cross_validate_agents(self, agent_outputs: Dict[str, Any]) -> Dict:\n",
    "        \"\"\"Cross-validate outputs from multiple agents for consistency.\"\"\"\n",
    "        validation_input = f\"\"\"Cross-validate these outputs from different agents for consistency:\n",
    "        {json.dumps(agent_outputs, default=str)[:3000]}\n",
    "        \n",
    "        Check for:\n",
    "        1. Consistent variable naming across outputs\n",
    "        2. Matching data types and formats\n",
    "        3. Coherent ontology mappings\n",
    "        4. Complete information flow between agents\n",
    "        \"\"\"\n",
    "        result = self.process(validation_input)\n",
    "        return self._parse_validation_result(result)\n",
    "    \n",
    "    def _parse_validation_result(self, result: str) -> Dict:\n",
    "        \"\"\"Parse the validation result from the LLM response.\"\"\"\n",
    "        if \"```json\" in result:\n",
    "            result = result.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        try:\n",
    "            parsed = json.loads(result)\n",
    "            # Ensure required fields exist\n",
    "            if 'validation_passed' not in parsed:\n",
    "                parsed['validation_passed'] = parsed.get('overall_score', 0) >= 70\n",
    "            if 'validated_at' not in parsed:\n",
    "                parsed['validated_at'] = datetime.now().isoformat()\n",
    "            return parsed\n",
    "        except json.JSONDecodeError:\n",
    "            return {\n",
    "                \"validation_passed\": False,\n",
    "                \"overall_score\": 0,\n",
    "                \"issues_found\": [{\n",
    "                    \"severity\": \"critical\",\n",
    "                    \"category\": \"parse_error\",\n",
    "                    \"description\": \"Could not parse validation result\",\n",
    "                    \"affected_field\": \"all\",\n",
    "                    \"suggestion\": \"Retry validation\"\n",
    "                }],\n",
    "                \"consistency_checks\": {\n",
    "                    \"naming_consistent\": False,\n",
    "                    \"types_valid\": False,\n",
    "                    \"ontologies_appropriate\": False,\n",
    "                    \"documentation_complete\": False\n",
    "                },\n",
    "                \"recommendations\": [\"Retry validation with clearer input\"],\n",
    "                \"validated_at\": datetime.now().isoformat()\n",
    "            }\n",
    "    \n",
    "    def generate_validation_report(self, all_validations: List[Dict]) -> str:\n",
    "        \"\"\"Generate a comprehensive validation report.\"\"\"\n",
    "        total_checks = len(all_validations)\n",
    "        passed = sum(1 for v in all_validations if v.get('validation_passed', False))\n",
    "        avg_score = sum(v.get('overall_score', 0) for v in all_validations) / max(total_checks, 1)\n",
    "        \n",
    "        all_issues = []\n",
    "        for v in all_validations:\n",
    "            all_issues.extend(v.get('issues_found', []))\n",
    "        \n",
    "        critical_issues = [i for i in all_issues if i.get('severity') == 'critical']\n",
    "        warnings = [i for i in all_issues if i.get('severity') == 'warning']\n",
    "        \n",
    "        report = f\"\"\"# Validation Report\n",
    "        \n",
    "## Summary\n",
    "- Total Validations: {total_checks}\n",
    "- Passed: {passed}/{total_checks} ({100*passed/max(total_checks,1):.1f}%)\n",
    "- Average Score: {avg_score:.1f}/100\n",
    "\n",
    "## Issues Found\n",
    "- Critical: {len(critical_issues)}\n",
    "- Warnings: {len(warnings)}\n",
    "- Info: {len(all_issues) - len(critical_issues) - len(warnings)}\n",
    "\n",
    "## Critical Issues\n",
    "\"\"\"\n",
    "        for issue in critical_issues[:10]:\n",
    "            report += f\"- [{issue.get('category')}] {issue.get('description')}\\n\"\n",
    "            report += f\"  Suggestion: {issue.get('suggestion')}\\n\\n\"\n",
    "        \n",
    "        report += \"\"\"\n",
    "## Recommendations\n",
    "\"\"\"\n",
    "        all_recs = []\n",
    "        for v in all_validations:\n",
    "            all_recs.extend(v.get('recommendations', []))\n",
    "        \n",
    "        for rec in list(set(all_recs))[:10]:\n",
    "            report += f\"- {rec}\\n\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "\n",
    "print(\"\u2713 Extended Agent classes defined:\")\n",
    "print(\"   - DesignImprovementAgent: Enhances documentation design\")\n",
    "print(\"   - DataConventionsAgent: Enforces naming conventions\")\n",
    "print(\"   - VersionControlAgent: Tracks documentation versions\")\n",
    "print(\"   - HigherLevelDocumentationAgent: Generates instrument/codebook docs\")\n",
    "print(\"   - ValidationAgent: Validates outputs for quality and consistency\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class Orchestrator:\n",
    "    \"\"\"Manages the workflow of agents and coordinates the documentation pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self, db_manager: DatabaseManager, api_config: APIConfig = None):\n",
    "        self.db = db_manager\n",
    "        self.config = api_config or API_CONFIG\n",
    "        self.snippet_manager = SnippetManager(db_manager)\n",
    "        self.review_queue = ReviewQueueManager(db_manager)\n",
    "        \n",
    "        # Initialize core agents with configuration\n",
    "        self.data_parser = DataParserAgent(config=self.config)\n",
    "        self.technical_analyzer = TechnicalAnalyzerAgent(config=self.config)\n",
    "        self.domain_ontology = DomainOntologyAgent(config=self.config)\n",
    "        self.plain_language = PlainLanguageAgent(config=self.config)\n",
    "        self.assembler = DocumentationAssemblerAgent(self.review_queue, config=self.config)\n",
    "        \n",
    "        # Initialize extended agents\n",
    "        self.design_improvement = DesignImprovementAgent(config=self.config)\n",
    "        self.data_conventions = DataConventionsAgent(config=self.config)\n",
    "        self.version_control = VersionControlAgent(db_manager, config=self.config)\n",
    "        self.higher_level_docs = HigherLevelDocumentationAgent(config=self.config)\n",
    "        self.validation = ValidationAgent(config=self.config)\n",
    "        \n",
    "        logger.info(f\"Orchestrator initialized with {self.config.requests_per_minute} req/min limit\")\n",
    "        print(f\"\u2713 Orchestrator initialized with {self.config.requests_per_minute} req/min limit\")\n",
    "        print(f\"   Core agents: DataParser, TechnicalAnalyzer, DomainOntology, PlainLanguage, Assembler\")\n",
    "        print(f\"   Extended agents: DesignImprovement, DataConventions, VersionControl, HigherLevelDocs, Validation\")\n",
    "    \n",
    "    def create_job(self, source_file: str) -> str:\n",
    "        \"\"\"Create a new documentation job.\"\"\"\n",
    "        job_id = hashlib.md5(f\"{source_file}_{datetime.now().isoformat()}\".encode()).hexdigest()[:12]\n",
    "        query = \"INSERT INTO Jobs (job_id, source_file, status) VALUES (?, ?, 'Running')\"\n",
    "        self.db.execute_update(query, (job_id, source_file))\n",
    "        logger.info(f\"Created job {job_id} for {source_file}\")\n",
    "        return job_id\n",
    "    \n",
    "    def process_data_dictionary(self, source_data: str, source_file: str = \"input.csv\",\n",
    "                                auto_approve: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        Main workflow: Process a data dictionary through the agent pipeline.\n",
    "        \n",
    "        Args:\n",
    "            source_data: The raw data dictionary content\n",
    "            source_file: Name of the source file\n",
    "            auto_approve: If True, automatically approve all generated content\n",
    "            \n",
    "        Returns:\n",
    "            job_id: The ID of the created job\n",
    "        \"\"\"\n",
    "        job_id = self.create_job(source_file)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing Job: {job_id}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Step 1: Parse data\n",
    "        print(\"\\n\ud83d\udcca Step 1: Parsing Data...\")\n",
    "        parsed_data = self.data_parser.parse_csv(source_data)\n",
    "        print(f\"   \u2713 Parsed {len(parsed_data)} variables\")\n",
    "        \n",
    "        # Step 2: Technical analysis\n",
    "        print(\"\\n\ud83d\udd2c Step 2: Technical Analysis...\")\n",
    "        analyzed_data = self.technical_analyzer.analyze(parsed_data)\n",
    "        print(f\"   \u2713 Analyzed {len(analyzed_data)} variables\")\n",
    "        \n",
    "        # Check for clarifications needed\n",
    "        needs_clarification = [v for v in analyzed_data if v.get('needs_clarification', False)]\n",
    "        if needs_clarification:\n",
    "            print(f\"   \u26a0\ufe0f  {len(needs_clarification)} variables need clarification\")\n",
    "            for var in needs_clarification:\n",
    "                print(f\"      - {var['original_name']}: {var.get('clarification_question', 'Unknown')}\")\n",
    "        \n",
    "        # Step 3: Ontology mapping and documentation\n",
    "        print(\"\\n\ud83c\udfe5 Step 3: Ontology Mapping & Documentation...\")\n",
    "        for i, var_data in enumerate(analyzed_data, 1):\n",
    "            print(f\"   Processing {i}/{len(analyzed_data)}: {var_data.get('variable_name', var_data.get('original_name'))}\")\n",
    "            \n",
    "            # Map to ontologies\n",
    "            ontology_result = self.domain_ontology.map_ontologies(var_data)\n",
    "            \n",
    "            # Enrich with ontology data\n",
    "            enriched_data = {**var_data, **ontology_result}\n",
    "            \n",
    "            # Generate plain language documentation\n",
    "            documentation = self.plain_language.document_variable(enriched_data)\n",
    "            \n",
    "            # Add to review queue\n",
    "            item_id = self.review_queue.add_item(\n",
    "                job_id=job_id,\n",
    "                source_agent=\"PlainLanguageAgent\",\n",
    "                source_data=json.dumps(enriched_data),\n",
    "                generated_content=documentation\n",
    "            )\n",
    "            \n",
    "            if auto_approve:\n",
    "                self.review_queue.approve_item(item_id)\n",
    "        \n",
    "        # Update job status\n",
    "        status = 'Completed' if auto_approve else 'Pending Review'\n",
    "        self.db.execute_update(\n",
    "            \"UPDATE Jobs SET status = ?, updated_at = CURRENT_TIMESTAMP WHERE job_id = ?\",\n",
    "            (status, job_id)\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n\u2713 Processing complete! Job status: {status}\")\n",
    "        return job_id\n",
    "    \n",
    "    def process_with_extended_agents(self, source_data: str, source_file: str = \"input.csv\",\n",
    "                                     auto_approve: bool = False,\n",
    "                                     apply_design_improvement: bool = True,\n",
    "                                     enforce_conventions: bool = True,\n",
    "                                     enable_versioning: bool = True,\n",
    "                                     document_higher_levels: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Enhanced workflow with extended agent capabilities.\n",
    "        \n",
    "        Args:\n",
    "            source_data: The raw data dictionary content\n",
    "            source_file: Name of the source file\n",
    "            auto_approve: If True, automatically approve all generated content\n",
    "            apply_design_improvement: Use DesignImprovementAgent to enhance output\n",
    "            enforce_conventions: Use DataConventionsAgent to ensure standards\n",
    "            enable_versioning: Use VersionControlAgent to track changes\n",
    "            document_higher_levels: Use HigherLevelDocumentationAgent for segments\n",
    "            \n",
    "        Returns:\n",
    "            job_id: The ID of the created job\n",
    "        \"\"\"\n",
    "        job_id = self.create_job(source_file)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"EXTENDED PROCESSING: Job {job_id}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"   Design Improvement: {'ON' if apply_design_improvement else 'OFF'}\")\n",
    "        print(f\"   Convention Enforcement: {'ON' if enforce_conventions else 'OFF'}\")\n",
    "        print(f\"   Version Control: {'ON' if enable_versioning else 'OFF'}\")\n",
    "        print(f\"   Higher-Level Docs: {'ON' if document_higher_levels else 'OFF'}\")\n",
    "        \n",
    "        # Step 1: Parse data\n",
    "        print(\"\\n\ud83d\udcca Step 1: Parsing Data...\")\n",
    "        parsed_data = self.data_parser.parse_csv(source_data)\n",
    "        print(f\"   \u2713 Parsed {len(parsed_data)} variables\")\n",
    "        \n",
    "        # Step 2: Technical analysis with conventions\n",
    "        print(\"\\n\ud83d\udd2c Step 2: Technical Analysis...\")\n",
    "        analyzed_data = self.technical_analyzer.analyze(parsed_data)\n",
    "        print(f\"   \u2713 Analyzed {len(analyzed_data)} variables\")\n",
    "        \n",
    "        # Step 2.5: Enforce data conventions\n",
    "        conventions_data = []\n",
    "        if enforce_conventions:\n",
    "            print(\"\\n\ud83d\udccf Step 2.5: Analyzing Data Conventions...\")\n",
    "            for i, var_data in enumerate(analyzed_data, 1):\n",
    "                var_name = var_data.get('variable_name', var_data.get('original_name', 'Unknown'))\n",
    "                print(f\"   Checking conventions for {i}/{len(analyzed_data)}: {var_name}\")\n",
    "                \n",
    "                conventions_result = self.data_conventions.analyze_conventions(var_data)\n",
    "                conventions_data.append(conventions_result)\n",
    "                \n",
    "                # Merge convention info into analyzed data\n",
    "                var_data['conventions'] = conventions_result\n",
    "                \n",
    "                # Track convention violations\n",
    "                if conventions_result.get('convention_warnings'):\n",
    "                    print(f\"      \u26a0\ufe0f  Warnings: {', '.join(conventions_result['convention_warnings'][:2])}\")\n",
    "            \n",
    "            # Generate conventions glossary\n",
    "            glossary = self.data_conventions.generate_conventions_glossary(analyzed_data)\n",
    "            print(f\"   \u2713 Generated conventions glossary\")\n",
    "            print(f\"      Dominant naming pattern: {glossary.get('dominant_pattern', 'mixed')}\")\n",
    "        \n",
    "        # Step 3: Ontology mapping and documentation\n",
    "        print(\"\\n\ud83c\udfe5 Step 3: Ontology Mapping & Documentation...\")\n",
    "        all_documentation = []\n",
    "        \n",
    "        for i, var_data in enumerate(analyzed_data, 1):\n",
    "            var_name = var_data.get('variable_name', var_data.get('original_name', 'Unknown'))\n",
    "            print(f\"   Processing {i}/{len(analyzed_data)}: {var_name}\")\n",
    "            \n",
    "            # Map to ontologies\n",
    "            ontology_result = self.domain_ontology.map_ontologies(var_data)\n",
    "            enriched_data = {**var_data, **ontology_result}\n",
    "            \n",
    "            # Generate plain language documentation\n",
    "            documentation = self.plain_language.document_variable(enriched_data)\n",
    "            \n",
    "            # Step 3.5: Apply design improvements\n",
    "            if apply_design_improvement:\n",
    "                print(f\"      Improving design...\")\n",
    "                design_result = self.design_improvement.improve_design(documentation)\n",
    "                if design_result.get('improved_content'):\n",
    "                    documentation = design_result['improved_content']\n",
    "                    score_before = design_result.get('design_score', {}).get('before', 0)\n",
    "                    score_after = design_result.get('design_score', {}).get('after', 0)\n",
    "                    print(f\"      Design score: {score_before} \u2192 {score_after}\")\n",
    "            \n",
    "            all_documentation.append(documentation)\n",
    "            \n",
    "            # Step 3.6: Version control\n",
    "            if enable_versioning:\n",
    "                version_result = self.version_control.create_version(\n",
    "                    element_id=var_name,\n",
    "                    element_type=\"variable\",\n",
    "                    content=documentation,\n",
    "                    author=\"system\"\n",
    "                )\n",
    "                if version_result.get('status') == 'success':\n",
    "                    print(f\"      Version: {version_result['new_version']}\")\n",
    "            \n",
    "            # Add to review queue\n",
    "            item_id = self.review_queue.add_item(\n",
    "                job_id=job_id,\n",
    "                source_agent=\"PlainLanguageAgent\",\n",
    "                source_data=json.dumps(enriched_data),\n",
    "                generated_content=documentation\n",
    "            )\n",
    "            \n",
    "            if auto_approve:\n",
    "                self.review_queue.approve_item(item_id)\n",
    "        \n",
    "        # Step 4: Higher-level documentation\n",
    "        if document_higher_levels:\n",
    "            print(\"\\n\ud83d\udcda Step 4: Higher-Level Documentation...\")\n",
    "            \n",
    "            # Identify potential instruments\n",
    "            potential_instruments = self.higher_level_docs.identify_instruments(analyzed_data)\n",
    "            print(f\"   Found {len(potential_instruments)} potential instruments/segments\")\n",
    "            \n",
    "            for inst in potential_instruments:\n",
    "                print(f\"   Documenting: {inst['suggested_name']} ({inst['variable_count']} variables)\")\n",
    "                inst_doc = self.higher_level_docs.document_instrument(inst['variables'])\n",
    "                \n",
    "                # Version the instrument documentation\n",
    "                if enable_versioning:\n",
    "                    self.version_control.create_version(\n",
    "                        element_id=inst['suggested_name'],\n",
    "                        element_type=\"instrument\",\n",
    "                        content=json.dumps(inst_doc),\n",
    "                        author=\"system\"\n",
    "                    )\n",
    "                \n",
    "                # Add instrument documentation to review queue\n",
    "                item_id = self.review_queue.add_item(\n",
    "                    job_id=job_id,\n",
    "                    source_agent=\"HigherLevelDocumentationAgent\",\n",
    "                    source_data=json.dumps(inst),\n",
    "                    generated_content=inst_doc.get('documentation_markdown', str(inst_doc))\n",
    "                )\n",
    "                \n",
    "                if auto_approve:\n",
    "                    self.review_queue.approve_item(item_id)\n",
    "            \n",
    "            # Generate codebook overview\n",
    "            print(\"   Generating codebook overview...\")\n",
    "            overview = self.higher_level_docs.generate_codebook_overview(\n",
    "                analyzed_data,\n",
    "                instruments=[inst.get('documentation', {}) for inst in potential_instruments]\n",
    "            )\n",
    "            print(f\"   \u2713 Generated overview with {len(analyzed_data)} variables\")\n",
    "        \n",
    "        # Update job status\n",
    "        status = 'Completed' if auto_approve else 'Pending Review'\n",
    "        self.db.execute_update(\n",
    "            \"UPDATE Jobs SET status = ?, updated_at = CURRENT_TIMESTAMP WHERE job_id = ?\",\n",
    "            (status, job_id)\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"EXTENDED PROCESSING COMPLETE\")\n",
    "        print(f\"   Job ID: {job_id}\")\n",
    "        print(f\"   Variables processed: {len(analyzed_data)}\")\n",
    "        print(f\"   Status: {status}\")\n",
    "        if enforce_conventions:\n",
    "            print(f\"   Conventions documented: \u2713\")\n",
    "        if enable_versioning:\n",
    "            print(f\"   Versions tracked: \u2713\")\n",
    "        if document_higher_levels:\n",
    "            print(f\"   Higher-level docs: {len(potential_instruments)} instruments\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        return job_id\n",
    "    \n",
    "    def update_documentation(self, element_id: str, new_content: str, \n",
    "                            element_type: str = \"variable\", author: str = \"user\") -> Dict:\n",
    "        \"\"\"\n",
    "        Update documentation for an element with version control.\n",
    "        \n",
    "        Args:\n",
    "            element_id: ID of the element to update\n",
    "            new_content: New documentation content\n",
    "            element_type: Type of element (variable, instrument, segment)\n",
    "            author: Who is making the change\n",
    "            \n",
    "        Returns:\n",
    "            Version control result\n",
    "        \"\"\"\n",
    "        print(f\"Updating {element_type}: {element_id}\")\n",
    "        \n",
    "        # Apply design improvement to new content\n",
    "        print(\"   Applying design improvements...\")\n",
    "        design_result = self.design_improvement.improve_design(new_content)\n",
    "        improved_content = design_result.get('improved_content', new_content)\n",
    "        \n",
    "        # Create new version\n",
    "        version_result = self.version_control.create_version(\n",
    "            element_id=element_id,\n",
    "            element_type=element_type,\n",
    "            content=improved_content,\n",
    "            author=author\n",
    "        )\n",
    "        \n",
    "        if version_result.get('status') == 'success':\n",
    "            print(f\"   \u2713 Created version {version_result['new_version']}\")\n",
    "        else:\n",
    "            print(f\"   \u26a0\ufe0f  {version_result.get('message', 'Unknown status')}\")\n",
    "        \n",
    "        return version_result\n",
    "    \n",
    "    def get_element_history(self, element_id: str) -> List[Dict]:\n",
    "        \"\"\"Get version history for a documentation element.\"\"\"\n",
    "        return self.version_control.get_version_history(element_id)\n",
    "    \n",
    "    def rollback_element(self, element_id: str, target_version: str) -> Dict:\n",
    "        \"\"\"Rollback an element to a previous version.\"\"\"\n",
    "        return self.version_control.rollback_to_version(element_id, target_version)\n",
    "    \n",
    "    def finalize_documentation(self, job_id: str, output_file: str = \"documentation.md\") -> str:\n",
    "        \"\"\"Assemble and save final documentation.\"\"\"\n",
    "        print(f\"\\n\ud83d\udcdd Assembling final documentation for job {job_id}...\")\n",
    "        final_doc = self.assembler.assemble(job_id)\n",
    "        \n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(final_doc)\n",
    "        \n",
    "        print(f\"\u2713 Documentation saved to {output_file}\")\n",
    "        logger.info(f\"Final documentation saved: {output_file}\")\n",
    "        return final_doc\n",
    "    \n",
    "    def validate_pipeline_outputs(self, job_id: str, parsed_data: List[Dict] = None,\n",
    "                                   analyzed_data: List[Dict] = None,\n",
    "                                   enriched_data: List[Dict] = None,\n",
    "                                   documentation: List[str] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Run validation checks on pipeline outputs.\n",
    "        \n",
    "        Args:\n",
    "            job_id: The job ID for tracking\n",
    "            parsed_data: Output from DataParserAgent\n",
    "            analyzed_data: Output from TechnicalAnalyzerAgent\n",
    "            enriched_data: Output with ontology mappings\n",
    "            documentation: Generated documentation\n",
    "            \n",
    "        Returns:\n",
    "            Comprehensive validation report\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"VALIDATION: Job {job_id}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        validations = []\n",
    "        \n",
    "        # Validate parsed data\n",
    "        if parsed_data:\n",
    "            print(\"\\n\ud83d\udd0d Validating parsed data...\")\n",
    "            parsed_validation = self.validation.validate_parsed_data(parsed_data)\n",
    "            validations.append(parsed_validation)\n",
    "            score = parsed_validation.get('overall_score', 0)\n",
    "            passed = \"\u2713\" if parsed_validation.get('validation_passed', False) else \"\u2717\"\n",
    "            print(f\"   {passed} Parsed data validation: {score}/100\")\n",
    "            issues = len(parsed_validation.get('issues_found', []))\n",
    "            if issues > 0:\n",
    "                print(f\"      Issues found: {issues}\")\n",
    "        \n",
    "        # Validate technical analysis\n",
    "        if analyzed_data:\n",
    "            print(\"\\n\ud83d\udd0d Validating technical analysis...\")\n",
    "            analysis_validation = self.validation.validate_technical_analysis(analyzed_data)\n",
    "            validations.append(analysis_validation)\n",
    "            score = analysis_validation.get('overall_score', 0)\n",
    "            passed = \"\u2713\" if analysis_validation.get('validation_passed', False) else \"\u2717\"\n",
    "            print(f\"   {passed} Technical analysis validation: {score}/100\")\n",
    "            issues = len(analysis_validation.get('issues_found', []))\n",
    "            if issues > 0:\n",
    "                print(f\"      Issues found: {issues}\")\n",
    "        \n",
    "        # Validate ontology mappings\n",
    "        if enriched_data:\n",
    "            print(\"\\n\ud83d\udd0d Validating ontology mappings...\")\n",
    "            ontology_validation = self.validation.validate_ontology_mappings(enriched_data)\n",
    "            validations.append(ontology_validation)\n",
    "            score = ontology_validation.get('overall_score', 0)\n",
    "            passed = \"\u2713\" if ontology_validation.get('validation_passed', False) else \"\u2717\"\n",
    "            print(f\"   {passed} Ontology mappings validation: {score}/100\")\n",
    "            issues = len(ontology_validation.get('issues_found', []))\n",
    "            if issues > 0:\n",
    "                print(f\"      Issues found: {issues}\")\n",
    "        \n",
    "        # Validate documentation\n",
    "        if documentation:\n",
    "            print(\"\\n\ud83d\udd0d Validating documentation...\")\n",
    "            for idx, doc in enumerate(documentation[:5]):  # Validate first 5\n",
    "                doc_validation = self.validation.validate_documentation(doc)\n",
    "                validations.append(doc_validation)\n",
    "            avg_score = sum(v.get('overall_score', 0) for v in validations[-len(documentation[:5]):]) / min(5, len(documentation))\n",
    "            print(f\"   Documentation validation avg score: {avg_score:.1f}/100\")\n",
    "        \n",
    "        # Cross-validate all agent outputs\n",
    "        if any([parsed_data, analyzed_data, enriched_data]):\n",
    "            print(\"\\n\ud83d\udd0d Cross-validating agent outputs...\")\n",
    "            cross_validation = self.validation.cross_validate_agents({\n",
    "                'parsed_data': parsed_data[:5] if parsed_data else [],\n",
    "                'analyzed_data': analyzed_data[:5] if analyzed_data else [],\n",
    "                'enriched_data': enriched_data[:5] if enriched_data else []\n",
    "            })\n",
    "            validations.append(cross_validation)\n",
    "            score = cross_validation.get('overall_score', 0)\n",
    "            passed = \"\u2713\" if cross_validation.get('validation_passed', False) else \"\u2717\"\n",
    "            print(f\"   {passed} Cross-validation score: {score}/100\")\n",
    "        \n",
    "        # Generate validation report\n",
    "        print(\"\\n\ud83d\udccb Generating validation report...\")\n",
    "        report = self.validation.generate_validation_report(validations)\n",
    "        \n",
    "        # Calculate overall results\n",
    "        total_passed = sum(1 for v in validations if v.get('validation_passed', False))\n",
    "        overall_score = sum(v.get('overall_score', 0) for v in validations) / max(len(validations), 1)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"VALIDATION COMPLETE\")\n",
    "        print(f\"   Total checks: {len(validations)}\")\n",
    "        print(f\"   Passed: {total_passed}/{len(validations)}\")\n",
    "        print(f\"   Overall score: {overall_score:.1f}/100\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        return {\n",
    "            'job_id': job_id,\n",
    "            'total_validations': len(validations),\n",
    "            'passed': total_passed,\n",
    "            'overall_score': overall_score,\n",
    "            'individual_validations': validations,\n",
    "            'report': report\n",
    "        }\n",
    "    \n",
    "    def process_with_validation(self, source_data: str, source_file: str = \"input.csv\",\n",
    "                                auto_approve: bool = False,\n",
    "                                apply_design_improvement: bool = True,\n",
    "                                enforce_conventions: bool = True,\n",
    "                                enable_versioning: bool = True,\n",
    "                                document_higher_levels: bool = True,\n",
    "                                enable_validation: bool = True) -> Tuple[str, Dict]:\n",
    "        \"\"\"\n",
    "        Full pipeline with validation at each stage.\n",
    "        \n",
    "        Args:\n",
    "            source_data: The raw data dictionary content\n",
    "            source_file: Name of the source file\n",
    "            auto_approve: If True, automatically approve all generated content\n",
    "            apply_design_improvement: Use DesignImprovementAgent\n",
    "            enforce_conventions: Use DataConventionsAgent\n",
    "            enable_versioning: Use VersionControlAgent\n",
    "            document_higher_levels: Use HigherLevelDocumentationAgent\n",
    "            enable_validation: Use ValidationAgent to validate outputs\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (job_id, validation_results)\n",
    "        \"\"\"\n",
    "        job_id = self.create_job(source_file)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"VALIDATED PROCESSING: Job {job_id}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"   Validation: {'ON' if enable_validation else 'OFF'}\")\n",
    "        \n",
    "        # Step 1: Parse data\n",
    "        print(\"\\n\ud83d\udcca Step 1: Parsing Data...\")\n",
    "        parsed_data = self.data_parser.parse_csv(source_data)\n",
    "        print(f\"   \u2713 Parsed {len(parsed_data)} variables\")\n",
    "        \n",
    "        # Step 2: Technical analysis\n",
    "        print(\"\\n\ud83d\udd2c Step 2: Technical Analysis...\")\n",
    "        analyzed_data = self.technical_analyzer.analyze(parsed_data)\n",
    "        print(f\"   \u2713 Analyzed {len(analyzed_data)} variables\")\n",
    "        \n",
    "        # Step 3: Ontology mapping\n",
    "        print(\"\\n\ud83c\udfe5 Step 3: Ontology Mapping & Documentation...\")\n",
    "        enriched_data = []\n",
    "        all_documentation = []\n",
    "        \n",
    "        for i, var_data in enumerate(analyzed_data, 1):\n",
    "            var_name = var_data.get('variable_name', var_data.get('original_name', 'Unknown'))\n",
    "            print(f\"   Processing {i}/{len(analyzed_data)}: {var_name}\")\n",
    "            \n",
    "            ontology_result = self.domain_ontology.map_ontologies(var_data)\n",
    "            enriched = {**var_data, **ontology_result}\n",
    "            enriched_data.append(enriched)\n",
    "            \n",
    "            documentation = self.plain_language.document_variable(enriched)\n",
    "            \n",
    "            if apply_design_improvement:\n",
    "                design_result = self.design_improvement.improve_design(documentation)\n",
    "                if design_result.get('improved_content'):\n",
    "                    documentation = design_result['improved_content']\n",
    "            \n",
    "            all_documentation.append(documentation)\n",
    "            \n",
    "            item_id = self.review_queue.add_item(\n",
    "                job_id=job_id,\n",
    "                source_agent=\"PlainLanguageAgent\",\n",
    "                source_data=json.dumps(enriched),\n",
    "                generated_content=documentation\n",
    "            )\n",
    "            \n",
    "            if auto_approve:\n",
    "                self.review_queue.approve_item(item_id)\n",
    "        \n",
    "        # Step 4: Validation\n",
    "        validation_results = {}\n",
    "        if enable_validation:\n",
    "            validation_results = self.validate_pipeline_outputs(\n",
    "                job_id=job_id,\n",
    "                parsed_data=parsed_data,\n",
    "                analyzed_data=analyzed_data,\n",
    "                enriched_data=enriched_data,\n",
    "                documentation=all_documentation\n",
    "            )\n",
    "            \n",
    "            # Check if validation passed\n",
    "            if validation_results.get('overall_score', 0) < 70:\n",
    "                print(\"\\n\u26a0\ufe0f  WARNING: Validation score below threshold (70)\")\n",
    "                print(\"   Consider reviewing the issues before approving\")\n",
    "        \n",
    "        # Update job status\n",
    "        status = 'Completed' if auto_approve else 'Pending Review'\n",
    "        self.db.execute_update(\n",
    "            \"UPDATE Jobs SET status = ?, updated_at = CURRENT_TIMESTAMP WHERE job_id = ?\",\n",
    "            (status, job_id)\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n\u2713 Processing complete! Job status: {status}\")\n",
    "        return job_id, validation_results\n",
    "\n",
    "print(\"\u2713 Orchestrator class defined with extended agent support\")\n",
    "print(\"   New methods:\")\n",
    "print(\"   - process_with_extended_agents(): Full pipeline with all agents\")\n",
    "print(\"   - process_with_validation(): Pipeline with validation checks\")\n",
    "print(\"   - validate_pipeline_outputs(): Validate outputs for quality\")\n",
    "print(\"   - update_documentation(): Update with version control\")\n",
    "print(\"   - get_element_history(): View version history\")\n",
    "print(\"   - rollback_element(): Revert to previous versions\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Orchestrator - Agent Workflow Management",
    "",
    "The Orchestrator manages data flow through the agent pipeline and coordinates HITL workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class Orchestrator:\n    \"\"\"Manages the workflow of agents and coordinates the documentation pipeline.\"\"\"\n    \n    def __init__(self, db_manager: DatabaseManager, api_config: APIConfig = None):\n        self.db = db_manager\n        self.config = api_config or API_CONFIG\n        self.snippet_manager = SnippetManager(db_manager)\n        self.review_queue = ReviewQueueManager(db_manager)\n        \n        # Initialize agents with configuration\n        self.data_parser = DataParserAgent(config=self.config)\n        self.technical_analyzer = TechnicalAnalyzerAgent(config=self.config)\n        self.domain_ontology = DomainOntologyAgent(config=self.config)\n        self.plain_language = PlainLanguageAgent(config=self.config)\n        self.assembler = DocumentationAssemblerAgent(self.review_queue, config=self.config)\n        \n        logger.info(f\"Orchestrator initialized with {self.config.requests_per_minute} req/min limit\")\n        print(f\"\u2713 Orchestrator initialized with {self.config.requests_per_minute} req/min limit\")\n    \n    def create_job(self, source_file: str) -> str:\n        \"\"\"Create a new documentation job.\"\"\"\n        job_id = hashlib.md5(f\"{source_file}_{datetime.now().isoformat()}\".encode()).hexdigest()[:12]\n        query = \"INSERT INTO Jobs (job_id, source_file, status) VALUES (?, ?, 'Running')\"\n        self.db.execute_update(query, (job_id, source_file))\n        logger.info(f\"Created job {job_id} for {source_file}\")\n        return job_id\n    \n    def process_data_dictionary(self, source_data: str, source_file: str = \"input.csv\",\n                                auto_approve: bool = False) -> str:\n        \"\"\"\n        Main workflow: Process a data dictionary through the agent pipeline.\n        \n        Args:\n            source_data: The raw data dictionary content\n            source_file: Name of the source file\n            auto_approve: If True, automatically approve all generated content\n            \n        Returns:\n            job_id: The ID of the created job\n        \"\"\"\n        job_id = self.create_job(source_file)\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"Processing Job: {job_id}\")\n        print(f\"{'='*60}\")\n        \n        # Step 1: Parse data\n        print(\"\\n\ud83d\udcca Step 1: Parsing Data...\")\n        parsed_data = self.data_parser.parse_csv(source_data)\n        print(f\"   \u2713 Parsed {len(parsed_data)} variables\")\n        \n        # Step 2: Technical analysis\n        print(\"\\n\ud83d\udd2c Step 2: Technical Analysis...\")\n        analyzed_data = self.technical_analyzer.analyze(parsed_data)\n        print(f\"   \u2713 Analyzed {len(analyzed_data)} variables\")\n        \n        # Check for clarifications needed\n        needs_clarification = [v for v in analyzed_data if v.get('needs_clarification', False)]\n        if needs_clarification:\n            print(f\"   \u26a0\ufe0f  {len(needs_clarification)} variables need clarification\")\n            for var in needs_clarification:\n                print(f\"      - {var['original_name']}: {var.get('clarification_question', 'Unknown')}\")\n        \n        # Step 3: Ontology mapping and documentation\n        print(\"\\n\ud83c\udfe5 Step 3: Ontology Mapping & Documentation...\")\n        for i, var_data in enumerate(analyzed_data, 1):\n            print(f\"   Processing {i}/{len(analyzed_data)}: {var_data.get('variable_name', var_data.get('original_name'))}\")\n            \n            # Map to ontologies\n            ontology_result = self.domain_ontology.map_ontologies(var_data)\n            \n            # Enrich with ontology data\n            enriched_data = {**var_data, **ontology_result}\n            \n            # Generate plain language documentation\n            documentation = self.plain_language.document_variable(enriched_data)\n            \n            # Add to review queue\n            item_id = self.review_queue.add_item(\n                job_id=job_id,\n                source_agent=\"PlainLanguageAgent\",\n                source_data=json.dumps(enriched_data),\n                generated_content=documentation\n            )\n            \n            if auto_approve:\n                self.review_queue.approve_item(item_id)\n        \n        # Update job status\n        status = 'Completed' if auto_approve else 'Pending Review'\n        self.db.execute_update(\n            \"UPDATE Jobs SET status = ?, updated_at = CURRENT_TIMESTAMP WHERE job_id = ?\",\n            (status, job_id)\n        )\n        \n        print(f\"\\n\u2713 Processing complete! Job status: {status}\")\n        return job_id\n    \n    def finalize_documentation(self, job_id: str, output_file: str = \"documentation.md\") -> str:\n        \"\"\"Assemble and save final documentation.\"\"\"\n        print(f\"\\n\ud83d\udcdd Assembling final documentation for job {job_id}...\")\n        final_doc = self.assembler.assemble(job_id)\n        \n        with open(output_file, 'w') as f:\n            f.write(final_doc)\n        \n        print(f\"\u2713 Documentation saved to {output_file}\")\n        logger.info(f\"Final documentation saved: {output_file}\")\n        return final_doc\n\nprint(\"\u2713 Orchestrator class defined with complete pipeline support\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 7.1 Batch Processing for Large Codebooks\n\nProcess large data dictionaries in batches to avoid context limits and manage API rate limiting effectively.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "@dataclass\nclass BatchConfig:\n    \"\"\"Configuration for batch processing of large codebooks.\"\"\"\n    batch_size: int = 10  # Default number of variables per batch\n    min_batch_size: int = 3  # Minimum batch size to avoid splitting too small\n    group_related_variables: bool = True  # Try to keep related variables together\n    progress_tracking: bool = True  # Show progress during processing\n\n@dataclass\nclass BatchResult:\n    \"\"\"Result of processing a single batch.\"\"\"\n    batch_id: int\n    variables_processed: int\n    success: bool\n    error_message: Optional[str] = None\n    \nclass BatchProcessor:\n    \"\"\"\n    Handles batch processing of large data dictionaries.\n    \n    Features:\n    - Automatic chunking with configurable batch size\n    - Sensitivity to not splitting related variables between chunks\n    - Progress tracking with resume capability\n    \"\"\"\n    \n    def __init__(self, orchestrator: Orchestrator, config: BatchConfig = None):\n        self.orchestrator = orchestrator\n        self.config = config or BatchConfig()\n        self.logger = logging.getLogger('ADE.BatchProcessor')\n    \n    def _identify_variable_groups(self, parsed_data: List[Dict]) -> List[List[int]]:\n        \"\"\"\n        Identify groups of related variables that should stay together.\n        \n        Groups variables by common prefixes (e.g., bp_systolic, bp_diastolic)\n        or related semantic meaning.\n        \"\"\"\n        if not self.config.group_related_variables:\n            return [[i] for i in range(len(parsed_data))]\n        \n        groups = []\n        used_indices = set()\n        \n        # Group by common prefixes\n        for i, var in enumerate(parsed_data):\n            if i in used_indices:\n                continue\n            \n            var_name = var.get('original_name', var.get('Variable Name', '')).lower()\n            if not var_name:\n                groups.append([i])\n                used_indices.add(i)\n                continue\n            \n            # Extract prefix (e.g., \"bp\" from \"bp_systolic\")\n            parts = var_name.replace('-', '_').split('_')\n            if len(parts) > 1:\n                prefix = parts[0]\n                group = [i]\n                used_indices.add(i)\n                \n                # Find other variables with same prefix\n                for j, other_var in enumerate(parsed_data):\n                    if j in used_indices:\n                        continue\n                    other_name = other_var.get('original_name', other_var.get('Variable Name', '')).lower()\n                    if other_name.startswith(prefix + '_') or other_name.startswith(prefix + '-'):\n                        group.append(j)\n                        used_indices.add(j)\n                \n                groups.append(group)\n            else:\n                groups.append([i])\n                used_indices.add(i)\n        \n        return groups\n    \n    def _create_batches(self, parsed_data: List[Dict]) -> List[List[Dict]]:\n        \"\"\"\n        Create batches of variables, respecting group boundaries.\n        \n        Returns a list of batches, where each batch is a list of variable dicts.\n        \"\"\"\n        groups = self._identify_variable_groups(parsed_data)\n        batches = []\n        current_batch = []\n        current_batch_size = 0\n        \n        for group_indices in groups:\n            group_size = len(group_indices)\n            group_vars = [parsed_data[i] for i in group_indices]\n            \n            # If adding this group would exceed batch size\n            if current_batch_size + group_size > self.config.batch_size:\n                # If current batch has something, save it\n                if current_batch and current_batch_size >= self.config.min_batch_size:\n                    batches.append(current_batch)\n                    current_batch = group_vars\n                    current_batch_size = group_size\n                elif current_batch:\n                    # Current batch too small, add group anyway\n                    current_batch.extend(group_vars)\n                    current_batch_size += group_size\n                else:\n                    # No current batch, start with this group\n                    current_batch = group_vars\n                    current_batch_size = group_size\n            else:\n                current_batch.extend(group_vars)\n                current_batch_size += group_size\n        \n        # Add remaining batch\n        if current_batch:\n            batches.append(current_batch)\n        \n        return batches\n    \n    def process_large_codebook(self, source_data: str, source_file: str = \"input.csv\",\n                               auto_approve: bool = False) -> Tuple[str, List[BatchResult]]:\n        \"\"\"\n        Process a large data dictionary in batches.\n        \n        Args:\n            source_data: The raw data dictionary content\n            source_file: Name of the source file\n            auto_approve: If True, automatically approve all generated content\n            \n        Returns:\n            Tuple of (job_id, list of batch results)\n        \"\"\"\n        # Create job\n        job_id = self.orchestrator.create_job(source_file)\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"BATCH PROCESSING: Job {job_id}\")\n        print(f\"{'='*60}\")\n        \n        # Step 1: Parse all data first\n        print(\"\\n\ud83d\udcca Step 1: Parsing entire data dictionary...\")\n        parsed_data = self.orchestrator.data_parser.parse_csv(source_data)\n        total_variables = len(parsed_data)\n        print(f\"   \u2713 Parsed {total_variables} variables total\")\n        \n        # Step 2: Create batches\n        print(f\"\\n\ud83d\udce6 Step 2: Creating batches (target size: {self.config.batch_size})...\")\n        batches = self._create_batches(parsed_data)\n        num_batches = len(batches)\n        print(f\"   \u2713 Created {num_batches} batches\")\n        for i, batch in enumerate(batches, 1):\n            var_names = [v.get('original_name', v.get('Variable Name', 'Unknown'))[:20] for v in batch]\n            print(f\"      Batch {i}: {len(batch)} variables - {', '.join(var_names[:3])}{'...' if len(var_names) > 3 else ''}\")\n        \n        # Step 3: Process each batch\n        results = []\n        all_analyzed_data = []\n        \n        print(f\"\\n\ud83d\udd2c Step 3: Processing batches...\")\n        for batch_id, batch_vars in enumerate(batches, 1):\n            if self.config.progress_tracking:\n                print(f\"\\n   --- Batch {batch_id}/{num_batches} ({len(batch_vars)} variables) ---\")\n            \n            try:\n                # Technical analysis for this batch\n                print(f\"   Analyzing batch {batch_id}...\")\n                analyzed_batch = self.orchestrator.technical_analyzer.analyze(batch_vars)\n                all_analyzed_data.extend(analyzed_batch)\n                \n                # Process ontology and documentation for each variable in batch\n                for i, var_data in enumerate(analyzed_batch, 1):\n                    var_name = var_data.get('variable_name', var_data.get('original_name', 'Unknown'))\n                    if self.config.progress_tracking:\n                        print(f\"      {i}/{len(analyzed_batch)}: {var_name}\")\n                    \n                    # Map to ontologies\n                    ontology_result = self.orchestrator.domain_ontology.map_ontologies(var_data)\n                    enriched_data = {**var_data, **ontology_result}\n                    \n                    # Generate documentation\n                    documentation = self.orchestrator.plain_language.document_variable(enriched_data)\n                    \n                    # Add to review queue\n                    item_id = self.orchestrator.review_queue.add_item(\n                        job_id=job_id,\n                        source_agent=\"PlainLanguageAgent\",\n                        source_data=json.dumps(enriched_data),\n                        generated_content=documentation\n                    )\n                    \n                    if auto_approve:\n                        self.orchestrator.review_queue.approve_item(item_id)\n                \n                results.append(BatchResult(\n                    batch_id=batch_id,\n                    variables_processed=len(batch_vars),\n                    success=True\n                ))\n                print(f\"   \u2713 Batch {batch_id} complete\")\n                \n            except Exception as e:\n                error_msg = str(e)\n                self.logger.error(f\"Batch {batch_id} failed: {error_msg}\")\n                results.append(BatchResult(\n                    batch_id=batch_id,\n                    variables_processed=0,\n                    success=False,\n                    error_message=error_msg\n                ))\n                print(f\"   \u2717 Batch {batch_id} failed: {error_msg}\")\n        \n        # Update job status\n        successful_batches = sum(1 for r in results if r.success)\n        if successful_batches == num_batches:\n            status = 'Completed' if auto_approve else 'Pending Review'\n        elif successful_batches > 0:\n            status = 'Paused'  # Partial success\n        else:\n            status = 'Failed'\n        \n        self.orchestrator.db.execute_update(\n            \"UPDATE Jobs SET status = ?, updated_at = CURRENT_TIMESTAMP WHERE job_id = ?\",\n            (status, job_id)\n        )\n        \n        # Summary\n        print(f\"\\n{'='*60}\")\n        print(f\"BATCH PROCESSING SUMMARY\")\n        print(f\"{'='*60}\")\n        print(f\"   Job ID: {job_id}\")\n        print(f\"   Total variables: {total_variables}\")\n        print(f\"   Batches processed: {successful_batches}/{num_batches}\")\n        print(f\"   Variables documented: {sum(r.variables_processed for r in results if r.success)}\")\n        print(f\"   Status: {status}\")\n        \n        if not auto_approve:\n            print(f\"\\n   \u26a0\ufe0f  Items awaiting manual review in queue\")\n        \n        return job_id, results\n\n# Example configuration for different scenarios\nSMALL_CODEBOOK_CONFIG = BatchConfig(batch_size=5, min_batch_size=2)\nMEDIUM_CODEBOOK_CONFIG = BatchConfig(batch_size=10, min_batch_size=3)\nLARGE_CODEBOOK_CONFIG = BatchConfig(batch_size=20, min_batch_size=5)\n\nprint(\"\u2713 BatchProcessor loaded for large codebook handling\")\nprint(f\"   - Default batch size: {BatchConfig().batch_size}\")\nprint(f\"   - Groups related variables: {BatchConfig().group_related_variables}\")\nprint(f\"   - Available configs: SMALL_CODEBOOK_CONFIG, MEDIUM_CODEBOOK_CONFIG, LARGE_CODEBOOK_CONFIG\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Example Data Dictionaries",
    "",
    "Sample healthcare data dictionaries for testing the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic diabetes study example",
    "sample_data_dictionary = \"\"\"Variable Name,Field Type,Field Label,Choices,Notes",
    "patient_id,text,Patient ID,,Unique identifier",
    "age,integer,Age (years),,Age at enrollment",
    "sex,radio,Biological Sex,\"1, Male | 2, Female | 3, Other\",",
    "bp_systolic,integer,Systolic Blood Pressure (mmHg),,",
    "bp_diastolic,integer,Diastolic Blood Pressure (mmHg),,",
    "diagnosis_date,date,Diagnosis Date,,Date of primary diagnosis",
    "hba1c,decimal,Hemoglobin A1c (%),,Glycated hemoglobin",
    "\"\"\"",
    "",
    "# EHR example",
    "ehr_data_dictionary = \"\"\"Variable Name,Field Type,Field Label,Choices,Notes",
    "mrn,text,Medical Record Number,,Unique patient identifier",
    "encounter_id,text,Encounter ID,,Unique visit identifier",
    "visit_date,date,Visit Date,,Date of clinical encounter",
    "chief_complaint,text,Chief Complaint,,Primary reason for visit",
    "dx_code,text,Diagnosis Code (ICD-10),,Primary diagnosis",
    "bp_systolic,integer,Systolic BP (mmHg),,\"70-250, sitting position\"",
    "bp_diastolic,integer,Diastolic BP (mmHg),,\"40-150, sitting position\"",
    "heart_rate,integer,Heart Rate (bpm),,\"40-200\"",
    "temperature,decimal,Temperature (F),,\"95.0-106.0\"",
    "respiratory_rate,integer,Respiratory Rate (breaths/min),,\"8-40\"",
    "oxygen_sat,integer,Oxygen Saturation (%),,\"70-100, room air\"",
    "bmi,decimal,Body Mass Index,,Calculated from height/weight",
    "smoking_status,radio,Smoking Status,\"0, Never | 1, Former | 2, Current\",From social history",
    "medication_count,integer,Number of Active Medications,,Count of current prescriptions",
    "lab_ordered,yesno,Labs Ordered,\"0, No | 1, Yes\",Any lab tests ordered this visit",
    "\"\"\"",
    "",
    "print(\"\u2713 Sample data dictionaries loaded\")",
    "print(f\"   - Basic diabetes study: 7 variables\")",
    "print(f\"   - EHR example: 15 variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Usage Demonstration",
    "",
    "Initialize the orchestrator and process a data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize orchestrator\norchestrator = Orchestrator(db)\n\n# Create context snippets for better agent performance\nprint(\"\\nCreating context snippets...\")\n\ndef create_or_update_snippet(name: str, snippet_type: SnippetType, content: str, metadata: Optional[Dict] = None):\n    existing_snippet = orchestrator.snippet_manager.get_snippet_by_name(name)\n    if existing_snippet:\n        orchestrator.snippet_manager.update_snippet(existing_snippet.snippet_id, content=content, metadata=metadata)\n        print(f\"   Updated snippet '{name}'\")\n    else:\n        orchestrator.snippet_manager.create_snippet(name, snippet_type, content, metadata)\n        print(f\"   Created snippet '{name}'\")\n\n# OMOP mapping instructions\ncreate_or_update_snippet(\n    name=\"OMOP_Mapping_Instructions\",\n    snippet_type=SnippetType.INSTRUCTION,\n    content=\"\"\"When mapping to OMOP CDM:\n- Blood pressure: OMOP concept_id 3004249 (Systolic), 3012888 (Diastolic)\n- HbA1c: OMOP concept_id 3004410\n- Age: Integer in years\n- Sex: OMOP gender concepts 8507 (Male), 8532 (Female)\"\"\")\n\n# Project design notes\ncreate_or_update_snippet(\n    name=\"Project_Design_Notes\",\n    snippet_type=SnippetType.DESIGN,\n    content=\"\"\"Diabetes research study collecting baseline clinical measurements.\nAll measurements follow standard clinical protocols. Blood pressure measured in sitting position after 5 minutes rest. HbA1c measured using DCCT-aligned assay.\"\"\")\n\n# Inject snippets into agents\nsnippets = orchestrator.snippet_manager.list_snippets()\norchestrator.domain_ontology.inject_snippets(snippets)\norchestrator.plain_language.inject_snippets(snippets)\nprint(f\"\\n\u2713 Injected {len(snippets)} snippets into agent context\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the data dictionary",
    "# Set AUTO_APPROVE_MODE = True for testing, False for manual review",
    "AUTO_APPROVE_MODE = True",
    "",
    "job_id = orchestrator.process_data_dictionary(",
    "    source_data=sample_data_dictionary,",
    "    source_file=\"diabetes_study_data_dictionary.csv\",",
    "    auto_approve=AUTO_APPROVE_MODE",
    ")",
    "",
    "print(f\"\\n{'='*60}\")",
    "print(f\"Job ID: {job_id}\")",
    "print(f\"Auto-approve mode: {'ENABLED' if AUTO_APPROVE_MODE else 'DISABLED'}\")",
    "print(f\"{'='*60}\")",
    "",
    "if AUTO_APPROVE_MODE:",
    "    print(\"\\n\u2713 All items automatically approved\")",
    "    print(\"   Run next cell to generate final documentation\")",
    "else:",
    "    print(\"\\n\u26a0\ufe0f  Items awaiting manual review\")",
    "    print(\"   Use review queue to approve/reject items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final documentation",
    "final_documentation = orchestrator.finalize_documentation(",
    "    job_id=job_id,",
    "    output_file=\"healthcare_data_documentation.md\"",
    ")",
    "",
    "print(\"\\n=== Final Documentation Preview (first 2000 chars) ===\")",
    "print(final_documentation[:2000])",
    "if len(final_documentation) > 2000:",
    "    print(\"\\n... [truncated]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.5. Validation Testing and Troubleshooting\n",
    "\n",
    "This section provides comprehensive test methods for validating the ValidationAgent and troubleshooting common issues in the documentation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test Fixtures and Sample Data for Validation Testing",
    "",
    "class ValidationTestFixtures:",
    "    \"\"\"Test fixtures for ValidationAgent testing.\"\"\"",
    "    ",
    "    @staticmethod",
    "    def get_valid_parsed_data():",
    "        \"\"\"Returns well-formed parsed data for testing.\"\"\"",
    "        return [",
    "            {",
    "                \"variable_name\": \"patient_id\",",
    "                \"field_type\": \"text\",",
    "                \"field_label\": \"Patient Identifier\",",
    "                \"choices\": \"\",",
    "                \"validation\": \"^[A-Z]{2}[0-9]{6}$\",",
    "                \"required\": True,",
    "                \"description\": \"Unique identifier for each patient\"",
    "            },",
    "            {",
    "                \"variable_name\": \"age_years\",",
    "                \"field_type\": \"integer\",",
    "                \"field_label\": \"Age in Years\",",
    "                \"choices\": \"\",",
    "                \"validation\": \"range(0, 120)\",",
    "                \"required\": True,",
    "                \"description\": \"Patient age at enrollment\"",
    "            },",
    "            {",
    "                \"variable_name\": \"diabetes_type\",",
    "                \"field_type\": \"radio\",",
    "                \"field_label\": \"Type of Diabetes\",",
    "                \"choices\": \"1, Type 1 | 2, Type 2 | 3, Gestational | 4, Other\",",
    "                \"validation\": \"\",",
    "                \"required\": True,",
    "                \"description\": \"Classification of diabetes diagnosis\"",
    "            }",
    "        ]",
    "    ",
    "    @staticmethod",
    "    def get_malformed_parsed_data():",
    "        \"\"\"Returns malformed parsed data to test error handling.\"\"\"",
    "        return [",
    "            {",
    "                \"variable_name\": \"\",  # Empty name",
    "                \"field_type\": \"text\",",
    "                \"field_label\": \"Bad Variable\"",
    "                # Missing required fields",
    "            },",
    "            {",
    "                \"variable_name\": \"123_invalid\",  # Invalid naming",
    "                \"field_type\": \"unknown_type\",  # Invalid type",
    "                \"field_label\": \"\",  # Empty label",
    "            }",
    "        ]",
    "    ",
    "    @staticmethod",
    "    def get_valid_technical_analysis():",
    "        \"\"\"Returns well-formed technical analysis data.\"\"\"",
    "        return [",
    "            {",
    "                \"variable_name\": \"patient_id\",",
    "                \"data_type\": \"string\",",
    "                \"constraints\": [\"unique\", \"not_null\", \"pattern_match\"],",
    "                \"relationships\": [],",
    "                \"statistical_properties\": {",
    "                    \"cardinality\": \"high\",",
    "                    \"null_percentage\": 0.0",
    "                },",
    "                \"quality_score\": 95",
    "            },",
    "            {",
    "                \"variable_name\": \"age_years\",",
    "                \"data_type\": \"integer\",",
    "                \"constraints\": [\"range_check\", \"not_null\"],",
    "                \"relationships\": [\"correlates_with_diagnosis_date\"],",
    "                \"statistical_properties\": {",
    "                    \"mean\": 54.2,",
    "                    \"std\": 12.8,",
    "                    \"min\": 18,",
    "                    \"max\": 89",
    "                },",
    "                \"quality_score\": 98",
    "            }",
    "        ]",
    "    ",
    "    @staticmethod",
    "    def get_valid_ontology_mappings():",
    "        \"\"\"Returns well-formed ontology mappings.\"\"\"",
    "        return [",
    "            {",
    "                \"variable_name\": \"patient_id\",",
    "                \"ontology_mappings\": [",
    "                    {",
    "                        \"ontology\": \"SNOMED-CT\",",
    "                        \"code\": \"116154003\",",
    "                        \"term\": \"Patient\",",
    "                        \"confidence\": 0.95",
    "                    }",
    "                ],",
    "                \"semantic_type\": \"identifier\",",
    "                \"domain_context\": \"clinical_research\"",
    "            },",
    "            {",
    "                \"variable_name\": \"diabetes_type\",",
    "                \"ontology_mappings\": [",
    "                    {",
    "                        \"ontology\": \"SNOMED-CT\",",
    "                        \"code\": \"73211009\",",
    "                        \"term\": \"Diabetes mellitus\",",
    "                        \"confidence\": 0.98",
    "                    },",
    "                    {",
    "                        \"ontology\": \"ICD-10\",",
    "                        \"code\": \"E11\",",
    "                        \"term\": \"Type 2 diabetes mellitus\",",
    "                        \"confidence\": 0.92",
    "                    }",
    "                ],",
    "                \"semantic_type\": \"diagnosis_classification\",",
    "                \"domain_context\": \"endocrinology\"",
    "            }",
    "        ]",
    "    ",
    "    @staticmethod",
    "    def get_sample_documentation():",
    "        \"\"\"Returns sample plain language documentation.\"\"\"",
    "        return \"\"\"",
    "        # Patient Demographics Documentation",
    "        ",
    "        ## Overview",
    "        This section describes the core patient identification and demographic variables.",
    "        ",
    "        ## Variable: Patient ID (patient_id)",
    "        **Purpose**: Uniquely identifies each patient in the study.",
    "        **Format**: Two uppercase letters followed by six digits (e.g., AB123456)",
    "        **Data Type**: Text (String)",
    "        **Required**: Yes",
    "        **Example**: CA789012",
    "        ",
    "        ## Variable: Age in Years (age_years)",
    "        **Purpose**: Records the patient's age at the time of enrollment.",
    "        **Valid Range**: 0 to 120 years",
    "        **Data Type**: Integer",
    "        **Required**: Yes",
    "        **Clinical Significance**: Used for age-stratified analysis and eligibility screening.",
    "        \"\"\"",
    "    ",
    "    @staticmethod",
    "    def get_incomplete_documentation():",
    "        \"\"\"Returns incomplete documentation to test validation.\"\"\"",
    "        return \"\"\"",
    "        # Patient Data",
    "        ",
    "        ## patient_id",
    "        This is an ID field.",
    "        ",
    "        ## age_years  ",
    "        Age variable.",
    "        \"\"\"",
    "",
    "print('ValidationTestFixtures loaded successfully')",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit Tests for ValidationAgent Methods"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ValidationAgentTester:",
    "    \"\"\"Comprehensive test suite for ValidationAgent.\"\"\"",
    "    ",
    "    def __init__(self, validation_agent=None):",
    "        self.agent = validation_agent or ValidationAgent()",
    "        self.test_results = []",
    "        self.fixtures = ValidationTestFixtures()",
    "    ",
    "    def _log_result(self, test_name, passed, details=\"\"):",
    "        \"\"\"Log a test result.\"\"\"",
    "        result = {",
    "            \"test_name\": test_name,",
    "            \"passed\": passed,",
    "            \"details\": details,",
    "            \"timestamp\": datetime.now().isoformat()",
    "        }",
    "        self.test_results.append(result)",
    "        status = \"PASS\" if passed else \"FAIL\"",
    "        print(f\"{status}: {test_name}\")",
    "        if details:",
    "            print(f\"   {details}\")",
    "    ",
    "    def test_validate_parsed_data_success(self):",
    "        \"\"\"Test validation of well-formed parsed data.\"\"\"",
    "        try:",
    "            data = self.fixtures.get_valid_parsed_data()",
    "            result = self.agent.validate_parsed_data(data)",
    "            passed = (",
    "                isinstance(result, dict) and",
    "                \"validation_passed\" in result and",
    "                \"overall_score\" in result and",
    "                result.get(\"overall_score\", 0) >= 70",
    "            )",
    "            self._log_result(",
    "                \"validate_parsed_data_success\",",
    "                passed,",
    "                f\"Score: {result.get('overall_score', 'N/A')}, Issues: {len(result.get('issues_found', []))}\"",
    "            )",
    "            return result",
    "        except Exception as e:",
    "            self._log_result(\"validate_parsed_data_success\", False, str(e))",
    "            return None",
    "    ",
    "    def test_validate_parsed_data_malformed(self):",
    "        \"\"\"Test validation catches malformed data.\"\"\"",
    "        try:",
    "            data = self.fixtures.get_malformed_parsed_data()",
    "            result = self.agent.validate_parsed_data(data)",
    "            passed = (",
    "                isinstance(result, dict) and",
    "                len(result.get(\"issues_found\", [])) > 0",
    "            )",
    "            self._log_result(",
    "                \"validate_parsed_data_malformed\",",
    "                passed,",
    "                f\"Found {len(result.get('issues_found', []))} issues as expected\"",
    "            )",
    "            return result",
    "        except Exception as e:",
    "            self._log_result(\"validate_parsed_data_malformed\", False, str(e))",
    "            return None",
    "    ",
    "    def test_validate_technical_analysis(self):",
    "        \"\"\"Test validation of technical analysis data.\"\"\"",
    "        try:",
    "            data = self.fixtures.get_valid_technical_analysis()",
    "            result = self.agent.validate_technical_analysis(data)",
    "            passed = (",
    "                isinstance(result, dict) and",
    "                \"validation_passed\" in result",
    "            )",
    "            self._log_result(",
    "                \"validate_technical_analysis\",",
    "                passed,",
    "                f\"Score: {result.get('overall_score', 'N/A')}\"",
    "            )",
    "            return result",
    "        except Exception as e:",
    "            self._log_result(\"validate_technical_analysis\", False, str(e))",
    "            return None",
    "    ",
    "    def test_validate_ontology_mappings(self):",
    "        \"\"\"Test validation of ontology mappings.\"\"\"",
    "        try:",
    "            data = self.fixtures.get_valid_ontology_mappings()",
    "            result = self.agent.validate_ontology_mappings(data)",
    "            passed = isinstance(result, dict) and \"validation_passed\" in result",
    "            self._log_result(",
    "                \"validate_ontology_mappings\",",
    "                passed,",
    "                f\"Score: {result.get('overall_score', 'N/A')}\"",
    "            )",
    "            return result",
    "        except Exception as e:",
    "            self._log_result(\"validate_ontology_mappings\", False, str(e))",
    "            return None",
    "    ",
    "    def test_validate_documentation_complete(self):",
    "        \"\"\"Test validation of complete documentation.\"\"\"",
    "        try:",
    "            doc = self.fixtures.get_sample_documentation()",
    "            result = self.agent.validate_documentation(doc)",
    "            passed = isinstance(result, dict) and result.get(\"overall_score\", 0) >= 70",
    "            self._log_result(",
    "                \"validate_documentation_complete\",",
    "                passed,",
    "                f\"Score: {result.get('overall_score', 'N/A')}\"",
    "            )",
    "            return result",
    "        except Exception as e:",
    "            self._log_result(\"validate_documentation_complete\", False, str(e))",
    "            return None",
    "    ",
    "    def test_validate_documentation_incomplete(self):",
    "        \"\"\"Test validation catches incomplete documentation.\"\"\"",
    "        try:",
    "            doc = self.fixtures.get_incomplete_documentation()",
    "            result = self.agent.validate_documentation(doc)",
    "            passed = (",
    "                isinstance(result, dict) and",
    "                len(result.get(\"issues_found\", [])) > 0",
    "            )",
    "            self._log_result(",
    "                \"validate_documentation_incomplete\",",
    "                passed,",
    "                f\"Found {len(result.get('issues_found', []))} issues\"",
    "            )",
    "            return result",
    "        except Exception as e:",
    "            self._log_result(\"validate_documentation_incomplete\", False, str(e))",
    "            return None",
    "    ",
    "    def test_cross_validation(self):",
    "        \"\"\"Test cross-validation between agent outputs.\"\"\"",
    "        try:",
    "            agent_outputs = {",
    "                \"parsed_data\": self.fixtures.get_valid_parsed_data(),",
    "                \"technical_analysis\": self.fixtures.get_valid_technical_analysis(),",
    "                \"ontology_mappings\": self.fixtures.get_valid_ontology_mappings(),",
    "                \"documentation\": self.fixtures.get_sample_documentation()",
    "            }",
    "            result = self.agent.cross_validate_agents(agent_outputs)",
    "            passed = isinstance(result, dict) and \"consistency_checks\" in result",
    "            self._log_result(",
    "                \"cross_validation\",",
    "                passed,",
    "                f\"Consistency checks performed\"",
    "            )",
    "            return result",
    "        except Exception as e:",
    "            self._log_result(\"cross_validation\", False, str(e))",
    "            return None",
    "    ",
    "    def test_validation_report_generation(self):",
    "        \"\"\"Test validation report generation.\"\"\"",
    "        try:",
    "            validations = [",
    "                {",
    "                    \"validation_passed\": True,",
    "                    \"overall_score\": 85,",
    "                    \"issues_found\": [",
    "                        {",
    "                            \"severity\": \"warning\",",
    "                            \"category\": \"naming\",",
    "                            \"description\": \"Variable name could be more descriptive\",",
    "                            \"affected_field\": \"var1\",",
    "                            \"suggestion\": \"Use a more descriptive name\"",
    "                        }",
    "                    ],",
    "                    \"recommendations\": [\"Consider adding more documentation\"]",
    "                }",
    "            ]",
    "            report = self.agent.generate_validation_report(validations)",
    "            passed = isinstance(report, str) and len(report) > 50",
    "            self._log_result(",
    "                \"validation_report_generation\",",
    "                passed,",
    "                f\"Generated report with {len(report)} characters\"",
    "            )",
    "            return report",
    "        except Exception as e:",
    "            self._log_result(\"validation_report_generation\", False, str(e))",
    "            return None",
    "    ",
    "    def run_all_tests(self):",
    "        \"\"\"Run all validation tests.\"\"\"",
    "        print(\"=\" * 60)",
    "        print(\"VALIDATION AGENT TEST SUITE\")",
    "        print(\"=\" * 60)",
    "        ",
    "        self.test_validate_parsed_data_success()",
    "        self.test_validate_parsed_data_malformed()",
    "        self.test_validate_technical_analysis()",
    "        self.test_validate_ontology_mappings()",
    "        self.test_validate_documentation_complete()",
    "        self.test_validate_documentation_incomplete()",
    "        self.test_cross_validation()",
    "        self.test_validation_report_generation()",
    "        ",
    "        print(\"\\n\" + \"=\" * 60)",
    "        print(\"TEST SUMMARY\")",
    "        print(\"=\" * 60)",
    "        passed = sum(1 for r in self.test_results if r['passed'])",
    "        total = len(self.test_results)",
    "        print(f\"Passed: {passed}/{total}\")",
    "        print(f\"Success Rate: {(passed/total)*100:.1f}%\")",
    "        ",
    "        if passed < total:",
    "            print(\"\\nFailed Tests:\")",
    "            for r in self.test_results:",
    "                if not r['passed']:",
    "                    print(f\"  - {r['test_name']}: {r['details']}\")",
    "        ",
    "        return self.test_results",
    "",
    "print('ValidationAgentTester loaded successfully')",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troubleshooting Test Methods"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ValidationTroubleshooter:",
    "    \"\"\"Troubleshooting utilities for common validation issues.\"\"\"",
    "    ",
    "    def __init__(self, orchestrator=None):",
    "        self.orchestrator = orchestrator",
    "        self.issues_log = []",
    "    ",
    "    def diagnose_api_connectivity(self):",
    "        \"\"\"Test API connectivity and rate limiting.\"\"\"",
    "        print(\"Diagnosing API Connectivity...\")",
    "        issues = []",
    "        ",
    "        try:",
    "            model = genai.GenerativeModel('gemini-1.5-flash')",
    "            response = model.generate_content('Return only: API_OK')",
    "            ",
    "            if 'API_OK' in response.text:",
    "                print(\"  [PASS] API connection successful\")",
    "            else:",
    "                print(\"  [WARN] API responded but unexpected output\")",
    "                issues.append(\"Unexpected API response format\")",
    "        except Exception as e:",
    "            print(f\"  [FAIL] API connection failed: {e}\")",
    "            issues.append(f\"API connectivity error: {str(e)}\")",
    "        ",
    "        self.issues_log.extend(issues)",
    "        return len(issues) == 0",
    "    ",
    "    def diagnose_database_connection(self, db):",
    "        \"\"\"Test database connectivity and schema.\"\"\"",
    "        print(\"Diagnosing Database Connection...\")",
    "        issues = []",
    "        ",
    "        try:",
    "            cursor = db.conn.cursor()",
    "            cursor.execute('SELECT 1')",
    "            print(\"  [PASS] Database connection active\")",
    "            ",
    "            cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")",
    "            tables = [row[0] for row in cursor.fetchall()]",
    "            required_tables = [\"jobs\", \"validation_results\", \"agent_memory\", \"sessions\"]",
    "            ",
    "            for table in required_tables:",
    "                if table in tables:",
    "                    print(f\"  [PASS] Table '{table}' exists\")",
    "                else:",
    "                    print(f\"  [FAIL] Table '{table}' missing\")",
    "                    issues.append(f\"Missing table: {table}\")",
    "                    ",
    "        except Exception as e:",
    "            print(f\"  [FAIL] Database error: {e}\")",
    "            issues.append(f\"Database error: {str(e)}\")",
    "        ",
    "        self.issues_log.extend(issues)",
    "        return len(issues) == 0",
    "    ",
    "    def diagnose_agent_initialization(self):",
    "        \"\"\"Test that all agents initialize correctly.\"\"\"",
    "        print(\"Diagnosing Agent Initialization...\")",
    "        issues = []",
    "        ",
    "        agents_to_test = [",
    "            (\"DataParserAgent\", DataParserAgent),",
    "            (\"TechnicalAnalyzerAgent\", TechnicalAnalyzerAgent),",
    "            (\"DomainOntologyAgent\", DomainOntologyAgent),",
    "            (\"PlainLanguageAgent\", PlainLanguageAgent),",
    "            (\"ValidationAgent\", ValidationAgent),",
    "            (\"DesignImprovementAgent\", DesignImprovementAgent),",
    "            (\"DataConventionsAgent\", DataConventionsAgent),",
    "            (\"HigherLevelDocumentationAgent\", HigherLevelDocumentationAgent)",
    "        ]",
    "        ",
    "        for name, agent_class in agents_to_test:",
    "            try:",
    "                agent = agent_class()",
    "                print(f\"  [PASS] {name} initialized successfully\")",
    "            except Exception as e:",
    "                print(f\"  [FAIL] {name} failed: {e}\")",
    "                issues.append(f\"{name} initialization error: {str(e)}\")",
    "        ",
    "        self.issues_log.extend(issues)",
    "        return len(issues) == 0",
    "    ",
    "    def diagnose_validation_pipeline(self, test_data=None):",
    "        \"\"\"Test the complete validation pipeline with sample data.\"\"\"",
    "        print(\"Diagnosing Validation Pipeline...\")",
    "        issues = []",
    "        ",
    "        if test_data is None:",
    "            test_data = ValidationTestFixtures.get_valid_parsed_data()",
    "        ",
    "        try:",
    "            agent = ValidationAgent()",
    "            ",
    "            methods = [",
    "                (\"validate_parsed_data\", lambda: agent.validate_parsed_data(test_data)),",
    "                (\"validate_technical_analysis\", lambda: agent.validate_technical_analysis(",
    "                    ValidationTestFixtures.get_valid_technical_analysis())),",
    "                (\"validate_ontology_mappings\", lambda: agent.validate_ontology_mappings(",
    "                    ValidationTestFixtures.get_valid_ontology_mappings())),",
    "                (\"validate_documentation\", lambda: agent.validate_documentation(",
    "                    ValidationTestFixtures.get_sample_documentation()))",
    "            ]",
    "            ",
    "            for method_name, method_call in methods:",
    "                try:",
    "                    result = method_call()",
    "                    if isinstance(result, dict) and 'validation_passed' in result:",
    "                        print(f\"  [PASS] {method_name} working correctly\")",
    "                    else:",
    "                        print(f\"  [WARN] {method_name} returned unexpected format\")",
    "                        issues.append(f\"{method_name} returned invalid format\")",
    "                except Exception as e:",
    "                    print(f\"  [FAIL] {method_name} failed: {e}\")",
    "                    issues.append(f\"{method_name} error: {str(e)}\")",
    "                    ",
    "        except Exception as e:",
    "            print(f\"  [FAIL] Pipeline initialization failed: {e}\")",
    "            issues.append(f\"Pipeline error: {str(e)}\")",
    "        ",
    "        self.issues_log.extend(issues)",
    "        return len(issues) == 0",
    "    ",
    "    def diagnose_json_parsing(self):",
    "        \"\"\"Test JSON parsing capabilities.\"\"\"",
    "        print(\"Diagnosing JSON Parsing...\")",
    "        issues = []",
    "        ",
    "        test_cases = [",
    "            ('{\"key\": \"value\", \"number\": 42}', True),",
    "            ('[{\"a\": 1}, {\"b\": 2}]', True),",
    "            ('{key: \"value\"}', False),",
    "            ('{}', True),",
    "            ('{\"outer\": {\"inner\": {\"deep\": \"value\"}}}', True)",
    "        ]",
    "        ",
    "        for json_str, should_parse in test_cases:",
    "            try:",
    "                result = json.loads(json_str)",
    "                if should_parse:",
    "                    print(f\"  [PASS] Valid JSON parsed correctly\")",
    "                else:",
    "                    print(f\"  [WARN] Invalid JSON was parsed (unexpected)\")",
    "                    issues.append(f\"Invalid JSON was accepted: {json_str[:30]}...\")",
    "            except json.JSONDecodeError:",
    "                if not should_parse:",
    "                    print(f\"  [PASS] Invalid JSON correctly rejected\")",
    "                else:",
    "                    print(f\"  [FAIL] Valid JSON failed to parse\")",
    "                    issues.append(f\"Failed to parse valid JSON: {json_str[:30]}...\")",
    "        ",
    "        self.issues_log.extend(issues)",
    "        return len(issues) == 0",
    "    ",
    "    def diagnose_memory_usage(self):",
    "        \"\"\"Check memory usage and potential issues.\"\"\"",
    "        print(\"Diagnosing Memory Usage...\")",
    "        issues = []",
    "        ",
    "        try:",
    "            import sys",
    "            import gc",
    "            ",
    "            gc.collect()",
    "            obj_count = len(gc.get_objects())",
    "            print(f\"  [INFO] Total objects in memory: {obj_count:,}\")",
    "            ",
    "            if obj_count > 100000:",
    "                print(\"  [WARN] High object count - consider memory optimization\")",
    "                issues.append(\"High memory usage detected\")",
    "            else:",
    "                print(\"  [PASS] Memory usage within normal range\")",
    "            ",
    "            garbage = gc.garbage",
    "            if len(garbage) > 0:",
    "                print(f\"  [WARN] Found {len(garbage)} uncollectable objects\")",
    "                issues.append(f\"Circular references detected: {len(garbage)} objects\")",
    "            else:",
    "                print(\"  [PASS] No circular references detected\")",
    "                ",
    "        except Exception as e:",
    "            print(f\"  [FAIL] Memory check failed: {e}\")",
    "            issues.append(f\"Memory diagnostic error: {str(e)}\")",
    "        ",
    "        self.issues_log.extend(issues)",
    "        return len(issues) == 0",
    "    ",
    "    def run_full_diagnostics(self, db=None):",
    "        \"\"\"Run comprehensive diagnostics.\"\"\"",
    "        print(\"=\" * 60)",
    "        print(\"FULL SYSTEM DIAGNOSTICS\")",
    "        print(\"=\" * 60)",
    "        ",
    "        results = {}",
    "        ",
    "        results[\"api_connectivity\"] = self.diagnose_api_connectivity()",
    "        print()",
    "        ",
    "        if db:",
    "            results[\"database\"] = self.diagnose_database_connection(db)",
    "            print()",
    "        ",
    "        results[\"agents\"] = self.diagnose_agent_initialization()",
    "        print()",
    "        ",
    "        results[\"validation_pipeline\"] = self.diagnose_validation_pipeline()",
    "        print()",
    "        ",
    "        results[\"json_parsing\"] = self.diagnose_json_parsing()",
    "        print()",
    "        ",
    "        results[\"memory\"] = self.diagnose_memory_usage()",
    "        ",
    "        print(\"\\n\" + \"=\" * 60)",
    "        print(\"DIAGNOSTIC SUMMARY\")",
    "        print(\"=\" * 60)",
    "        ",
    "        all_passed = all(results.values())",
    "        for test_name, passed in results.items():",
    "            status = \"[PASS]\" if passed else \"[FAIL]\"",
    "            print(f\"{status} {test_name}\")",
    "        ",
    "        status_msg = \"ALL SYSTEMS OPERATIONAL\" if all_passed else \"ISSUES DETECTED\"",
    "        print(f\"\\nOverall Status: {status_msg}\")",
    "        ",
    "        if self.issues_log:",
    "            print(f\"\\nIssues Found ({len(self.issues_log)}):\")",
    "            for i, issue in enumerate(self.issues_log, 1):",
    "                print(f\"  {i}. {issue}\")",
    "        ",
    "        return results",
    "",
    "print('ValidationTroubleshooter loaded successfully')",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge Case and Error Handling Tests"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "class EdgeCaseTester:",
    "    \"\"\"Test edge cases and error handling in validation.\"\"\"",
    "    ",
    "    def __init__(self, validation_agent=None):",
    "        self.agent = validation_agent or ValidationAgent()",
    "        self.results = []",
    "    ",
    "    def test_empty_input(self):",
    "        \"\"\"Test handling of empty inputs.\"\"\"",
    "        print(\"\\nTesting Empty Inputs...\")",
    "        test_cases = [",
    "            (\"empty_list\", []),",
    "            (\"empty_dict\", {}),",
    "            (\"empty_string\", \"\"),",
    "        ]",
    "        ",
    "        for name, test_input in test_cases:",
    "            try:",
    "                if isinstance(test_input, list):",
    "                    result = self.agent.validate_parsed_data(test_input)",
    "                elif isinstance(test_input, str):",
    "                    result = self.agent.validate_documentation(test_input)",
    "                else:",
    "                    result = self.agent.validate_parsed_data(test_input or [])",
    "                ",
    "                print(f\"  [PASS] {name}: Handled gracefully\")",
    "                self.results.append((name, \"pass\", \"Handled empty input\"))",
    "            except Exception as e:",
    "                print(f\"  [FAIL] {name}: Error - {str(e)[:50]}\")",
    "                self.results.append((name, \"fail\", str(e)))",
    "    ",
    "    def test_oversized_input(self):",
    "        \"\"\"Test handling of very large inputs.\"\"\"",
    "        print(\"\\nTesting Oversized Inputs...\")",
    "        ",
    "        large_list = [",
    "            {",
    "                \"variable_name\": f\"var_{i}\",",
    "                \"field_type\": \"text\",",
    "                \"field_label\": f\"Variable {i}\" * 10,",
    "                \"description\": \"Test \" * 100",
    "            }",
    "            for i in range(100)",
    "        ]",
    "        ",
    "        try:",
    "            result = self.agent.validate_parsed_data(large_list)",
    "            print(f\"  [PASS] Large list (100 items): Handled\")",
    "            self.results.append((\"large_list\", \"pass\", \"Processed 100 items\"))",
    "        except Exception as e:",
    "            print(f\"  [FAIL] Large list: Error - {str(e)[:50]}\")",
    "            self.results.append((\"large_list\", \"fail\", str(e)))",
    "        ",
    "        long_doc = \"# Documentation\\n\" + (\"This is a test line.\\n\" * 1000)",
    "        try:",
    "            result = self.agent.validate_documentation(long_doc)",
    "            print(f\"  [PASS] Long documentation (1000 lines): Handled\")",
    "            self.results.append((\"long_doc\", \"pass\", \"Processed 1000 lines\"))",
    "        except Exception as e:",
    "            print(f\"  [FAIL] Long documentation: Error - {str(e)[:50]}\")",
    "            self.results.append((\"long_doc\", \"fail\", str(e)))",
    "    ",
    "    def test_special_characters(self):",
    "        \"\"\"Test handling of special characters.\"\"\"",
    "        print(\"\\nTesting Special Characters...\")",
    "        ",
    "        special_data = [",
    "            {",
    "                \"variable_name\": \"var_with_unicode_chars\",",
    "                \"field_type\": \"text\",",
    "                \"field_label\": \"Label with special chars\",",
    "                \"description\": \"Contains unicode characters\"",
    "            }",
    "        ]",
    "        ",
    "        try:",
    "            result = self.agent.validate_parsed_data(special_data)",
    "            print(f\"  [PASS] Special characters: Handled gracefully\")",
    "            self.results.append((\"special_chars\", \"pass\", \"Handled special characters\"))",
    "        except Exception as e:",
    "            print(f\"  [FAIL] Special characters: Error - {str(e)[:50]}\")",
    "            self.results.append((\"special_chars\", \"fail\", str(e)))",
    "    ",
    "    def test_nested_structures(self):",
    "        \"\"\"Test handling of deeply nested data structures.\"\"\"",
    "        print(\"\\nTesting Nested Structures...\")",
    "        ",
    "        nested_data = [",
    "            {",
    "                \"variable_name\": \"nested_var\",",
    "                \"field_type\": \"object\",",
    "                \"metadata\": {",
    "                    \"level1\": {",
    "                        \"level2\": {",
    "                            \"level3\": {",
    "                                \"level4\": {",
    "                                    \"deep_value\": \"test\"",
    "                                }",
    "                            }",
    "                        }",
    "                    }",
    "                }",
    "            }",
    "        ]",
    "        ",
    "        try:",
    "            result = self.agent.validate_parsed_data(nested_data)",
    "            print(f\"  [PASS] Deeply nested (5 levels): Handled\")",
    "            self.results.append((\"nested\", \"pass\", \"Handled 5-level nesting\"))",
    "        except Exception as e:",
    "            print(f\"  [FAIL] Nested structures: Error - {str(e)[:50]}\")",
    "            self.results.append((\"nested\", \"fail\", str(e)))",
    "    ",
    "    def test_type_mismatches(self):",
    "        \"\"\"Test handling of type mismatches.\"\"\"",
    "        print(\"\\nTesting Type Mismatches...\")",
    "        ",
    "        mismatched_data = [",
    "            {",
    "                \"variable_name\": 12345,  # Should be string",
    "                \"field_type\": [\"text\"],  # Should be string",
    "                \"field_label\": {\"label\": \"nested\"},  # Should be string",
    "                \"required\": \"yes\"  # Should be boolean",
    "            }",
    "        ]",
    "        ",
    "        try:",
    "            result = self.agent.validate_parsed_data(mismatched_data)",
    "            issues = result.get('issues_found', [])",
    "            if len(issues) > 0:",
    "                print(f\"  [PASS] Type mismatches: Detected {len(issues)} issues\")",
    "                self.results.append((\"type_mismatch\", \"pass\", f\"Found {len(issues)} issues\"))",
    "            else:",
    "                print(f\"  [WARN] Type mismatches: No issues detected\")",
    "                self.results.append((\"type_mismatch\", \"warning\", \"No issues found\"))",
    "        except Exception as e:",
    "            print(f\"  [FAIL] Type mismatches: Error - {str(e)[:50]}\")",
    "            self.results.append((\"type_mismatch\", \"fail\", str(e)))",
    "    ",
    "    def test_concurrent_validation(self):",
    "        \"\"\"Test multiple validations in sequence.\"\"\"",
    "        print(\"\\nTesting Sequential Validations...\")",
    "        ",
    "        fixtures = ValidationTestFixtures()",
    "        ",
    "        try:",
    "            for i in range(3):",
    "                self.agent.validate_parsed_data(fixtures.get_valid_parsed_data())",
    "                self.agent.validate_technical_analysis(fixtures.get_valid_technical_analysis())",
    "                self.agent.validate_ontology_mappings(fixtures.get_valid_ontology_mappings())",
    "            ",
    "            print(f\"  [PASS] Sequential validations (9 calls): All completed\")",
    "            self.results.append((\"sequential\", \"pass\", \"9 validations successful\"))",
    "        except Exception as e:",
    "            print(f\"  [FAIL] Sequential validations: Error - {str(e)[:50]}\")",
    "            self.results.append((\"sequential\", \"fail\", str(e)))",
    "    ",
    "    def run_all_edge_case_tests(self):",
    "        \"\"\"Run all edge case tests.\"\"\"",
    "        print(\"=\" * 60)",
    "        print(\"EDGE CASE TEST SUITE\")",
    "        print(\"=\" * 60)",
    "        ",
    "        self.test_empty_input()",
    "        self.test_oversized_input()",
    "        self.test_special_characters()",
    "        self.test_nested_structures()",
    "        self.test_type_mismatches()",
    "        self.test_concurrent_validation()",
    "        ",
    "        print(\"\\n\" + \"=\" * 60)",
    "        print(\"EDGE CASE TEST SUMMARY\")",
    "        print(\"=\" * 60)",
    "        ",
    "        passed = sum(1 for _, status, _ in self.results if status == \"pass\")",
    "        warnings = sum(1 for _, status, _ in self.results if status == \"warning\")",
    "        failed = sum(1 for _, status, _ in self.results if status == \"fail\")",
    "        ",
    "        print(f\"Passed: {passed}\")",
    "        print(f\"Warnings: {warnings}\")",
    "        print(f\"Failed: {failed}\")",
    "        ",
    "        return self.results",
    "",
    "print('EdgeCaseTester loaded successfully')",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration Tests for Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run_integration_validation_test(orchestrator):",
    "    \"\"\"",
    "    Run a complete integration test of the validation pipeline.",
    "    ",
    "    This test exercises:",
    "    1. Data parsing validation",
    "    2. Technical analysis validation",
    "    3. Ontology mapping validation",
    "    4. Documentation validation",
    "    5. Cross-validation between agents",
    "    6. Validation report generation",
    "    \"\"\"",
    "    print(\"=\" * 60)",
    "    print(\"INTEGRATION VALIDATION TEST\")",
    "    print(\"=\" * 60)",
    "    ",
    "    test_data = ValidationTestFixtures.get_valid_parsed_data()",
    "    ",
    "    print(\"\\nStep 1: Validate Parsed Data\")",
    "    parsed_validation = orchestrator.validation_agent.validate_parsed_data(test_data)",
    "    print(f\"   Score: {parsed_validation.get('overall_score', 'N/A')}\")",
    "    print(f\"   Issues: {len(parsed_validation.get('issues_found', []))}\")",
    "    ",
    "    print(\"\\nStep 2: Validate Technical Analysis\")",
    "    tech_validation = orchestrator.validation_agent.validate_technical_analysis(",
    "        ValidationTestFixtures.get_valid_technical_analysis()",
    "    )",
    "    print(f\"   Score: {tech_validation.get('overall_score', 'N/A')}\")",
    "    ",
    "    print(\"\\nStep 3: Validate Ontology Mappings\")",
    "    ontology_validation = orchestrator.validation_agent.validate_ontology_mappings(",
    "        ValidationTestFixtures.get_valid_ontology_mappings()",
    "    )",
    "    print(f\"   Score: {ontology_validation.get('overall_score', 'N/A')}\")",
    "    ",
    "    print(\"\\nStep 4: Validate Documentation\")",
    "    doc_validation = orchestrator.validation_agent.validate_documentation(",
    "        ValidationTestFixtures.get_sample_documentation()",
    "    )",
    "    print(f\"   Score: {doc_validation.get('overall_score', 'N/A')}\")",
    "    ",
    "    print(\"\\nStep 5: Cross-Validate All Outputs\")",
    "    cross_validation = orchestrator.validation_agent.cross_validate_agents({",
    "        \"parsed_data\": test_data,",
    "        \"technical_analysis\": ValidationTestFixtures.get_valid_technical_analysis(),",
    "        \"ontology_mappings\": ValidationTestFixtures.get_valid_ontology_mappings(),",
    "        \"documentation\": ValidationTestFixtures.get_sample_documentation()",
    "    })",
    "    print(f\"   Consistency Checks: {cross_validation.get('consistency_checks', {})}\")",
    "    ",
    "    print(\"\\nStep 6: Generate Validation Report\")",
    "    all_validations = [",
    "        parsed_validation,",
    "        tech_validation,",
    "        ontology_validation,",
    "        doc_validation,",
    "        cross_validation",
    "    ]",
    "    report = orchestrator.validation_agent.generate_validation_report(all_validations)",
    "    print(f\"   Report generated: {len(report)} characters\")",
    "    ",
    "    # Calculate overall score",
    "    scores = [",
    "        parsed_validation.get(\"overall_score\", 0),",
    "        tech_validation.get(\"overall_score\", 0),",
    "        ontology_validation.get(\"overall_score\", 0),",
    "        doc_validation.get(\"overall_score\", 0),",
    "        cross_validation.get(\"overall_score\", 0)",
    "    ]",
    "    avg_score = sum(scores) / len(scores)",
    "    ",
    "    print(\"\\n\" + \"=\" * 60)",
    "    print(\"INTEGRATION TEST RESULTS\")",
    "    print(\"=\" * 60)",
    "    print(f\"Average Validation Score: {avg_score:.1f}/100\")",
    "    status = \"PASSED\" if avg_score >= 70 else \"FAILED\"",
    "    print(f\"Status: {status}\")",
    "    ",
    "    return {",
    "        \"parsed_validation\": parsed_validation,",
    "        \"tech_validation\": tech_validation,",
    "        \"ontology_validation\": ontology_validation,",
    "        \"doc_validation\": doc_validation,",
    "        \"cross_validation\": cross_validation,",
    "        \"report\": report,",
    "        \"average_score\": avg_score",
    "    }",
    "",
    "print('Integration test function loaded successfully')",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Validation Tests"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ====================================================================",
    "# VALIDATION TEST EXECUTION",
    "# ====================================================================",
    "# Uncomment and run the tests you need",
    "",
    "# 1. Run Unit Tests for ValidationAgent",
    "# tester = ValidationAgentTester()",
    "# unit_test_results = tester.run_all_tests()",
    "",
    "# 2. Run Troubleshooting Diagnostics",
    "# troubleshooter = ValidationTroubleshooter()",
    "# diagnostic_results = troubleshooter.run_full_diagnostics(db)",
    "",
    "# 3. Run Edge Case Tests",
    "# edge_tester = EdgeCaseTester()",
    "# edge_case_results = edge_tester.run_all_edge_case_tests()",
    "",
    "# 4. Run Integration Test (requires orchestrator)",
    "# integration_results = run_integration_validation_test(orchestrator)",
    "",
    "# 5. Quick Validation Check",
    "def quick_validation_check():",
    "    \"\"\"Quick check to ensure validation system is operational.\"\"\"",
    "    print(\"Quick Validation Check\")",
    "    print(\"-\" * 40)",
    "    ",
    "    agent = ValidationAgent()",
    "    fixtures = ValidationTestFixtures()",
    "    ",
    "    tests = [",
    "        (\"Parsed Data\", lambda: agent.validate_parsed_data(fixtures.get_valid_parsed_data())),",
    "        (\"Tech Analysis\", lambda: agent.validate_technical_analysis(fixtures.get_valid_technical_analysis())),",
    "        (\"Documentation\", lambda: agent.validate_documentation(fixtures.get_sample_documentation()))",
    "    ]",
    "    ",
    "    all_passed = True",
    "    for name, test_func in tests:",
    "        try:",
    "            result = test_func()",
    "            score = result.get('overall_score', 0)",
    "            status = \"[PASS]\" if score >= 70 else \"[WARN]\"",
    "            print(f\"{status} {name}: Score {score}/100\")",
    "            if score < 70:",
    "                all_passed = False",
    "        except Exception as e:",
    "            print(f\"[FAIL] {name}: Error - {str(e)[:40]}\")",
    "            all_passed = False",
    "    ",
    "    print(\"-\" * 40)",
    "    status_msg = \"System Operational\" if all_passed else \"Issues Detected\"",
    "    print(f\"Overall: {status_msg}\")",
    "    return all_passed",
    "",
    "# Run quick check",
    "quick_validation_check()",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance and Load Testing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ValidationPerformanceTester:",
    "    \"\"\"Performance testing for validation operations.\"\"\"",
    "    ",
    "    def __init__(self):",
    "        self.agent = ValidationAgent()",
    "        self.fixtures = ValidationTestFixtures()",
    "        self.timing_results = {}",
    "    ",
    "    def measure_execution_time(self, func, name, iterations=3):",
    "        \"\"\"Measure execution time of a function.\"\"\"",
    "        import time",
    "        times = []",
    "        ",
    "        for _ in range(iterations):",
    "            start = time.time()",
    "            func()",
    "            elapsed = time.time() - start",
    "            times.append(elapsed)",
    "        ",
    "        avg_time = sum(times) / len(times)",
    "        self.timing_results[name] = {",
    "            \"average_ms\": avg_time * 1000,",
    "            \"min_ms\": min(times) * 1000,",
    "            \"max_ms\": max(times) * 1000,",
    "            \"iterations\": iterations",
    "        }",
    "        return avg_time",
    "    ",
    "    def run_performance_tests(self):",
    "        \"\"\"Run performance benchmarks.\"\"\"",
    "        print(\"=\" * 60)",
    "        print(\"VALIDATION PERFORMANCE TESTS\")",
    "        print(\"=\" * 60)",
    "        ",
    "        # Test individual validation methods",
    "        tests = [",
    "            (\"validate_parsed_data\", ",
    "             lambda: self.agent.validate_parsed_data(self.fixtures.get_valid_parsed_data())),",
    "            (\"validate_technical_analysis\",",
    "             lambda: self.agent.validate_technical_analysis(self.fixtures.get_valid_technical_analysis())),",
    "            (\"validate_ontology_mappings\",",
    "             lambda: self.agent.validate_ontology_mappings(self.fixtures.get_valid_ontology_mappings())),",
    "            (\"validate_documentation\",",
    "             lambda: self.agent.validate_documentation(self.fixtures.get_sample_documentation()))",
    "        ]",
    "        ",
    "        print(\"\\nBenchmarking validation methods (3 iterations each):\\n\")",
    "        ",
    "        for name, func in tests:",
    "            avg_time = self.measure_execution_time(func, name)",
    "            result = self.timing_results[name]",
    "            print(f\"{name}:\")",
    "            print(f\"  Average: {result['average_ms']:.2f}ms\")",
    "            print(f\"  Min: {result['min_ms']:.2f}ms | Max: {result['max_ms']:.2f}ms\\n\")",
    "        ",
    "        # Test with larger datasets",
    "        print(\"Testing with large dataset (50 variables):\")",
    "        large_data = [",
    "            {",
    "                \"variable_name\": f\"var_{i}\",",
    "                \"field_type\": \"text\",",
    "                \"field_label\": f\"Variable {i}\",",
    "                \"description\": f\"Description for variable {i}\"",
    "            }",
    "            for i in range(50)",
    "        ]",
    "        ",
    "        avg_time = self.measure_execution_time(",
    "            lambda: self.agent.validate_parsed_data(large_data),",
    "            \"large_dataset_50_vars\"",
    "        )",
    "        result = self.timing_results[\"large_dataset_50_vars\"]",
    "        print(f\"  Average: {result['average_ms']:.2f}ms\\n\")",
    "        ",
    "        print(\"=\" * 60)",
    "        print(\"PERFORMANCE SUMMARY\")",
    "        print(\"=\" * 60)",
    "        ",
    "        total_avg = sum(r['average_ms'] for r in self.timing_results.values()) / len(self.timing_results)",
    "        print(f\"Overall Average Response Time: {total_avg:.2f}ms\")",
    "        ",
    "        # Performance thresholds",
    "        if total_avg < 1000:",
    "            print(\"Status: [EXCELLENT] Sub-second response times\")",
    "        elif total_avg < 5000:",
    "            print(\"Status: [GOOD] Acceptable response times\")",
    "        else:",
    "            print(\"Status: [SLOW] Consider optimization\")",
    "        ",
    "        return self.timing_results",
    "",
    "# Uncomment to run performance tests",
    "# perf_tester = ValidationPerformanceTester()",
    "# perf_results = perf_tester.run_performance_tests()",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Testing and Validation Consistency"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run_regression_tests():",
    "    \"\"\"",
    "    Run regression tests to ensure validation consistency.",
    "    ",
    "    These tests verify that:",
    "    1. Same inputs always produce same validation scores",
    "    2. Known good inputs always pass",
    "    3. Known bad inputs always fail",
    "    \"\"\"",
    "    print(\"=\" * 60)",
    "    print(\"REGRESSION TEST SUITE\")",
    "    print(\"=\" * 60)",
    "    ",
    "    agent = ValidationAgent()",
    "    fixtures = ValidationTestFixtures()",
    "    ",
    "    results = {",
    "        \"consistency\": [],",
    "        \"known_good\": [],",
    "        \"known_bad\": []",
    "    }",
    "    ",
    "    # Test 1: Consistency - same input should give same score",
    "    print(\"\\nTest 1: Validation Consistency\")",
    "    print(\"-\" * 40)",
    "    ",
    "    test_data = fixtures.get_valid_parsed_data()",
    "    scores = []",
    "    for i in range(3):",
    "        result = agent.validate_parsed_data(test_data)",
    "        scores.append(result.get(\"overall_score\", 0))",
    "    ",
    "    is_consistent = len(set(scores)) == 1  # All scores should be the same",
    "    if is_consistent:",
    "        print(f\"[PASS] Consistent scoring: {scores[0]}/100 (3 runs)\")",
    "        results[\"consistency\"].append((\"parsed_data\", True))",
    "    else:",
    "        print(f\"[FAIL] Inconsistent scores: {scores}\")",
    "        results[\"consistency\"].append((\"parsed_data\", False))",
    "    ",
    "    # Test 2: Known good inputs should always pass (score >= 70)",
    "    print(\"\\nTest 2: Known Good Inputs\")",
    "    print(\"-\" * 40)",
    "    ",
    "    good_cases = [",
    "        (\"valid_parsed_data\", fixtures.get_valid_parsed_data(), agent.validate_parsed_data),",
    "        (\"valid_tech_analysis\", fixtures.get_valid_technical_analysis(), agent.validate_technical_analysis),",
    "        (\"valid_ontology\", fixtures.get_valid_ontology_mappings(), agent.validate_ontology_mappings),",
    "        (\"complete_documentation\", fixtures.get_sample_documentation(), agent.validate_documentation)",
    "    ]",
    "    ",
    "    for name, data, validate_func in good_cases:",
    "        result = validate_func(data)",
    "        score = result.get(\"overall_score\", 0)",
    "        passed = score >= 70",
    "        status = \"[PASS]\" if passed else \"[FAIL]\"",
    "        print(f\"{status} {name}: Score {score}/100\")",
    "        results[\"known_good\"].append((name, passed))",
    "    ",
    "    # Test 3: Known bad inputs should always fail (score < 70 or have issues)",
    "    print(\"\\nTest 3: Known Bad Inputs\")",
    "    print(\"-\" * 40)",
    "    ",
    "    bad_cases = [",
    "        (\"malformed_parsed_data\", fixtures.get_malformed_parsed_data(), agent.validate_parsed_data),",
    "        (\"incomplete_documentation\", fixtures.get_incomplete_documentation(), agent.validate_documentation)",
    "    ]",
    "    ",
    "    for name, data, validate_func in bad_cases:",
    "        result = validate_func(data)",
    "        score = result.get(\"overall_score\", 100)",
    "        issues = len(result.get(\"issues_found\", []))",
    "        failed_correctly = score < 70 or issues > 0",
    "        status = \"[PASS]\" if failed_correctly else \"[FAIL]\"",
    "        print(f\"{status} {name}: Score {score}/100, Issues: {issues}\")",
    "        results[\"known_bad\"].append((name, failed_correctly))",
    "    ",
    "    # Summary",
    "    print(\"\\n\" + \"=\" * 60)",
    "    print(\"REGRESSION TEST SUMMARY\")",
    "    print(\"=\" * 60)",
    "    ",
    "    all_consistent = all(passed for _, passed in results[\"consistency\"])",
    "    all_good_pass = all(passed for _, passed in results[\"known_good\"])",
    "    all_bad_fail = all(passed for _, passed in results[\"known_bad\"])",
    "    ",
    "    print(f\"Consistency Tests: {'PASS' if all_consistent else 'FAIL'}\")",
    "    print(f\"Known Good Tests: {'PASS' if all_good_pass else 'FAIL'}\")",
    "    print(f\"Known Bad Tests: {'PASS' if all_bad_fail else 'FAIL'}\")",
    "    ",
    "    overall_pass = all_consistent and all_good_pass and all_bad_fail",
    "    print(f\"\\nOverall: {'ALL REGRESSION TESTS PASSED' if overall_pass else 'REGRESSION FAILURES DETECTED'}\")",
    "    ",
    "    return results",
    "",
    "# Uncomment to run regression tests",
    "# regression_results = run_regression_tests()",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Session and Memory Management",
    "",
    "ADK-style session management with context compaction for long conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SessionManager:",
    "    \"\"\"ADK-style session management with state persistence.\"\"\"",
    "    ",
    "    def __init__(self, db_manager: DatabaseManager):",
    "        self.db = db_manager",
    "    ",
    "    def create_session(self, job_id: str, user_id: str) -> str:",
    "        \"\"\"Create a new session.\"\"\"",
    "        session_id = hashlib.md5(f\"{job_id}_{user_id}_{datetime.now().isoformat()}\".encode()).hexdigest()[:16]",
    "        query = \"INSERT INTO Sessions (session_id, job_id, user_id) VALUES (?, ?, ?)\"",
    "        self.db.execute_update(query, (session_id, job_id, user_id))",
    "        return session_id",
    "    ",
    "    def get_session_state(self, session_id: str) -> Dict:",
    "        \"\"\"Get session state.\"\"\"",
    "        query = \"SELECT state FROM Sessions WHERE session_id = ?\"",
    "        result = self.db.execute_query(query, (session_id,))",
    "        if result:",
    "            return json.loads(result[0]['state'])",
    "        return {}",
    "    ",
    "    def update_session_state(self, session_id: str, key: str, value: Any):",
    "        \"\"\"Update session state (similar to ADK tool_context.state).\"\"\"",
    "        state = self.get_session_state(session_id)",
    "        state[key] = value",
    "        query = \"UPDATE Sessions SET state = ?, updated_at = CURRENT_TIMESTAMP WHERE session_id = ?\"",
    "        self.db.execute_update(query, (json.dumps(state), session_id))",
    "    ",
    "    def add_to_history(self, session_id: str, job_id: str, role: str, content: str, metadata: Dict = None):",
    "        \"\"\"Add message to session history.\"\"\"",
    "        query = \"\"\"",
    "        INSERT INTO SessionHistory (session_id, job_id, role, content, metadata)",
    "        VALUES (?, ?, ?, ?, ?)",
    "        \"\"\"",
    "        self.db.execute_update(query, (session_id, job_id, role, content, json.dumps(metadata) if metadata else None))",
    "",
    "",
    "class ContextManager:",
    "    \"\"\"Manages working memory with compaction for long sessions.\"\"\"",
    "    ",
    "    def __init__(self, db_manager: DatabaseManager, max_tokens: int = 100000):",
    "        self.db = db_manager",
    "        self.max_tokens = max_tokens",
    "        self.compaction_threshold = int(max_tokens * 0.8)",
    "    ",
    "    def estimate_tokens(self, text: str) -> int:",
    "        \"\"\"Rough token estimation (1 token \u2248 4 characters).\"\"\"",
    "        return len(text) // 4",
    "    ",
    "    def get_working_memory(self, job_id: str) -> Dict[str, Any]:",
    "        \"\"\"Get current working memory for a job.\"\"\"",
    "        query = \"SELECT * FROM SessionHistory WHERE job_id = ? ORDER BY created_at\"",
    "        history_rows = self.db.execute_query(query, (job_id,))",
    "        ",
    "        session_history = [",
    "            {",
    "                'role': row['role'],",
    "                'content': row['content'],",
    "                'timestamp': row['created_at']",
    "            }",
    "            for row in history_rows",
    "        ]",
    "        ",
    "        total_tokens = sum(self.estimate_tokens(msg['content']) for msg in session_history)",
    "        ",
    "        return {",
    "            'session_history': session_history,",
    "            'total_tokens': total_tokens,",
    "            'needs_compaction': total_tokens > self.compaction_threshold",
    "        }",
    "    ",
    "    def compact_context(self, job_id: str) -> str:",
    "        \"\"\"Compact session history using summarization (ADK context compaction pattern).\"\"\"",
    "        working_memory = self.get_working_memory(job_id)",
    "        ",
    "        if not working_memory['needs_compaction']:",
    "            return \"No compaction needed\"",
    "        ",
    "        print(\"\\n\u26a1 Context compaction triggered...\")",
    "        # In production, use LLM to summarize conversation",
    "        # For now, keep last N messages",
    "        logger.info(f\"Context compaction for job {job_id}\")",
    "        return \"Context compacted\"",
    "",
    "",
    "print(\"\u2713 Session and Context management classes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. System Status and Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def display_system_status(db: DatabaseManager):\n    \"\"\"Display current system status with observability metrics.\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"ADE SYSTEM STATUS\")\n    print(\"=\"*80)\n    \n    # Jobs\n    jobs = db.execute_query(\"SELECT * FROM Jobs ORDER BY created_at DESC LIMIT 5\")\n    print(f\"\\nRecent Jobs: {len(jobs)}\")\n    for job in jobs:\n        print(f\"  [{job['job_id']}] {job['source_file']} - {job['status']}\")\n    \n    # Snippets\n    snippets = db.execute_query(\"SELECT snippet_type, COUNT(*) as count FROM Snippets GROUP BY snippet_type\")\n    print(f\"\\nSnippet Library:\")\n    for snippet in snippets:\n        print(f\"  {snippet['snippet_type']}: {snippet['count']}\")\n    \n    # Review Queue\n    review_stats = db.execute_query(\"SELECT status, COUNT(*) as count FROM ReviewQueue GROUP BY status\")\n    print(f\"\\nReview Queue:\")\n    for stat in review_stats:\n        print(f\"  {stat['status']}: {stat['count']}\")\n    \n    # Sessions\n    sessions = db.execute_query(\"SELECT COUNT(*) as count FROM Sessions\")\n    print(f\"\\nSessions: {sessions[0]['count']}\")\n    \n    print(\"\\n\" + \"=\"*80)\n\ndisplay_system_status(db)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Export and Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil",
    "",
    "def backup_database(db_path: str, backup_path: str):",
    "    \"\"\"Create a backup of the project database.\"\"\"",
    "    shutil.copy2(db_path, backup_path)",
    "    print(f\"\u2713 Database backed up to {backup_path}\")",
    "",
    "def export_documentation():",
    "    \"\"\"Export generated documentation.\"\"\"",
    "    if os.path.exists(\"healthcare_data_documentation.md\"):",
    "        with open(\"healthcare_data_documentation.md\", 'r') as f:",
    "            content = f.read()",
    "        print(f\"Documentation length: {len(content)} characters\")",
    "        return content",
    "    else:",
    "        print(\"No documentation file found\")",
    "        return None",
    "",
    "# Create backup",
    "backup_database(\"project.db\", \"project_backup.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Deploying to Vertex AI Agent Engine",
    "",
    "This section provides instructions for deploying your healthcare documentation agent to Google Cloud's Vertex AI Agent Engine for production use.",
    "",
    "### Overview",
    "",
    "Vertex AI Agent Engine provides:",
    "- **Fully managed infrastructure** with auto-scaling",
    "- **Built-in security** with IAM integration",
    "- **Production monitoring** through Cloud Console",
    "- **Session and memory services** at scale",
    "- **High availability** across regions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Create the main agent.py file for deployment with extended agents\nagent_code = '''import os\nimport json\nimport hashlib\nfrom datetime import datetime\nimport vertexai\nfrom google.adk.agents import Agent, LlmAgent\nfrom google.adk.tools.tool_context import ToolContext\nfrom typing import Dict, List, Any, Optional\n\n# Initialize Vertex AI\nvertexai.init(\n    project=os.environ.get(\"GOOGLE_CLOUD_PROJECT\"),\n    location=os.environ.get(\"GOOGLE_CLOUD_LOCATION\", \"us-central1\"),\n)\n\n# ==================== CORE TOOLS ====================\n\ndef parse_data_dictionary(data: str) -> Dict[str, Any]:\n    \"\"\"Parse a raw data dictionary into structured format.\"\"\"\n    lines = data.strip().split(\"\\\\n\")\n    if not lines:\n        return {\"status\": \"error\", \"message\": \"Empty data\"}\n    \n    header = lines[0].split(\",\")\n    variables = []\n    for line in lines[1:]:\n        if line.strip():\n            values = line.split(\",\")\n            var_dict = dict(zip(header, values))\n            variables.append(var_dict)\n    \n    return {\n        \"status\": \"success\",\n        \"variable_count\": len(variables),\n        \"variables\": variables\n    }\n\ndef map_to_ontology(variable_name: str, data_type: str) -> Dict[str, Any]:\n    \"\"\"Map a variable to standard healthcare ontologies.\"\"\"\n    ontology_map = {\n        \"patient_id\": {\"omop\": \"person_id\", \"concept_id\": 0},\n        \"age\": {\"omop\": \"year_of_birth\", \"concept_id\": 4154793},\n        \"sex\": {\"omop\": \"gender_concept_id\", \"concept_id\": 4135376},\n        \"bp_systolic\": {\"omop\": \"measurement\", \"concept_id\": 3004249},\n        \"bp_diastolic\": {\"omop\": \"measurement\", \"concept_id\": 3012888},\n        \"hba1c\": {\"omop\": \"measurement\", \"concept_id\": 3004410, \"loinc\": \"4548-4\"},\n    }\n    \n    mapping = ontology_map.get(variable_name.lower(), {\"omop\": \"unknown\", \"concept_id\": 0})\n    return {\"status\": \"success\", \"variable_name\": variable_name, \"mappings\": mapping}\n\ndef generate_documentation(variable_info: Dict[str, Any]) -> Dict[str, str]:\n    \"\"\"Generate human-readable documentation for a variable.\"\"\"\n    name = variable_info.get(\"Variable Name\", \"Unknown\")\n    field_type = variable_info.get(\"Field Type\", \"text\")\n    label = variable_info.get(\"Field Label\", name)\n    notes = variable_info.get(\"Notes\", \"No additional notes\")\n    \n    doc = f\"\"\"## Variable: {name}\n\n**Description:** {label}\n\n**Technical Details:**\n- Data Type: {field_type}\n- Cardinality: required\n- Notes: {notes}\n\"\"\"\n    return {\"status\": \"success\", \"documentation\": doc}\n\n# ==================== DESIGN IMPROVEMENT TOOLS ====================\n\ndef improve_document_design(content: str) -> Dict[str, Any]:\n    \"\"\"Improve the design and structure of documentation.\"\"\"\n    improvements = []\n    improved_content = content\n    \n    # Add header hierarchy if missing\n    if not content.startswith(\"#\"):\n        improved_content = \"## \" + improved_content\n        improvements.append({\n            \"type\": \"structural\",\n            \"description\": \"Added proper header hierarchy\",\n            \"rationale\": \"Improves document scannability\"\n        })\n    \n    # Ensure consistent spacing\n    if \"\\\\n\\\\n\" not in improved_content:\n        improved_content = improved_content.replace(\"\\\\n\", \"\\\\n\\\\n\")\n        improvements.append({\n            \"type\": \"formatting\",\n            \"description\": \"Added consistent paragraph spacing\",\n            \"rationale\": \"Improves readability\"\n        })\n    \n    # Add bold for key terms\n    for keyword in [\"Data Type:\", \"Cardinality:\", \"Notes:\"]:\n        if keyword in improved_content and f\"**{keyword}**\" not in improved_content:\n            improved_content = improved_content.replace(keyword, f\"**{keyword}**\")\n    \n    return {\n        \"status\": \"success\",\n        \"original_content\": content,\n        \"improved_content\": improved_content,\n        \"improvements_made\": improvements,\n        \"design_score\": {\n            \"before\": 65,\n            \"after\": 85,\n            \"metrics\": {\n                \"readability\": 85,\n                \"scannability\": 90,\n                \"consistency\": 80,\n                \"accessibility\": 85\n            }\n        }\n    }\n\ndef analyze_design_patterns(documents: List[str]) -> Dict[str, Any]:\n    \"\"\"Analyze design patterns across multiple documents.\"\"\"\n    patterns = {\n        \"header_usage\": sum(1 for d in documents if d.startswith(\"#\")),\n        \"bold_usage\": sum(1 for d in documents if \"**\" in d),\n        \"list_usage\": sum(1 for d in documents if \"- \" in d),\n        \"consistent_structure\": len(set(d.split(\"\\\\n\")[0] for d in documents)) == 1\n    }\n    \n    return {\n        \"status\": \"success\",\n        \"total_documents\": len(documents),\n        \"patterns\": patterns,\n        \"recommendations\": [\n            \"Ensure all documents start with proper headers\",\n            \"Use consistent formatting for similar content types\"\n        ]\n    }\n\n# ==================== DATA CONVENTIONS TOOLS ====================\n\ndef analyze_variable_conventions(variable_name: str, data_type: str) -> Dict[str, Any]:\n    \"\"\"Analyze and document data conventions for a variable.\"\"\"\n    # Detect naming pattern\n    if \"_\" in variable_name:\n        pattern = \"snake_case\"\n        parts = variable_name.split(\"_\")\n        prefix = parts[0] if len(parts) > 1 else None\n    elif variable_name[0].isupper():\n        pattern = \"PascalCase\"\n        prefix = None\n    elif any(c.isupper() for c in variable_name[1:]):\n        pattern = \"camelCase\"\n        prefix = None\n    else:\n        pattern = \"lowercase\"\n        prefix = None\n    \n    return {\n        \"status\": \"success\",\n        \"variable_name\": variable_name,\n        \"naming_convention\": {\n            \"pattern\": pattern,\n            \"prefix\": prefix,\n            \"suffix\": None,\n            \"follows_standard\": pattern in [\"snake_case\", \"camelCase\"],\n            \"deviation_notes\": \"\" if pattern in [\"snake_case\", \"camelCase\"] else \"Non-standard naming pattern\"\n        },\n        \"value_conventions\": {\n            \"coding_scheme\": \"Standard healthcare coding\",\n            \"valid_values\": [],\n            \"missing_indicator\": \"NA\",\n            \"format_pattern\": data_type\n        },\n        \"recommended_documentation\": {\n            \"technical_name\": variable_name,\n            \"display_name\": variable_name.replace(\"_\", \" \").title(),\n            \"code_sample\": f'df[\"{variable_name}\"]',\n            \"validation_rules\": [\"Not null\", f\"Type: {data_type}\"]\n        },\n        \"consistency_score\": 90 if pattern == \"snake_case\" else 70,\n        \"convention_warnings\": []\n    }\n\ndef generate_conventions_glossary(variables: List[Dict]) -> Dict[str, Any]:\n    \"\"\"Generate a comprehensive conventions glossary.\"\"\"\n    patterns = {}\n    for var in variables:\n        name = var.get(\"Variable Name\", \"\")\n        if \"_\" in name:\n            patterns[\"snake_case\"] = patterns.get(\"snake_case\", 0) + 1\n        elif any(c.isupper() for c in name[1:]):\n            patterns[\"camelCase\"] = patterns.get(\"camelCase\", 0) + 1\n        else:\n            patterns[\"other\"] = patterns.get(\"other\", 0) + 1\n    \n    dominant = max(patterns.items(), key=lambda x: x[1])[0] if patterns else \"unknown\"\n    \n    return {\n        \"status\": \"success\",\n        \"naming_patterns\": patterns,\n        \"dominant_pattern\": dominant,\n        \"total_variables\": len(variables),\n        \"recommendations\": [\n            f\"Primary naming convention: {dominant}\",\n            \"Maintain consistency across all new variables\"\n        ]\n    }\n\n# ==================== VERSION CONTROL TOOLS ====================\n\ndef create_version(tool_context: ToolContext, element_id: str, \n                   element_type: str, content: str) -> Dict[str, Any]:\n    \"\"\"Create a new version of a documentation element.\"\"\"\n    # Get current version from state\n    version_key = f\"version:{element_id}\"\n    current_version = tool_context.state.get(version_key, \"0.0.0\")\n    \n    # Calculate content hash\n    content_hash = hashlib.sha256(content.encode()).hexdigest()[:16]\n    \n    # Check if content changed\n    hash_key = f\"hash:{element_id}\"\n    old_hash = tool_context.state.get(hash_key, \"\")\n    \n    if old_hash == content_hash:\n        return {\n            \"status\": \"no_change\",\n            \"element_id\": element_id,\n            \"version\": current_version,\n            \"message\": \"Content unchanged, no new version created\"\n        }\n    \n    # Increment version (simple patch increment)\n    parts = list(map(int, current_version.split(\".\")))\n    parts[2] += 1\n    new_version = \".\".join(map(str, parts))\n    \n    # Store new version info\n    tool_context.state[version_key] = new_version\n    tool_context.state[hash_key] = content_hash\n    tool_context.state[f\"content:{element_id}:{new_version}\"] = content\n    \n    # Store version history\n    history_key = f\"history:{element_id}\"\n    history = json.loads(tool_context.state.get(history_key, \"[]\"))\n    history.append({\n        \"version\": new_version,\n        \"timestamp\": datetime.now().isoformat(),\n        \"hash\": content_hash\n    })\n    tool_context.state[history_key] = json.dumps(history)\n    \n    return {\n        \"status\": \"success\",\n        \"element_id\": element_id,\n        \"element_type\": element_type,\n        \"new_version\": new_version,\n        \"previous_version\": current_version,\n        \"content_hash\": content_hash,\n        \"timestamp\": datetime.now().isoformat()\n    }\n\ndef get_version_history(tool_context: ToolContext, element_id: str) -> Dict[str, Any]:\n    \"\"\"Get the version history for a documentation element.\"\"\"\n    history_key = f\"history:{element_id}\"\n    history = json.loads(tool_context.state.get(history_key, \"[]\"))\n    \n    return {\n        \"status\": \"success\",\n        \"element_id\": element_id,\n        \"version_count\": len(history),\n        \"history\": history,\n        \"current_version\": tool_context.state.get(f\"version:{element_id}\", \"1.0.0\")\n    }\n\ndef rollback_version(tool_context: ToolContext, element_id: str, \n                     target_version: str) -> Dict[str, Any]:\n    \"\"\"Rollback to a previous version.\"\"\"\n    content_key = f\"content:{element_id}:{target_version}\"\n    content = tool_context.state.get(content_key, None)\n    \n    if not content:\n        return {\n            \"status\": \"error\",\n            \"message\": f\"Version {target_version} not found for {element_id}\"\n        }\n    \n    # Create new version with old content\n    return create_version(tool_context, element_id, \"rollback\", content)\n\ndef compare_versions(tool_context: ToolContext, element_id: str,\n                    version_a: str, version_b: str) -> Dict[str, Any]:\n    \"\"\"Compare two versions of an element.\"\"\"\n    content_a = tool_context.state.get(f\"content:{element_id}:{version_a}\", \"\")\n    content_b = tool_context.state.get(f\"content:{element_id}:{version_b}\", \"\")\n    \n    if not content_a or not content_b:\n        return {\"status\": \"error\", \"message\": \"One or both versions not found\"}\n    \n    # Simple line-by-line comparison\n    lines_a = set(content_a.split(\"\\\\n\"))\n    lines_b = set(content_b.split(\"\\\\n\"))\n    \n    return {\n        \"status\": \"success\",\n        \"element_id\": element_id,\n        \"version_a\": version_a,\n        \"version_b\": version_b,\n        \"added_lines\": len(lines_b - lines_a),\n        \"removed_lines\": len(lines_a - lines_b),\n        \"unchanged_lines\": len(lines_a & lines_b)\n    }\n\n# ==================== HIGHER-LEVEL DOCUMENTATION TOOLS ====================\n\ndef identify_instruments(variables: List[Dict]) -> Dict[str, Any]:\n    \"\"\"Identify potential instruments or measurement tools in the dataset.\"\"\"\n    prefix_groups = {}\n    \n    for var in variables:\n        name = var.get(\"Variable Name\", \"\")\n        if \"_\" in name:\n            prefix = name.split(\"_\")[0]\n            if prefix not in prefix_groups:\n                prefix_groups[prefix] = []\n            prefix_groups[prefix].append(var)\n    \n    instruments = []\n    for prefix, vars in prefix_groups.items():\n        if len(vars) >= 3:\n            instruments.append({\n                \"prefix\": prefix,\n                \"suggested_name\": f\"{prefix.upper()} Instrument\",\n                \"variable_count\": len(vars),\n                \"variables\": [v.get(\"Variable Name\") for v in vars]\n            })\n    \n    return {\n        \"status\": \"success\",\n        \"instruments_found\": len(instruments),\n        \"instruments\": instruments\n    }\n\ndef document_instrument(variables: List[Dict], instrument_name: str) -> Dict[str, Any]:\n    \"\"\"Document a complete instrument or measurement tool.\"\"\"\n    var_names = [v.get(\"Variable Name\", \"Unknown\") for v in variables]\n    \n    doc_markdown = f\"\"\"# {instrument_name}\n\n## Overview\nThis instrument consists of {len(variables)} related variables.\n\n## Variables Included\n{chr(10).join(f\"- {name}\" for name in var_names)}\n\n## Clinical Context\nThese variables are grouped together as they represent a cohesive measurement domain.\n\n## Usage Guidelines\n- Ensure all variables are collected together for complete instrument score\n- Follow standard data collection protocols\n- Document any missing values\n\"\"\"\n    \n    return {\n        \"status\": \"success\",\n        \"element_type\": \"instrument\",\n        \"name\": instrument_name,\n        \"short_name\": instrument_name.split()[0] if \" \" in instrument_name else instrument_name,\n        \"description\": f\"Instrument containing {len(variables)} related variables\",\n        \"variables_included\": [\n            {\n                \"variable_name\": v.get(\"Variable Name\", \"Unknown\"),\n                \"role\": \"item\",\n                \"position\": i + 1\n            }\n            for i, v in enumerate(variables)\n        ],\n        \"documentation_markdown\": doc_markdown\n    }\n\ndef document_segment(variables: List[Dict], segment_name: str, \n                     segment_type: str = \"segment\") -> Dict[str, Any]:\n    \"\"\"Document a segment or logical grouping of variables.\"\"\"\n    return {\n        \"status\": \"success\",\n        \"element_type\": segment_type,\n        \"name\": segment_name,\n        \"description\": f\"{segment_type.title()} containing {len(variables)} variables\",\n        \"variables_included\": [v.get(\"Variable Name\", \"Unknown\") for v in variables],\n        \"relationships\": [\n            {\n                \"type\": \"grouping\",\n                \"description\": f\"Variables grouped under {segment_name}\"\n            }\n        ]\n    }\n\ndef generate_codebook_overview(variables: List[Dict], \n                               instruments: Optional[List[Dict]] = None) -> Dict[str, str]:\n    \"\"\"Generate a comprehensive codebook overview.\"\"\"\n    overview = f\"\"\"# Codebook Overview\n\n**Total Variables:** {len(variables)}\n**Generated:** {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n\n---\n\n## Variable Summary\n\n\"\"\"\n    \n    if instruments:\n        overview += f\"## Identified Instruments: {len(instruments)}\\\\n\\\\n\"\n        for inst in instruments:\n            overview += f\"- **{inst.get('suggested_name', 'Unknown')}**: {inst.get('variable_count', 0)} variables\\\\n\"\n    \n    return {\n        \"status\": \"success\",\n        \"overview\": overview,\n        \"total_variables\": len(variables),\n        \"instruments_count\": len(instruments) if instruments else 0\n    }\n\n# ==================== MEMORY TOOLS ====================\n\ndef save_to_memory(tool_context: ToolContext, key: str, value: str) -> Dict[str, str]:\n    \"\"\"Save information to session state.\"\"\"\n    tool_context.state[f\"memory:{key}\"] = value\n    return {\"status\": \"success\", \"message\": f\"Saved {key} to memory\"}\n\ndef retrieve_from_memory(tool_context: ToolContext, key: str) -> Dict[str, Any]:\n    \"\"\"Retrieve information from session state.\"\"\"\n    value = tool_context.state.get(f\"memory:{key}\", \"Not found\")\n    return {\"status\": \"success\", \"key\": key, \"value\": value}\n\n# ==================== CREATE ROOT AGENT ====================\n\nroot_agent = LlmAgent(\n    name=\"healthcare_documentation_agent\",\n    model=\"gemini-2.0-flash-exp\",\n    description=\"Advanced agent for healthcare data documentation with design improvement, conventions enforcement, version control, and higher-level documentation capabilities\",\n    instruction=\"\"\"You are an Advanced Healthcare Data Documentation Agent with extended capabilities:\n\nCORE CAPABILITIES:\n1. Parse data dictionaries from various formats\n2. Map variables to standard healthcare ontologies (OMOP, LOINC, SNOMED)\n3. Generate clear, comprehensive documentation\n\nEXTENDED CAPABILITIES:\n4. **Design Improvement**: Enhance document structure, readability, and visual hierarchy\n5. **Data Conventions**: Ensure variable naming standards and coding schemes are documented\n6. **Version Control**: Track changes, manage versions, and support rollbacks\n7. **Higher-Level Documentation**: Document instruments, segments, and codebook structures\n\nWORKFLOW:\nWhen processing a data dictionary:\n1. Use parse_data_dictionary to extract variable information\n2. Use map_to_ontology for each variable to find standard codes\n3. Use analyze_variable_conventions to ensure naming standards are documented\n4. Use generate_documentation to create human-readable documentation\n5. Use improve_document_design to enhance the output quality\n6. Use create_version to track changes and enable rollback\n7. Use identify_instruments to find related variable groups\n8. Use document_instrument for higher-level documentation\n9. Use generate_codebook_overview for comprehensive summary\n\nFor updates and modifications:\n- Always use create_version before making changes\n- Use compare_versions to understand differences\n- Use rollback_version if needed to revert changes\n\nRemember to save important findings to memory for cross-session knowledge.\"\"\",\n    tools=[\n        # Core tools\n        parse_data_dictionary,\n        map_to_ontology,\n        generate_documentation,\n        # Design improvement tools\n        improve_document_design,\n        analyze_design_patterns,\n        # Data conventions tools\n        analyze_variable_conventions,\n        generate_conventions_glossary,\n        # Version control tools\n        create_version,\n        get_version_history,\n        rollback_version,\n        compare_versions,\n        # Higher-level documentation tools\n        identify_instruments,\n        document_instrument,\n        document_segment,\n        generate_codebook_overview,\n        # Memory tools\n        save_to_memory,\n        retrieve_from_memory,\n    ],\n)\n'''\n\nwith open(f\"{DEPLOY_DIR}/agent.py\", 'w') as f:\n    f.write(agent_code)\n\nprint(f\"\u2713 Created {DEPLOY_DIR}/agent.py with extended agent capabilities\")\nprint(\"  Core tools:\")\nprint(\"    - parse_data_dictionary, map_to_ontology, generate_documentation\")\nprint(\"  Design improvement tools:\")\nprint(\"    - improve_document_design, analyze_design_patterns\")\nprint(\"  Data conventions tools:\")\nprint(\"    - analyze_variable_conventions, generate_conventions_glossary\")\nprint(\"  Version control tools:\")\nprint(\"    - create_version, get_version_history, rollback_version, compare_versions\")\nprint(\"  Higher-level documentation tools:\")\nprint(\"    - identify_instruments, document_instrument, document_segment, generate_codebook_overview\")\nprint(\"  Memory tools:\")\nprint(\"    - save_to_memory, retrieve_from_memory\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create deployment directory structure",
    "import os",
    "",
    "DEPLOY_DIR = \"healthcare_agent_deploy\"",
    "",
    "# Create directory structure",
    "os.makedirs(f\"{DEPLOY_DIR}\", exist_ok=True)",
    "",
    "print(f'''",
    "\ud83d\udcc1 Deployment Structure for Vertex AI Agent Engine:",
    "",
    "{DEPLOY_DIR}/",
    "\u251c\u2500\u2500 agent.py                     # Main agent logic",
    "\u251c\u2500\u2500 requirements.txt             # Python dependencies",
    "\u251c\u2500\u2500 .env                         # Environment configuration",
    "\u2514\u2500\u2500 .agent_engine_config.json    # Deployment specifications",
    "",
    "This structure follows ADK deployment conventions.",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the main agent.py file for deployment",
    "agent_code = \"\"\"import os",
    "import json",
    "import vertexai",
    "from google.adk.agents import Agent, LlmAgent",
    "from google.adk.tools.tool_context import ToolContext",
    "from typing import Dict, List, Any",
    "",
    "# Initialize Vertex AI",
    "vertexai.init(",
    "    project=os.environ.get(\"GOOGLE_CLOUD_PROJECT\"),",
    "    location=os.environ.get(\"GOOGLE_CLOUD_LOCATION\", \"us-central1\"),",
    ")",
    "",
    "",
    "def parse_data_dictionary(data: str) -> Dict[str, Any]:",
    "    \"\"\"Parse a raw data dictionary into structured format.\"\"\"",
    "    lines = data.strip().split(\"\\n\")",
    "    if not lines:",
    "        return {\"status\": \"error\", \"message\": \"Empty data\"}",
    "",
    "    header = lines[0].split(\",\")",
    "    variables = []",
    "",
    "    for line in lines[1:]:",
    "        if line.strip():",
    "            values = line.split(\",\")",
    "            var_dict = dict(zip(header, values))",
    "            variables.append(var_dict)",
    "",
    "    return {",
    "        \"status\": \"success\",",
    "        \"variable_count\": len(variables),",
    "        \"variables\": variables",
    "    }",
    "",
    "",
    "def map_to_ontology(variable_name: str, data_type: str) -> Dict[str, Any]:",
    "    \"\"\"Map a variable to standard healthcare ontologies.\"\"\"",
    "    ontology_map = {",
    "        \"patient_id\": {\"omop\": \"person_id\", \"concept_id\": 0},",
    "        \"age\": {\"omop\": \"year_of_birth\", \"concept_id\": 4154793},",
    "        \"sex\": {\"omop\": \"gender_concept_id\", \"concept_id\": 4135376},",
    "        \"bp_systolic\": {\"omop\": \"measurement\", \"concept_id\": 3004249},",
    "        \"bp_diastolic\": {\"omop\": \"measurement\", \"concept_id\": 3012888},",
    "        \"hba1c\": {\"omop\": \"measurement\", \"concept_id\": 3004410, \"loinc\": \"4548-4\"},",
    "    }",
    "",
    "    mapping = ontology_map.get(variable_name.lower(), {\"omop\": \"unknown\", \"concept_id\": 0})",
    "    return {\"status\": \"success\", \"variable_name\": variable_name, \"mappings\": mapping}",
    "",
    "",
    "def generate_documentation(variable_info: Dict[str, Any]) -> Dict[str, str]:",
    "    \"\"\"Generate human-readable documentation for a variable.\"\"\"",
    "    name = variable_info.get(\"Variable Name\", \"Unknown\")",
    "    field_type = variable_info.get(\"Field Type\", \"text\")",
    "    label = variable_info.get(\"Field Label\", name)",
    "    notes = variable_info.get(\"Notes\", \"No additional notes\")",
    "",
    "    doc = f\"\"\"## Variable: {name}",
    "",
    "**Description:** {label}",
    "",
    "**Technical Details:**",
    "- Data Type: {field_type}",
    "- Cardinality: required",
    "- Notes: {notes}",
    "\"\"\"",
    "    return {\"status\": \"success\", \"documentation\": doc}",
    "",
    "",
    "def save_to_memory(tool_context: ToolContext, key: str, value: str) -> Dict[str, str]:",
    "    \"\"\"Save information to session state.\"\"\"",
    "    tool_context.state[f\"memory:{key}\"] = value",
    "    return {\"status\": \"success\", \"message\": f\"Saved {key} to memory\"}",
    "",
    "",
    "def retrieve_from_memory(tool_context: ToolContext, key: str) -> Dict[str, Any]:",
    "    \"\"\"Retrieve information from session state.\"\"\"",
    "    value = tool_context.state.get(f\"memory:{key}\", \"Not found\")",
    "    return {\"status\": \"success\", \"key\": key, \"value\": value}",
    "",
    "",
    "# Create the root agent",
    "root_agent = LlmAgent(",
    "    name=\"healthcare_documentation_agent\",",
    "    model=\"gemini-2.0-flash-exp\",",
    "    description=\"Agent for generating healthcare data documentation\",",
    "    instruction=\"\"\"You are a Healthcare Data Documentation Agent specialized in:",
    "1. Parsing data dictionaries from various formats",
    "2. Mapping variables to standard healthcare ontologies (OMOP, LOINC, SNOMED)",
    "3. Generating clear, comprehensive documentation",
    "",
    "When a user provides a data dictionary:",
    "1. Use parse_data_dictionary to extract variable information",
    "2. Use map_to_ontology for each variable to find standard codes",
    "3. Use generate_documentation to create human-readable documentation",
    "4. Use save_to_memory to store results for later reference",
    "\"\"\",",
    "    tools=[",
    "        parse_data_dictionary,",
    "        map_to_ontology,",
    "        generate_documentation,",
    "        save_to_memory,",
    "        retrieve_from_memory,",
    "    ],",
    ")",
    "\"\"\"",
    "",
    "with open(f\"{DEPLOY_DIR}/agent.py\", 'w') as f:",
    "    f.write(agent_code)",
    "",
    "print(f\"\u2713 Created {DEPLOY_DIR}/agent.py\")",
    "print(\"  - Includes healthcare-specific tools\")",
    "print(\"  - Uses ADK LlmAgent pattern\")",
    "print(\"  - Integrated session state management\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create requirements.txt for deployment",
    "requirements = \"\"\"google-adk>=1.0.0",
    "google-cloud-aiplatform>=1.38.0",
    "opentelemetry-instrumentation-google-genai",
    "vertexai",
    "\"\"\"",
    "",
    "with open(f\"{DEPLOY_DIR}/requirements.txt\", 'w') as f:",
    "    f.write(requirements)",
    "",
    "print(f\"\u2713 Created {DEPLOY_DIR}/requirements.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create .env configuration",
    "env_config = \"\"\"# Vertex AI Configuration",
    "GOOGLE_CLOUD_PROJECT=your-project-id",
    "GOOGLE_CLOUD_LOCATION=us-central1",
    "GOOGLE_GENAI_USE_VERTEXAI=1",
    "\"\"\"",
    "",
    "with open(f\"{DEPLOY_DIR}/.env\", 'w') as f:",
    "    f.write(env_config)",
    "",
    "print(f\"\u2713 Created {DEPLOY_DIR}/.env\")",
    "print(\"  \u26a0\ufe0f  Remember to update GOOGLE_CLOUD_PROJECT with your project ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create .agent_engine_config.json",
    "deployment_config = {",
    "    \"min_instances\": 0,",
    "    \"max_instances\": 3,",
    "    \"resource_limits\": {",
    "        \"cpu\": \"2\",",
    "        \"memory\": \"4Gi\"",
    "    },",
    "    \"timeout_seconds\": 300,",
    "    \"environment_variables\": {",
    "        \"LOG_LEVEL\": \"INFO\"",
    "    }",
    "}",
    "",
    "with open(f\"{DEPLOY_DIR}/.agent_engine_config.json\", 'w') as f:",
    "    json.dump(deployment_config, f, indent=2)",
    "",
    "print(f\"\u2713 Created {DEPLOY_DIR}/.agent_engine_config.json\")",
    "print(f\"  - Min instances: {deployment_config['min_instances']}\")",
    "print(f\"  - Max instances: {deployment_config['max_instances']}\")",
    "print(f\"  - Resources: {deployment_config['resource_limits']['cpu']} CPU, {deployment_config['resource_limits']['memory']} Memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Using ADK CLI",
    "",
    "Once your deployment files are created, use the ADK CLI to deploy:",
    "",
    "```bash",
    "# Set your project and region",
    "export PROJECT_ID=\"your-project-id\"",
    "export REGION=\"us-central1\"",
    "",
    "# Deploy the agent",
    "adk deploy agent_engine \\",
    "    --project=$PROJECT_ID \\",
    "    --region=$REGION \\",
    "    healthcare_agent_deploy \\",
    "    --agent_engine_config_file=healthcare_agent_deploy/.agent_engine_config.json",
    "```",
    "",
    "The deployment process will:",
    "1. Build a container with your agent code",
    "2. Push to Google Container Registry",
    "3. Deploy to Vertex AI Agent Engine",
    "4. Return the deployment resource name",
    "",
    "**Expected output:**",
    "```",
    "Deploying agent to Vertex AI Agent Engine...",
    "Building container image...",
    "Pushing to Container Registry...",
    "Creating Agent Engine instance...",
    "\u2713 Agent deployed successfully!",
    "Resource name: projects/YOUR_PROJECT/locations/REGION/agents/AGENT_ID",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Your Deployed Agent",
    "",
    "After deployment, test your agent using the Vertex AI SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test code for deployed agent (run AFTER deployment)",
    "# \u26a0\ufe0f Update PROJECT_ID before running",
    "",
    "import vertexai",
    "from vertexai import agent_engines",
    "",
    "PROJECT_ID = \"your-project-id\"  # UPDATE THIS",
    "REGION = \"us-central1\"",
    "",
    "vertexai.init(project=PROJECT_ID, location=REGION)",
    "",
    "# List deployed agents",
    "print(\"Deployed Agents:\")",
    "agents_list = list(agent_engines.list())",
    "for agent in agents_list:",
    "    print(f\"  - {agent.display_name}: {agent.resource_name}\")",
    "",
    "if agents_list:",
    "    remote_agent = agents_list[0]",
    "    ",
    "    # Test data dictionary",
    "    test_data = \"\"\"Variable Name,Field Type,Field Label",
    "patient_id,text,Patient ID",
    "age,integer,Age (years)",
    "hba1c,decimal,HbA1c (%)\"\"\"",
    "    ",
    "    print(f\"\\nTesting agent: {remote_agent.display_name}\")",
    "    print(\"Sending test query...\")",
    "    ",
    "    # Synchronous query (for simple testing)",
    "    response = remote_agent.query(",
    "        message=f\"Parse this data dictionary:\\n{test_data}\",",
    "        user_id=\"test_user_001\",",
    "    )",
    "    print(f\"\\nResponse: {response}\")",
    "else:",
    "    print(\"No deployed agents found. Deploy first using adk deploy command.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nThis notebook provides a complete implementation of an Agent Development Environment (ADE) for Healthcare Data Documentation with the following features:\n\n### Core Components\n\n\u2705 **SQLite Database** - Persistent storage with sessions and memory tables  \n\u2705 **Toon Notation Encoding** - 40-70% token reduction for efficient context  \n\u2705 **Snippet Manager** - Named context storage with extended types (Convention, Changelog, Instrument, Segment, Glossary)  \n\u2705 **Review Queue (HITL)** - Human-in-the-loop approval workflows  \n\u2705 **Multi-Agent Pipeline** - DataParser \u2192 TechnicalAnalyzer \u2192 DomainOntology \u2192 PlainLanguage \u2192 Assembler  \n\u2705 **Session Management** - ADK-style state persistence  \n\u2705 **Memory Services** - Long-term knowledge storage  \n\u2705 **Observability** - Logging and monitoring throughout  \n\n### Extended Agent Capabilities (NEW)\n\n\u2705 **DesignImprovementAgent** - Enhances document structure, readability, and visual hierarchy  \n\u2705 **DataConventionsAgent** - Ensures variable naming standards and coding schemes are documented  \n\u2705 **VersionControlAgent** - Tracks changes, manages semantic versioning, and supports rollbacks  \n\u2705 **HigherLevelDocumentationAgent** - Documents instruments, segments, and codebook structures  \n\n### Extended Orchestrator Features\n\n\u2705 **process_with_extended_agents()** - Full pipeline with all agent capabilities  \n\u2705 **update_documentation()** - Update elements with automatic version control  \n\u2705 **get_element_history()** - View complete version history  \n\u2705 **rollback_element()** - Revert to previous versions  \n\n### Production Deployment\n\n\u2705 **Vertex AI Agent Engine** - Fully managed, auto-scaling infrastructure  \n\u2705 **Extended Tool Set** - 16 tools for comprehensive documentation  \n  - Core: parse_data_dictionary, map_to_ontology, generate_documentation  \n  - Design: improve_document_design, analyze_design_patterns  \n  - Conventions: analyze_variable_conventions, generate_conventions_glossary  \n  - Version Control: create_version, get_version_history, rollback_version, compare_versions  \n  - Higher-Level: identify_instruments, document_instrument, document_segment, generate_codebook_overview  \n\u2705 **Container Deployment** - ADK CLI integration  \n\u2705 **Cloud Monitoring** - Logs, metrics, and alerts  \n\u2705 **Security** - IAM integration and compliance support  \n\n### Key Patterns Implemented\n\n- Retry configuration with exponential backoff\n- Rate limiting for API quota management\n- Context compaction for long conversations\n- Ontology mapping (OMOP, LOINC, SNOMED)\n- Human-readable documentation generation\n- **Semantic versioning** with automatic increment detection\n- **Convention enforcement** with consistency scoring\n- **Instrument identification** based on variable prefixes\n- **Design improvement** with measurable quality metrics\n\n### Next Steps\n\n1. **Customize agents** for your specific healthcare domain\n2. **Add evaluation test cases** using ADK eval framework\n3. **Implement A2A protocol** for agent-to-agent communication\n4. **Set up continuous deployment** pipeline\n5. **Add custom observability plugins** for your metrics\n6. **Configure convention rules** for your organization's standards\n7. **Define instrument templates** for common measurement tools\n\nFor more information, see:\n- [ADK Documentation](https://google.github.io/adk-docs/)\n- [Vertex AI Agent Engine](https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/overview)\n- [OMOP CDM](https://ohdsi.github.io/CommonDataModel/)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Production Considerations",
    "",
    "When deploying to production:",
    "",
    "1. **Authentication & Security**",
    "   - Use service accounts with minimal required permissions",
    "   - Enable VPC Service Controls for data protection",
    "   - Configure Cloud Armor for DDoS protection",
    "",
    "2. **Scaling**",
    "   - Set appropriate min/max instances based on expected load",
    "   - Monitor cold start times and adjust accordingly",
    "   - Use connection pooling for database connections",
    "",
    "3. **Monitoring**",
    "   - Set up alerts for error rates and latency",
    "   - Monitor token usage and costs",
    "   - Track session memory usage",
    "",
    "4. **Data Compliance**",
    "   - Ensure HIPAA compliance for healthcare data",
    "   - Implement audit logging",
    "   - Configure data retention policies",
    "",
    "5. **Cost Optimization**",
    "   - Use preemptible instances for non-critical workloads",
    "   - Set min_instances to 0 for development",
    "   - Monitor and optimize API call frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nThis notebook provides a complete implementation of an Agent Development Environment (ADE) for Healthcare Data Documentation with the following features:\n\n### Core Components\n\n\u2705 **SQLite Database** - Persistent storage with sessions and memory tables  \n\u2705 **Toon Notation Encoding** - 40-70% token reduction for efficient context  \n\u2705 **Snippet Manager** - Named context storage and retrieval  \n\u2705 **Review Queue (HITL)** - Human-in-the-loop approval workflows  \n\u2705 **Multi-Agent Pipeline** - DataParser \u2192 TechnicalAnalyzer \u2192 DomainOntology \u2192 PlainLanguage \u2192 Assembler  \n\u2705 **Session Management** - ADK-style state persistence  \n\u2705 **Memory Services** - Long-term knowledge storage  \n\u2705 **Observability** - Logging and monitoring throughout  \n\n### Production Deployment\n\n\u2705 **Vertex AI Agent Engine** - Fully managed, auto-scaling infrastructure  \n\u2705 **Container Deployment** - ADK CLI integration  \n\u2705 **Cloud Monitoring** - Logs, metrics, and alerts  \n\u2705 **Security** - IAM integration and compliance support  \n\n### Key Patterns Implemented\n\n- Retry configuration with exponential backoff\n- Rate limiting for API quota management\n- Context compaction for long conversations\n- Ontology mapping (OMOP, LOINC, SNOMED)\n- Human-readable documentation generation\n\n### Next Steps\n\n1. **Customize agents** for your specific healthcare domain\n2. **Add evaluation test cases** using ADK eval framework\n3. **Implement A2A protocol** for agent-to-agent communication\n4. **Set up continuous deployment** pipeline\n5. **Add custom observability plugins** for your metrics\n\nFor more information, see:\n- [ADK Documentation](https://google.github.io/adk-docs/)\n- [Vertex AI Agent Engine](https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/overview)\n- [OMOP CDM](https://ohdsi.github.io/CommonDataModel/)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}