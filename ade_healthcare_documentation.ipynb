{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Agent Development Environment (ADE) for Healthcare Data Documentation\n\n**Version 2.0 - November 2025**\n\nThis notebook implements a production-ready agent development environment using Google's Agent Development Kit (ADK) patterns for healthcare data documentation.\n\n## Key Features\n- **Modern ADK Architecture**: Sessions, memory services, and async patterns\n- **Toon Notation**: Compact encoding for 40-70% token reduction\n- **Snippet Manager**: Named context storage for efficient retrieval\n- **Batch Processing**: Handle large codebooks with automatic chunking\n- **Human-in-the-Loop (HITL)**: Review workflows with approval/rejection cycles\n- **Multi-Agent Orchestration**: Specialized agents for parsing, analysis, and documentation\n- **Observability**: Logging plugins and monitoring capabilities\n- **Production Deployment**: Vertex AI Agent Engine ready\n\n## Architecture Overview\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Input     â”‚â”€â”€â”€â”€â–¶â”‚  Orchestrator â”‚â”€â”€â”€â”€â–¶â”‚  Review Queue   â”‚\nâ”‚   Data      â”‚     â”‚   (Runner)    â”‚     â”‚    (HITL)       â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                           â”‚\n                    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”\n                    â–¼             â–¼\n              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n              â”‚  Agents  â”‚  â”‚  Snippet â”‚\n              â”‚          â”‚  â”‚  Manager â”‚\n              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages",
    "!pip install -q google-generativeai google-adk sqlite3 pandas numpy opentelemetry-instrumentation-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3",
    "import json",
    "import pandas as pd",
    "import numpy as np",
    "from datetime import datetime",
    "from typing import Dict, List, Optional, Any, Tuple",
    "from enum import Enum",
    "import google.generativeai as genai",
    "from dataclasses import dataclass, asdict, field",
    "import hashlib",
    "import os",
    "import time",
    "import asyncio",
    "import logging",
    "",
    "# Set up logging for observability",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')",
    "logger = logging.getLogger('ADE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Google Gemini API",
    "from google.colab import userdata",
    "",
    "api_key = userdata.get('GOOGLE_API_KEY')",
    "genai.configure(api_key=api_key)",
    "",
    "print(\"âœ“ Gemini API configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. API Configuration and Rate Limits",
    "",
    "Configure rate limiting based on your Gemini API tier for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass",
    "class APIConfig:",
    "    \"\"\"Configuration for API rate limits and retry behavior.\"\"\"",
    "    requests_per_minute: int = 10",
    "    max_retries: int = 3",
    "    base_retry_delay: float = 6.0",
    "    model_name: str = \"gemini-2.0-flash-exp\"",
    "    ",
    "    def __post_init__(self):",
    "        self.min_delay = 60.0 / self.requests_per_minute",
    "",
    "",
    "class APITier:",
    "    \"\"\"Predefined API configurations for different Gemini tiers.\"\"\"",
    "    ",
    "    FREE = APIConfig(requests_per_minute=10, max_retries=3, base_retry_delay=6.0)",
    "    PAYG = APIConfig(requests_per_minute=360, max_retries=3, base_retry_delay=2.0)",
    "    ENTERPRISE = APIConfig(requests_per_minute=1000, max_retries=2, base_retry_delay=1.0)",
    "    CONSERVATIVE = APIConfig(requests_per_minute=8, max_retries=5, base_retry_delay=8.0)",
    "    ",
    "    @staticmethod",
    "    def custom(requests_per_minute: int, **kwargs) -> APIConfig:",
    "        return APIConfig(requests_per_minute=requests_per_minute, **kwargs)",
    "",
    "",
    "# Set your tier here",
    "API_CONFIG = APITier.FREE",
    "",
    "print(f\"ðŸ“Š API Configuration:\")",
    "print(f\"   Requests/minute: {API_CONFIG.requests_per_minute}\")",
    "print(f\"   Min delay: {API_CONFIG.min_delay:.1f}s\")",
    "print(f\"   Model: {API_CONFIG.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Database Schema and Setup",
    "",
    "SQLite database provides persistent storage for sessions, memory, and HITL workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class DatabaseManager:\n    \"\"\"Manages SQLite database operations with session and memory support.\"\"\"\n    \n    def __init__(self, db_path: str = \"project.db\"):\n        self.db_path = db_path\n        self.conn = None\n        self.cursor = None\n    \n    def connect(self):\n        \"\"\"Establish database connection.\"\"\"\n        self.conn = sqlite3.connect(self.db_path)\n        self.conn.row_factory = sqlite3.Row\n        self.cursor = self.conn.cursor()\n    \n    def close(self):\n        \"\"\"Close database connection.\"\"\"\n        if self.conn:\n            self.conn.close()\n    \n    def execute_query(self, query: str, params: tuple = ()) -> List[Dict]:\n        \"\"\"Execute SELECT query and return results.\"\"\"\n        self.cursor.execute(query, params)\n        rows = self.cursor.fetchall()\n        return [dict(row) for row in rows]\n    \n    def execute_update(self, query: str, params: tuple = ()) -> int:\n        \"\"\"Execute INSERT/UPDATE/DELETE and return affected row ID.\"\"\"\n        self.cursor.execute(query, params)\n        self.conn.commit()\n        return self.cursor.lastrowid\n    \n    def initialize_schema(self):\n        \"\"\"Create all required tables.\"\"\"\n        \n        # Agents table\n        self.cursor.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS Agents (\n            agent_id INTEGER PRIMARY KEY AUTOINCREMENT,\n            name TEXT NOT NULL UNIQUE,\n            system_prompt TEXT NOT NULL,\n            agent_type TEXT NOT NULL,\n            config JSON,\n            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP\n        )\n        \"\"\")\n        \n        # Snippets table - Named context storage\n        self.cursor.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS Snippets (\n            snippet_id INTEGER PRIMARY KEY AUTOINCREMENT,\n            name TEXT NOT NULL UNIQUE,\n            snippet_type TEXT NOT NULL CHECK(snippet_type IN (\n                'Summary', 'Chunk', 'Instruction',\n                'Version', 'Design', 'Mapping'\n            )),\n            content TEXT NOT NULL,\n            metadata JSON,\n            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP\n        )\n        \"\"\")\n        \n        # Jobs table with enhanced metadata\n        self.cursor.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS Jobs (\n            job_id TEXT PRIMARY KEY,\n            source_file TEXT NOT NULL,\n            status TEXT NOT NULL DEFAULT 'Running' CHECK(status IN (\n                'Running', 'Completed', 'Failed', 'Paused'\n            )),\n            metadata JSON,\n            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP\n        )\n        \"\"\")\n        \n        # ReviewQueue table - HITL workflow\n        self.cursor.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS ReviewQueue (\n            item_id INTEGER PRIMARY KEY AUTOINCREMENT,\n            job_id TEXT NOT NULL,\n            status TEXT NOT NULL DEFAULT 'Pending' CHECK(status IN (\n                'Pending', 'Approved', 'Rejected', 'Needs_Clarification'\n            )),\n            source_agent TEXT NOT NULL,\n            target_agent TEXT,\n            source_data TEXT NOT NULL,\n            generated_content TEXT NOT NULL,\n            approved_content TEXT,\n            rejection_feedback TEXT,\n            clarification_response TEXT,\n            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n            FOREIGN KEY (job_id) REFERENCES Jobs(job_id)\n        )\n        \"\"\")\n        \n        # Sessions table - ADK-style session management\n        self.cursor.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS Sessions (\n            session_id TEXT PRIMARY KEY,\n            job_id TEXT NOT NULL,\n            user_id TEXT NOT NULL,\n            state JSON DEFAULT '{}',\n            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n            FOREIGN KEY (job_id) REFERENCES Jobs(job_id)\n        )\n        \"\"\")\n        \n        # SessionHistory - Conversation history\n        self.cursor.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS SessionHistory (\n            history_id INTEGER PRIMARY KEY AUTOINCREMENT,\n            session_id TEXT NOT NULL,\n            job_id TEXT NOT NULL,\n            role TEXT NOT NULL CHECK(role IN ('user', 'assistant', 'system', 'tool')),\n            content TEXT NOT NULL,\n            metadata JSON,\n            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n            FOREIGN KEY (session_id) REFERENCES Sessions(session_id),\n            FOREIGN KEY (job_id) REFERENCES Jobs(job_id)\n        )\n        \"\"\")\n        \n        # Memory table - Long-term knowledge storage\n        self.cursor.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS Memory (\n            memory_id INTEGER PRIMARY KEY AUTOINCREMENT,\n            user_id TEXT NOT NULL,\n            content TEXT NOT NULL,\n            embedding JSON,\n            metadata JSON,\n            created_at DATETIME DEFAULT CURRENT_TIMESTAMP\n        )\n        \"\"\")\n        \n        # SystemState table\n        self.cursor.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS SystemState (\n            state_key TEXT PRIMARY KEY,\n            state_value TEXT NOT NULL,\n            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP\n        )\n        \"\"\")\n        \n        self.conn.commit()\n        print(\"âœ“ Database schema initialized with session and memory support\")\n\n# Initialize database\ndb = DatabaseManager(\"project.db\")\ndb.connect()\ndb.initialize_schema()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Toon Notation Encoding\n\nCompact data encoding that reduces token usage by 40-70% while preserving all information."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class ToonNotation:\n    \"\"\"\n    Compact notation for encoding data to maximize context efficiency.\n    Reduces token usage by 40-70% compared to standard JSON.\n    \"\"\"\n    \n    @staticmethod\n    def _needs_quoting(value: str) -> bool:\n        \"\"\"Check if a string value needs quotes to avoid ambiguity.\"\"\"\n        if not isinstance(value, str):\n            return False\n        if ',' in value or ':' in value:\n            return True\n        if value.lower() in ['true', 'false', 'null', 'none']:\n            return True\n        try:\n            float(value)\n            return True\n        except:\n            return False\n    \n    @staticmethod\n    def _is_tabular(arr: list) -> bool:\n        \"\"\"Check if array is uniform objects (tabular format).\"\"\"\n        if not arr or not isinstance(arr[0], dict):\n            return False\n        keys = set(arr[0].keys())\n        return all(isinstance(item, dict) and set(item.keys()) == keys for item in arr)\n    \n    @staticmethod\n    def encode(data: Any, indent: int = 0) -> str:\n        \"\"\"Encode data in Toon notation for token-efficient context.\"\"\"\n        prefix = \"  \" * indent\n        \n        if data is None:\n            return \"null\"\n        if isinstance(data, bool):\n            return str(data).lower()\n        if isinstance(data, (int, float)):\n            return str(data)\n        if isinstance(data, str):\n            return f'\"{data}\"' if ToonNotation._needs_quoting(data) else data\n        \n        if isinstance(data, dict) and not data:\n            return \"\"\n        if isinstance(data, list) and not data:\n            return \"[0]:\"\n        \n        if isinstance(data, list):\n            if ToonNotation._is_tabular(data):\n                keys = list(data[0].keys())\n                header = f\"[{len(data)}]{{{','.join(keys)}}}:\"\n                rows = []\n                for item in data:\n                    row_vals = [str(item[k]) if item[k] is not None else \"null\" for k in keys]\n                    rows.append(\"  \" + \",\".join(row_vals))\n                return header + \"\\n\" + \"\\n\".join(rows)\n            else:\n                items = [ToonNotation.encode(item, indent + 1) for item in data]\n                return f\"[{len(data)}]: \" + \",\".join(items)\n        \n        if isinstance(data, dict):\n            lines = []\n            for key, value in data.items():\n                if isinstance(value, dict):\n                    lines.append(f\"{prefix}{key}:\")\n                    lines.append(ToonNotation.encode(value, indent + 1))\n                elif isinstance(value, list) and ToonNotation._is_tabular(value):\n                    encoded = ToonNotation.encode(value, indent)\n                    lines.append(f\"{prefix}{key}{encoded}\")\n                else:\n                    encoded = ToonNotation.encode(value, indent)\n                    lines.append(f\"{prefix}{key}: {encoded}\")\n            return \"\\n\".join(lines)\n        \n        return str(data)\n    \n    @staticmethod\n    def decode(toon_str: str) -> Any:\n        \"\"\"Decode Toon notation back to Python objects (basic implementation).\"\"\"\n        pass\n\nprint(\"âœ“ ToonNotation encoder loaded\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class SnippetType(Enum):\n    \"\"\"Enumeration of snippet types for context management.\"\"\"\n    SUMMARY = \"Summary\"\n    CHUNK = \"Chunk\"\n    INSTRUCTION = \"Instruction\"\n    VERSION = \"Version\"\n    DESIGN = \"Design\"\n    MAPPING = \"Mapping\"\n\n@dataclass\nclass Snippet:\n    \"\"\"Represents a named context snippet.\"\"\"\n    name: str\n    snippet_type: SnippetType\n    content: str\n    metadata: Optional[Dict[str, Any]] = None\n    snippet_id: Optional[int] = None\n\nclass SnippetManager:\n    \"\"\"Manages the Snippet Library for named context storage and retrieval.\"\"\"\n    \n    def __init__(self, db_manager: DatabaseManager):\n        self.db = db_manager\n    \n    def create_snippet(self, name: str, snippet_type: SnippetType, content: str,\n                      metadata: Optional[Dict] = None) -> int:\n        \"\"\"Create a new snippet in the library.\"\"\"\n        query = \"\"\"\n        INSERT INTO Snippets (name, snippet_type, content, metadata)\n        VALUES (?, ?, ?, ?)\n        \"\"\"\n        metadata_json = json.dumps(metadata) if metadata else None\n        snippet_id = self.db.execute_update(query, (name, snippet_type.value, content, metadata_json))\n        logger.info(f\"Created Snippet '{name}' (ID: {snippet_id})\")\n        return snippet_id\n    \n    def get_snippet_by_name(self, name: str) -> Optional[Snippet]:\n        \"\"\"Retrieve a snippet by name.\"\"\"\n        query = \"SELECT * FROM Snippets WHERE name = ?\"\n        result = self.db.execute_query(query, (name,))\n        if result:\n            row = result[0]\n            return Snippet(\n                snippet_id=row['snippet_id'],\n                name=row['name'],\n                snippet_type=SnippetType(row['snippet_type']),\n                content=row['content'],\n                metadata=json.loads(row['metadata']) if row['metadata'] else None\n            )\n        return None\n    \n    def update_snippet(self, snippet_id: int, content: str = None, metadata: Dict = None):\n        \"\"\"Update an existing snippet.\"\"\"\n        if content:\n            self.db.execute_update(\n                \"UPDATE Snippets SET content = ?, updated_at = CURRENT_TIMESTAMP WHERE snippet_id = ?\",\n                (content, snippet_id)\n            )\n        if metadata:\n            self.db.execute_update(\n                \"UPDATE Snippets SET metadata = ?, updated_at = CURRENT_TIMESTAMP WHERE snippet_id = ?\",\n                (json.dumps(metadata), snippet_id)\n            )\n    \n    def list_snippets(self, snippet_type: Optional[SnippetType] = None) -> List[Snippet]:\n        \"\"\"List all snippets, optionally filtered by type.\"\"\"\n        if snippet_type:\n            query = \"SELECT * FROM Snippets WHERE snippet_type = ?\"\n            results = self.db.execute_query(query, (snippet_type.value,))\n        else:\n            query = \"SELECT * FROM Snippets\"\n            results = self.db.execute_query(query)\n        \n        return [\n            Snippet(\n                snippet_id=row['snippet_id'],\n                name=row['name'],\n                snippet_type=SnippetType(row['snippet_type']),\n                content=row['content'],\n                metadata=json.loads(row['metadata']) if row['metadata'] else None\n            )\n            for row in results\n        ]\n    \n    def delete_snippet(self, snippet_id: int):\n        \"\"\"Delete a snippet from the library.\"\"\"\n        self.db.execute_update(\"DELETE FROM Snippets WHERE snippet_id = ?\", (snippet_id,))\n        logger.info(f\"Deleted Snippet ID: {snippet_id}\")\n\nprint(\"âœ“ SnippetManager loaded for context storage\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Human-in-the-Loop Review Queue",
    "",
    "The ReviewQueue manages approval workflows for generated content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass",
    "class ReviewItem:",
    "    \"\"\"Represents an item in the review queue.\"\"\"",
    "    item_id: int",
    "    job_id: str",
    "    status: str",
    "    source_agent: str",
    "    target_agent: Optional[str]",
    "    source_data: str",
    "    generated_content: str",
    "    approved_content: Optional[str] = None",
    "    rejection_feedback: Optional[str] = None",
    "",
    "",
    "class ReviewQueueManager:",
    "    \"\"\"Manages the HITL review workflow.\"\"\"",
    "    ",
    "    def __init__(self, db_manager: DatabaseManager):",
    "        self.db = db_manager",
    "    ",
    "    def add_item(self, job_id: str, source_agent: str, source_data: str,",
    "                 generated_content: str, target_agent: Optional[str] = None) -> int:",
    "        \"\"\"Add an item to the review queue.\"\"\"",
    "        query = \"\"\"",
    "        INSERT INTO ReviewQueue (job_id, source_agent, target_agent, source_data, generated_content)",
    "        VALUES (?, ?, ?, ?, ?)",
    "        \"\"\"",
    "        item_id = self.db.execute_update(",
    "            query, (job_id, source_agent, target_agent, source_data, generated_content)",
    "        )",
    "        logger.info(f\"Added review item {item_id} from {source_agent}\")",
    "        return item_id",
    "    ",
    "    def get_pending_items(self, job_id: str) -> List[ReviewItem]:",
    "        \"\"\"Get all pending review items for a job.\"\"\"",
    "        query = \"SELECT * FROM ReviewQueue WHERE job_id = ? AND status = 'Pending'\"",
    "        results = self.db.execute_query(query, (job_id,))",
    "        return [",
    "            ReviewItem(",
    "                item_id=row['item_id'],",
    "                job_id=row['job_id'],",
    "                status=row['status'],",
    "                source_agent=row['source_agent'],",
    "                target_agent=row['target_agent'],",
    "                source_data=row['source_data'],",
    "                generated_content=row['generated_content'],",
    "                approved_content=row['approved_content'],",
    "                rejection_feedback=row['rejection_feedback']",
    "            )",
    "            for row in results",
    "        ]",
    "    ",
    "    def approve_item(self, item_id: int, approved_content: Optional[str] = None):",
    "        \"\"\"Approve a review item.\"\"\"",
    "        if approved_content:",
    "            query = \"\"\"",
    "            UPDATE ReviewQueue ",
    "            SET status = 'Approved', approved_content = ?, updated_at = CURRENT_TIMESTAMP",
    "            WHERE item_id = ?",
    "            \"\"\"",
    "            self.db.execute_update(query, (approved_content, item_id))",
    "        else:",
    "            query = \"\"\"",
    "            UPDATE ReviewQueue ",
    "            SET status = 'Approved', approved_content = generated_content, updated_at = CURRENT_TIMESTAMP",
    "            WHERE item_id = ?",
    "            \"\"\"",
    "            self.db.execute_update(query, (item_id,))",
    "        logger.info(f\"Approved review item {item_id}\")",
    "    ",
    "    def reject_item(self, item_id: int, feedback: str):",
    "        \"\"\"Reject a review item with feedback.\"\"\"",
    "        query = \"\"\"",
    "        UPDATE ReviewQueue ",
    "        SET status = 'Rejected', rejection_feedback = ?, updated_at = CURRENT_TIMESTAMP",
    "        WHERE item_id = ?",
    "        \"\"\"",
    "        self.db.execute_update(query, (feedback, item_id))",
    "        logger.info(f\"Rejected review item {item_id}\")",
    "    ",
    "    def get_approved_items(self, job_id: str) -> List[ReviewItem]:",
    "        \"\"\"Get all approved items for a job.\"\"\"",
    "        query = \"SELECT * FROM ReviewQueue WHERE job_id = ? AND status = 'Approved'\"",
    "        results = self.db.execute_query(query, (job_id,))",
    "        return [",
    "            ReviewItem(",
    "                item_id=row['item_id'],",
    "                job_id=row['job_id'],",
    "                status=row['status'],",
    "                source_agent=row['source_agent'],",
    "                target_agent=row['target_agent'],",
    "                source_data=row['source_data'],",
    "                generated_content=row['generated_content'],",
    "                approved_content=row['approved_content'],",
    "                rejection_feedback=row['rejection_feedback']",
    "            )",
    "            for row in results",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Core Agent Classes",
    "",
    "Specialized agents with retry logic, rate limiting, and Toon context injection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class BaseAgent:\n    \"\"\"Base class for all agents with rate limiting, retry logic, and observability.\"\"\"\n    \n    def __init__(self, name: str, system_prompt: str, config: APIConfig = None):\n        self.name = name\n        self.system_prompt = system_prompt\n        self.config = config or API_CONFIG\n        self.model = genai.GenerativeModel(self.config.model_name)\n        self.active_snippets: List[Snippet] = []\n        self.last_request_time = 0\n        self.request_count = 0\n        self.logger = logging.getLogger(f'ADE.{name}')\n    \n    def inject_snippets(self, snippets: List[Snippet]):\n        \"\"\"Inject context snippets into agent.\"\"\"\n        self.active_snippets = snippets\n        self.logger.info(f\"Injected {len(snippets)} snippets\")\n    \n    def build_prompt(self, user_input: str, additional_context: str = \"\") -> str:\n        \"\"\"Build the full prompt with system prompt, snippets, and user input.\"\"\"\n        prompt_parts = [self.system_prompt]\n        \n        if self.active_snippets:\n            prompt_parts.append(\"\\n=== CONTEXT (Snippets) ===\")\n            for snippet in self.active_snippets:\n                prompt_parts.append(f\"\\n[{snippet.snippet_type.value}: {snippet.name}]\")\n                prompt_parts.append(snippet.content)\n        \n        if additional_context:\n            prompt_parts.append(\"\\n=== ADDITIONAL CONTEXT ===\")\n            prompt_parts.append(additional_context)\n        \n        prompt_parts.append(\"\\n=== INPUT ===\")\n        prompt_parts.append(user_input)\n        \n        return \"\\n\".join(prompt_parts)\n    \n    def _wait_for_rate_limit(self):\n        \"\"\"Implement rate limiting by waiting if necessary.\"\"\"\n        if self.last_request_time > 0:\n            elapsed = time.time() - self.last_request_time\n            if elapsed < self.config.min_delay:\n                wait_time = self.config.min_delay - elapsed\n                print(f\"â±ï¸  Rate limiting: waiting {wait_time:.1f}s...\")\n                time.sleep(wait_time)\n    \n    def generate(self, prompt: str) -> str:\n        \"\"\"Generate response with retry logic and rate limiting.\"\"\"\n        for attempt in range(self.config.max_retries):\n            try:\n                self._wait_for_rate_limit()\n                self.last_request_time = time.time()\n                self.request_count += 1\n                \n                response = self.model.generate_content(prompt)\n                self.logger.info(f\"Request {self.request_count} successful\")\n                return response.text\n                \n            except Exception as e:\n                error_str = str(e)\n                if \"429\" in error_str or \"quota\" in error_str.lower():\n                    wait_time = self.config.base_retry_delay * (2 ** attempt)\n                    self.logger.warning(f\"Rate limit hit, retrying in {wait_time}s (attempt {attempt + 1})\")\n                    print(f\"âš ï¸  Rate limit hit, waiting {wait_time}s before retry {attempt + 1}/{self.config.max_retries}\")\n                    time.sleep(wait_time)\n                else:\n                    self.logger.error(f\"API error: {error_str}\")\n                    raise\n        \n        raise Exception(f\"Max retries ({self.config.max_retries}) exceeded\")\n    \n    def process(self, user_input: str, additional_context: str = \"\") -> str:\n        \"\"\"Process input through the agent.\"\"\"\n        prompt = self.build_prompt(user_input, additional_context)\n        return self.generate(prompt)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataParserAgent(BaseAgent):",
    "    \"\"\"Agent for parsing raw data into standardized JSON format.\"\"\"",
    "    ",
    "    def __init__(self, config: APIConfig = None):",
    "        system_prompt = \"\"\"You are a DataParserAgent specialized in converting raw data specifications into standardized JSON format.",
    "",
    "Your task:",
    "1. Parse the input data (CSV, JSON, or XML)",
    "2. Preserve all original field names and values",
    "3. Output a JSON array where each element represents one variable/field",
    "4. Include: original_name, original_type, original_description, and any metadata",
    "",
    "Output format:",
    "```json",
    "[",
    "  {",
    "    \"original_name\": \"field_name\",",
    "    \"original_type\": \"type\",",
    "    \"original_description\": \"description\",",
    "    \"metadata\": {}",
    "  }",
    "]",
    "```",
    "",
    "Only output valid JSON. No additional commentary.\"\"\"",
    "        super().__init__(\"DataParserAgent\", system_prompt, config)",
    "    ",
    "    def parse_csv(self, csv_data: str) -> List[Dict]:",
    "        \"\"\"Parse CSV data dictionary.\"\"\"",
    "        result = self.process(csv_data)",
    "        if \"```json\" in result:",
    "            result = result.split(\"```json\")[1].split(\"```\")[0].strip()",
    "        elif \"```\" in result:",
    "            result = result.split(\"```\")[1].split(\"```\")[0].strip()",
    "        return json.loads(result)",
    "",
    "",
    "class TechnicalAnalyzerAgent(BaseAgent):",
    "    \"\"\"Agent for analyzing technical properties and mapping to internal standards.\"\"\"",
    "    ",
    "    def __init__(self, config: APIConfig = None):",
    "        system_prompt = \"\"\"You are a TechnicalAnalyzerAgent specialized in analyzing data fields and mapping them to internal standards.",
    "",
    "**Input Format: Toon Notation**",
    "Input data is provided in Toon notation (compact format):",
    "- `key: value` for simple fields",
    "- `key[n]{col1,col2}:` followed by data rows for tabular data",
    "",
    "Your task:",
    "1. Analyze each field from the parsed data",
    "2. Infer technical properties (data_type, constraints, cardinality)",
    "3. Map to standardized field names following healthcare data conventions",
    "4. Flag unclear mappings for clarification",
    "",
    "Output format:",
    "```json",
    "[",
    "  {",
    "    \"original_name\": \"field_name\",",
    "    \"variable_name\": \"standardized_name\",",
    "    \"data_type\": \"categorical|continuous|date|text|boolean\",",
    "    \"description\": \"description\",",
    "    \"constraints\": {},",
    "    \"cardinality\": \"required|optional|repeated\",",
    "    \"confidence\": \"high|medium|low\",",
    "    \"needs_clarification\": false,",
    "    \"clarification_question\": \"\"",
    "  }",
    "]",
    "```",
    "",
    "Only output valid JSON. No additional commentary.\"\"\"",
    "        super().__init__(\"TechnicalAnalyzerAgent\", system_prompt, config)",
    "    ",
    "    def analyze(self, parsed_data: List[Dict], clarifications: Optional[Dict[str, str]] = None) -> List[Dict]:",
    "        \"\"\"Analyze parsed data and map to internal standards.\"\"\"",
    "        additional_context = \"\"",
    "        if clarifications:",
    "            additional_context = \"\\n=== USER CLARIFICATIONS ===\\n\"",
    "            for field, clarification in clarifications.items():",
    "                additional_context += f\"{field}: {clarification}\\n\"",
    "        ",
    "        toon_encoded = ToonNotation.encode({\"variables\": parsed_data})",
    "        format_context = \"\\nData is in Toon notation format. Output JSON as specified.\\n\"",
    "        result = self.process(toon_encoded, format_context + additional_context)",
    "        ",
    "        if \"```json\" in result:",
    "            result = result.split(\"```json\")[1].split(\"```\")[0].strip()",
    "        elif \"```\" in result:",
    "            result = result.split(\"```\")[1].split(\"```\")[0].strip()",
    "        return json.loads(result)",
    "",
    "",
    "class DomainOntologyAgent(BaseAgent):",
    "    \"\"\"Agent for mapping to standard healthcare ontologies.\"\"\"",
    "    ",
    "    def __init__(self, config: APIConfig = None):",
    "        system_prompt = \"\"\"You are a DomainOntologyAgent specialized in mapping healthcare data fields to standard ontologies.",
    "",
    "Your task:",
    "1. For each variable, identify appropriate standard ontology codes",
    "2. Primary ontologies: OMOP CDM, LOINC, SNOMED CT, RxNorm",
    "3. Provide code and standard term",
    "4. Include confidence score for each mapping",
    "",
    "Output format:",
    "```json",
    "{",
    "  \"variable_name\": \"standardized_name\",",
    "  \"ontology_mappings\": [",
    "    {",
    "      \"system\": \"OMOP\",",
    "      \"code\": \"123456\",",
    "      \"display\": \"Standard Concept Name\",",
    "      \"confidence\": \"high\"",
    "    }",
    "  ]",
    "}",
    "```",
    "",
    "Only output valid JSON. No additional commentary.\"\"\"",
    "        super().__init__(\"DomainOntologyAgent\", system_prompt, config)",
    "    ",
    "    def map_ontologies(self, variable_data: Dict) -> Dict:",
    "        \"\"\"Map a variable to standard ontologies.\"\"\"",
    "        toon_encoded = ToonNotation.encode(variable_data)",
    "        result = self.process(toon_encoded, \"\\nInput is in Toon notation. Output JSON.\\n\")",
    "        ",
    "        if \"```json\" in result:",
    "            result = result.split(\"```json\")[1].split(\"```\")[0].strip()",
    "        elif \"```\" in result:",
    "            result = result.split(\"```\")[1].split(\"```\")[0].strip()",
    "        return json.loads(result)",
    "",
    "",
    "class PlainLanguageAgent(BaseAgent):",
    "    \"\"\"Agent for generating human-readable documentation.\"\"\"",
    "    ",
    "    def __init__(self, config: APIConfig = None):",
    "        system_prompt = \"\"\"You are a PlainLanguageAgent specialized in creating clear, comprehensive documentation for healthcare data variables.",
    "",
    "Your task:",
    "1. Convert technical variable specifications into plain language",
    "2. Explain clinical/research context",
    "3. Describe data type, constraints, and valid values",
    "4. Include ontology mappings and significance",
    "5. Write for interdisciplinary audience (clinicians, researchers, data scientists)",
    "",
    "Output format (Markdown):",
    "```markdown",
    "## Variable: [Variable Name]",
    "",
    "**Description:** [Clear, concise description]",
    "",
    "**Technical Details:**",
    "- Data Type: [type]",
    "- Cardinality: [required/optional]",
    "- Valid Values: [constraints or ranges]",
    "",
    "**Standard Ontology Mappings:**",
    "- OMOP: [code] - [term]",
    "- LOINC: [code] - [term]",
    "",
    "**Clinical Context:** [Explanation of why this variable matters]",
    "```",
    "",
    "Only output Markdown documentation. No additional commentary.\"\"\"",
    "        super().__init__(\"PlainLanguageAgent\", system_prompt, config)",
    "    ",
    "    def document_variable(self, enriched_data: Dict) -> str:",
    "        \"\"\"Generate plain language documentation for a variable.\"\"\"",
    "        toon_encoded = ToonNotation.encode(enriched_data)",
    "        result = self.process(toon_encoded, \"\\nInput is in Toon notation. Generate markdown.\\n\")",
    "        ",
    "        if \"```markdown\" in result:",
    "            result = result.split(\"```markdown\")[1].split(\"```\")[0].strip()",
    "        elif result.startswith(\"```\") and result.endswith(\"```\"):",
    "            result = result.split(\"```\")[1].split(\"```\")[0].strip()",
    "        return result",
    "",
    "",
    "class DocumentationAssemblerAgent(BaseAgent):",
    "    \"\"\"Agent for assembling final documentation from approved items.\"\"\"",
    "    ",
    "    def __init__(self, review_queue: ReviewQueueManager, config: APIConfig = None):",
    "        system_prompt = \"\"\"You are a DocumentationAssemblerAgent specialized in creating comprehensive, well-structured data documentation.",
    "",
    "Your task:",
    "1. Compile all approved variable documentation into a cohesive document",
    "2. Add a table of contents",
    "3. Include metadata (generation date, source file, etc.)",
    "4. Organize by logical groupings if applicable",
    "5. Ensure consistent formatting throughout",
    "",
    "Output: A complete Markdown document ready for publication.\"\"\"",
    "        super().__init__(\"DocumentationAssemblerAgent\", system_prompt, config)",
    "        self.review_queue = review_queue",
    "    ",
    "    def assemble(self, job_id: str) -> str:",
    "        \"\"\"Assemble final documentation from approved review items.\"\"\"",
    "        approved_items = self.review_queue.get_approved_items(job_id)",
    "        ",
    "        if not approved_items:",
    "            return \"# No approved documentation found for this job.\"",
    "        ",
    "        doc_parts = [",
    "            \"# Healthcare Data Documentation\",",
    "            f\"\\n**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",",
    "            f\"**Job ID:** {job_id}\",",
    "            \"\\n---\\n\"",
    "        ]",
    "        ",
    "        doc_parts.append(\"## Table of Contents\\n\")",
    "        for i, item in enumerate(approved_items, 1):",
    "            content = item.approved_content",
    "            if \"## Variable:\" in content:",
    "                var_name = content.split(\"## Variable:\")[1].split(\"\\n\")[0].strip()",
    "                doc_parts.append(f\"{i}. [{var_name}](#{var_name.lower().replace(' ', '-')})\")",
    "        ",
    "        doc_parts.append(\"\\n---\\n\")",
    "        ",
    "        for item in approved_items:",
    "            doc_parts.append(item.approved_content)",
    "            doc_parts.append(\"\\n---\\n\")",
    "        ",
    "        return \"\\n\".join(doc_parts)",
    "",
    "",
    "print(\"âœ“ All agent classes defined with Toon support and observability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Orchestrator - Agent Workflow Management",
    "",
    "The Orchestrator manages data flow through the agent pipeline and coordinates HITL workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class Orchestrator:\n    \"\"\"Manages the workflow of agents and coordinates the documentation pipeline.\"\"\"\n    \n    def __init__(self, db_manager: DatabaseManager, api_config: APIConfig = None):\n        self.db = db_manager\n        self.config = api_config or API_CONFIG\n        self.snippet_manager = SnippetManager(db_manager)\n        self.review_queue = ReviewQueueManager(db_manager)\n        \n        # Initialize agents with configuration\n        self.data_parser = DataParserAgent(config=self.config)\n        self.technical_analyzer = TechnicalAnalyzerAgent(config=self.config)\n        self.domain_ontology = DomainOntologyAgent(config=self.config)\n        self.plain_language = PlainLanguageAgent(config=self.config)\n        self.assembler = DocumentationAssemblerAgent(self.review_queue, config=self.config)\n        \n        logger.info(f\"Orchestrator initialized with {self.config.requests_per_minute} req/min limit\")\n        print(f\"âœ“ Orchestrator initialized with {self.config.requests_per_minute} req/min limit\")\n    \n    def create_job(self, source_file: str) -> str:\n        \"\"\"Create a new documentation job.\"\"\"\n        job_id = hashlib.md5(f\"{source_file}_{datetime.now().isoformat()}\".encode()).hexdigest()[:12]\n        query = \"INSERT INTO Jobs (job_id, source_file, status) VALUES (?, ?, 'Running')\"\n        self.db.execute_update(query, (job_id, source_file))\n        logger.info(f\"Created job {job_id} for {source_file}\")\n        return job_id\n    \n    def process_data_dictionary(self, source_data: str, source_file: str = \"input.csv\",\n                                auto_approve: bool = False) -> str:\n        \"\"\"\n        Main workflow: Process a data dictionary through the agent pipeline.\n        \n        Args:\n            source_data: The raw data dictionary content\n            source_file: Name of the source file\n            auto_approve: If True, automatically approve all generated content\n            \n        Returns:\n            job_id: The ID of the created job\n        \"\"\"\n        job_id = self.create_job(source_file)\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"Processing Job: {job_id}\")\n        print(f\"{'='*60}\")\n        \n        # Step 1: Parse data\n        print(\"\\nðŸ“Š Step 1: Parsing Data...\")\n        parsed_data = self.data_parser.parse_csv(source_data)\n        print(f\"   âœ“ Parsed {len(parsed_data)} variables\")\n        \n        # Step 2: Technical analysis\n        print(\"\\nðŸ”¬ Step 2: Technical Analysis...\")\n        analyzed_data = self.technical_analyzer.analyze(parsed_data)\n        print(f\"   âœ“ Analyzed {len(analyzed_data)} variables\")\n        \n        # Check for clarifications needed\n        needs_clarification = [v for v in analyzed_data if v.get('needs_clarification', False)]\n        if needs_clarification:\n            print(f\"   âš ï¸  {len(needs_clarification)} variables need clarification\")\n            for var in needs_clarification:\n                print(f\"      - {var['original_name']}: {var.get('clarification_question', 'Unknown')}\")\n        \n        # Step 3: Ontology mapping and documentation\n        print(\"\\nðŸ¥ Step 3: Ontology Mapping & Documentation...\")\n        for i, var_data in enumerate(analyzed_data, 1):\n            print(f\"   Processing {i}/{len(analyzed_data)}: {var_data.get('variable_name', var_data.get('original_name'))}\")\n            \n            # Map to ontologies\n            ontology_result = self.domain_ontology.map_ontologies(var_data)\n            \n            # Enrich with ontology data\n            enriched_data = {**var_data, **ontology_result}\n            \n            # Generate plain language documentation\n            documentation = self.plain_language.document_variable(enriched_data)\n            \n            # Add to review queue\n            item_id = self.review_queue.add_item(\n                job_id=job_id,\n                source_agent=\"PlainLanguageAgent\",\n                source_data=json.dumps(enriched_data),\n                generated_content=documentation\n            )\n            \n            if auto_approve:\n                self.review_queue.approve_item(item_id)\n        \n        # Update job status\n        status = 'Completed' if auto_approve else 'Pending Review'\n        self.db.execute_update(\n            \"UPDATE Jobs SET status = ?, updated_at = CURRENT_TIMESTAMP WHERE job_id = ?\",\n            (status, job_id)\n        )\n        \n        print(f\"\\nâœ“ Processing complete! Job status: {status}\")\n        return job_id\n    \n    def finalize_documentation(self, job_id: str, output_file: str = \"documentation.md\") -> str:\n        \"\"\"Assemble and save final documentation.\"\"\"\n        print(f\"\\nðŸ“ Assembling final documentation for job {job_id}...\")\n        final_doc = self.assembler.assemble(job_id)\n        \n        with open(output_file, 'w') as f:\n            f.write(final_doc)\n        \n        print(f\"âœ“ Documentation saved to {output_file}\")\n        logger.info(f\"Final documentation saved: {output_file}\")\n        return final_doc\n\nprint(\"âœ“ Orchestrator class defined with complete pipeline support\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 7.1 Batch Processing for Large Codebooks\n\nProcess large data dictionaries in batches to avoid context limits and manage API rate limiting effectively.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "@dataclass\nclass BatchConfig:\n    \"\"\"Configuration for batch processing of large codebooks.\"\"\"\n    batch_size: int = 10  # Default number of variables per batch\n    min_batch_size: int = 3  # Minimum batch size to avoid splitting too small\n    group_related_variables: bool = True  # Try to keep related variables together\n    progress_tracking: bool = True  # Show progress during processing\n\n@dataclass\nclass BatchResult:\n    \"\"\"Result of processing a single batch.\"\"\"\n    batch_id: int\n    variables_processed: int\n    success: bool\n    error_message: Optional[str] = None\n    \nclass BatchProcessor:\n    \"\"\"\n    Handles batch processing of large data dictionaries.\n    \n    Features:\n    - Automatic chunking with configurable batch size\n    - Sensitivity to not splitting related variables between chunks\n    - Progress tracking with resume capability\n    \"\"\"\n    \n    def __init__(self, orchestrator: Orchestrator, config: BatchConfig = None):\n        self.orchestrator = orchestrator\n        self.config = config or BatchConfig()\n        self.logger = logging.getLogger('ADE.BatchProcessor')\n    \n    def _identify_variable_groups(self, parsed_data: List[Dict]) -> List[List[int]]:\n        \"\"\"\n        Identify groups of related variables that should stay together.\n        \n        Groups variables by common prefixes (e.g., bp_systolic, bp_diastolic)\n        or related semantic meaning.\n        \"\"\"\n        if not self.config.group_related_variables:\n            return [[i] for i in range(len(parsed_data))]\n        \n        groups = []\n        used_indices = set()\n        \n        # Group by common prefixes\n        for i, var in enumerate(parsed_data):\n            if i in used_indices:\n                continue\n            \n            var_name = var.get('original_name', var.get('Variable Name', '')).lower()\n            if not var_name:\n                groups.append([i])\n                used_indices.add(i)\n                continue\n            \n            # Extract prefix (e.g., \"bp\" from \"bp_systolic\")\n            parts = var_name.replace('-', '_').split('_')\n            if len(parts) > 1:\n                prefix = parts[0]\n                group = [i]\n                used_indices.add(i)\n                \n                # Find other variables with same prefix\n                for j, other_var in enumerate(parsed_data):\n                    if j in used_indices:\n                        continue\n                    other_name = other_var.get('original_name', other_var.get('Variable Name', '')).lower()\n                    if other_name.startswith(prefix + '_') or other_name.startswith(prefix + '-'):\n                        group.append(j)\n                        used_indices.add(j)\n                \n                groups.append(group)\n            else:\n                groups.append([i])\n                used_indices.add(i)\n        \n        return groups\n    \n    def _create_batches(self, parsed_data: List[Dict]) -> List[List[Dict]]:\n        \"\"\"\n        Create batches of variables, respecting group boundaries.\n        \n        Returns a list of batches, where each batch is a list of variable dicts.\n        \"\"\"\n        groups = self._identify_variable_groups(parsed_data)\n        batches = []\n        current_batch = []\n        current_batch_size = 0\n        \n        for group_indices in groups:\n            group_size = len(group_indices)\n            group_vars = [parsed_data[i] for i in group_indices]\n            \n            # If adding this group would exceed batch size\n            if current_batch_size + group_size > self.config.batch_size:\n                # If current batch has something, save it\n                if current_batch and current_batch_size >= self.config.min_batch_size:\n                    batches.append(current_batch)\n                    current_batch = group_vars\n                    current_batch_size = group_size\n                elif current_batch:\n                    # Current batch too small, add group anyway\n                    current_batch.extend(group_vars)\n                    current_batch_size += group_size\n                else:\n                    # No current batch, start with this group\n                    current_batch = group_vars\n                    current_batch_size = group_size\n            else:\n                current_batch.extend(group_vars)\n                current_batch_size += group_size\n        \n        # Add remaining batch\n        if current_batch:\n            batches.append(current_batch)\n        \n        return batches\n    \n    def process_large_codebook(self, source_data: str, source_file: str = \"input.csv\",\n                               auto_approve: bool = False) -> Tuple[str, List[BatchResult]]:\n        \"\"\"\n        Process a large data dictionary in batches.\n        \n        Args:\n            source_data: The raw data dictionary content\n            source_file: Name of the source file\n            auto_approve: If True, automatically approve all generated content\n            \n        Returns:\n            Tuple of (job_id, list of batch results)\n        \"\"\"\n        # Create job\n        job_id = self.orchestrator.create_job(source_file)\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"BATCH PROCESSING: Job {job_id}\")\n        print(f\"{'='*60}\")\n        \n        # Step 1: Parse all data first\n        print(\"\\nðŸ“Š Step 1: Parsing entire data dictionary...\")\n        parsed_data = self.orchestrator.data_parser.parse_csv(source_data)\n        total_variables = len(parsed_data)\n        print(f\"   âœ“ Parsed {total_variables} variables total\")\n        \n        # Step 2: Create batches\n        print(f\"\\nðŸ“¦ Step 2: Creating batches (target size: {self.config.batch_size})...\")\n        batches = self._create_batches(parsed_data)\n        num_batches = len(batches)\n        print(f\"   âœ“ Created {num_batches} batches\")\n        for i, batch in enumerate(batches, 1):\n            var_names = [v.get('original_name', v.get('Variable Name', 'Unknown'))[:20] for v in batch]\n            print(f\"      Batch {i}: {len(batch)} variables - {', '.join(var_names[:3])}{'...' if len(var_names) > 3 else ''}\")\n        \n        # Step 3: Process each batch\n        results = []\n        all_analyzed_data = []\n        \n        print(f\"\\nðŸ”¬ Step 3: Processing batches...\")\n        for batch_id, batch_vars in enumerate(batches, 1):\n            if self.config.progress_tracking:\n                print(f\"\\n   --- Batch {batch_id}/{num_batches} ({len(batch_vars)} variables) ---\")\n            \n            try:\n                # Technical analysis for this batch\n                print(f\"   Analyzing batch {batch_id}...\")\n                analyzed_batch = self.orchestrator.technical_analyzer.analyze(batch_vars)\n                all_analyzed_data.extend(analyzed_batch)\n                \n                # Process ontology and documentation for each variable in batch\n                for i, var_data in enumerate(analyzed_batch, 1):\n                    var_name = var_data.get('variable_name', var_data.get('original_name', 'Unknown'))\n                    if self.config.progress_tracking:\n                        print(f\"      {i}/{len(analyzed_batch)}: {var_name}\")\n                    \n                    # Map to ontologies\n                    ontology_result = self.orchestrator.domain_ontology.map_ontologies(var_data)\n                    enriched_data = {**var_data, **ontology_result}\n                    \n                    # Generate documentation\n                    documentation = self.orchestrator.plain_language.document_variable(enriched_data)\n                    \n                    # Add to review queue\n                    item_id = self.orchestrator.review_queue.add_item(\n                        job_id=job_id,\n                        source_agent=\"PlainLanguageAgent\",\n                        source_data=json.dumps(enriched_data),\n                        generated_content=documentation\n                    )\n                    \n                    if auto_approve:\n                        self.orchestrator.review_queue.approve_item(item_id)\n                \n                results.append(BatchResult(\n                    batch_id=batch_id,\n                    variables_processed=len(batch_vars),\n                    success=True\n                ))\n                print(f\"   âœ“ Batch {batch_id} complete\")\n                \n            except Exception as e:\n                error_msg = str(e)\n                self.logger.error(f\"Batch {batch_id} failed: {error_msg}\")\n                results.append(BatchResult(\n                    batch_id=batch_id,\n                    variables_processed=0,\n                    success=False,\n                    error_message=error_msg\n                ))\n                print(f\"   âœ— Batch {batch_id} failed: {error_msg}\")\n        \n        # Update job status\n        successful_batches = sum(1 for r in results if r.success)\n        if successful_batches == num_batches:\n            status = 'Completed' if auto_approve else 'Pending Review'\n        elif successful_batches > 0:\n            status = 'Paused'  # Partial success\n        else:\n            status = 'Failed'\n        \n        self.orchestrator.db.execute_update(\n            \"UPDATE Jobs SET status = ?, updated_at = CURRENT_TIMESTAMP WHERE job_id = ?\",\n            (status, job_id)\n        )\n        \n        # Summary\n        print(f\"\\n{'='*60}\")\n        print(f\"BATCH PROCESSING SUMMARY\")\n        print(f\"{'='*60}\")\n        print(f\"   Job ID: {job_id}\")\n        print(f\"   Total variables: {total_variables}\")\n        print(f\"   Batches processed: {successful_batches}/{num_batches}\")\n        print(f\"   Variables documented: {sum(r.variables_processed for r in results if r.success)}\")\n        print(f\"   Status: {status}\")\n        \n        if not auto_approve:\n            print(f\"\\n   âš ï¸  Items awaiting manual review in queue\")\n        \n        return job_id, results\n\n# Example configuration for different scenarios\nSMALL_CODEBOOK_CONFIG = BatchConfig(batch_size=5, min_batch_size=2)\nMEDIUM_CODEBOOK_CONFIG = BatchConfig(batch_size=10, min_batch_size=3)\nLARGE_CODEBOOK_CONFIG = BatchConfig(batch_size=20, min_batch_size=5)\n\nprint(\"âœ“ BatchProcessor loaded for large codebook handling\")\nprint(f\"   - Default batch size: {BatchConfig().batch_size}\")\nprint(f\"   - Groups related variables: {BatchConfig().group_related_variables}\")\nprint(f\"   - Available configs: SMALL_CODEBOOK_CONFIG, MEDIUM_CODEBOOK_CONFIG, LARGE_CODEBOOK_CONFIG\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Example Data Dictionaries",
    "",
    "Sample healthcare data dictionaries for testing the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic diabetes study example",
    "sample_data_dictionary = \"\"\"Variable Name,Field Type,Field Label,Choices,Notes",
    "patient_id,text,Patient ID,,Unique identifier",
    "age,integer,Age (years),,Age at enrollment",
    "sex,radio,Biological Sex,\"1, Male | 2, Female | 3, Other\",",
    "bp_systolic,integer,Systolic Blood Pressure (mmHg),,",
    "bp_diastolic,integer,Diastolic Blood Pressure (mmHg),,",
    "diagnosis_date,date,Diagnosis Date,,Date of primary diagnosis",
    "hba1c,decimal,Hemoglobin A1c (%),,Glycated hemoglobin",
    "\"\"\"",
    "",
    "# EHR example",
    "ehr_data_dictionary = \"\"\"Variable Name,Field Type,Field Label,Choices,Notes",
    "mrn,text,Medical Record Number,,Unique patient identifier",
    "encounter_id,text,Encounter ID,,Unique visit identifier",
    "visit_date,date,Visit Date,,Date of clinical encounter",
    "chief_complaint,text,Chief Complaint,,Primary reason for visit",
    "dx_code,text,Diagnosis Code (ICD-10),,Primary diagnosis",
    "bp_systolic,integer,Systolic BP (mmHg),,\"70-250, sitting position\"",
    "bp_diastolic,integer,Diastolic BP (mmHg),,\"40-150, sitting position\"",
    "heart_rate,integer,Heart Rate (bpm),,\"40-200\"",
    "temperature,decimal,Temperature (F),,\"95.0-106.0\"",
    "respiratory_rate,integer,Respiratory Rate (breaths/min),,\"8-40\"",
    "oxygen_sat,integer,Oxygen Saturation (%),,\"70-100, room air\"",
    "bmi,decimal,Body Mass Index,,Calculated from height/weight",
    "smoking_status,radio,Smoking Status,\"0, Never | 1, Former | 2, Current\",From social history",
    "medication_count,integer,Number of Active Medications,,Count of current prescriptions",
    "lab_ordered,yesno,Labs Ordered,\"0, No | 1, Yes\",Any lab tests ordered this visit",
    "\"\"\"",
    "",
    "print(\"âœ“ Sample data dictionaries loaded\")",
    "print(f\"   - Basic diabetes study: 7 variables\")",
    "print(f\"   - EHR example: 15 variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Usage Demonstration",
    "",
    "Initialize the orchestrator and process a data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize orchestrator\norchestrator = Orchestrator(db)\n\n# Create context snippets for better agent performance\nprint(\"\\nCreating context snippets...\")\n\ndef create_or_update_snippet(name: str, snippet_type: SnippetType, content: str, metadata: Optional[Dict] = None):\n    existing_snippet = orchestrator.snippet_manager.get_snippet_by_name(name)\n    if existing_snippet:\n        orchestrator.snippet_manager.update_snippet(existing_snippet.snippet_id, content=content, metadata=metadata)\n        print(f\"   Updated snippet '{name}'\")\n    else:\n        orchestrator.snippet_manager.create_snippet(name, snippet_type, content, metadata)\n        print(f\"   Created snippet '{name}'\")\n\n# OMOP mapping instructions\ncreate_or_update_snippet(\n    name=\"OMOP_Mapping_Instructions\",\n    snippet_type=SnippetType.INSTRUCTION,\n    content=\"\"\"When mapping to OMOP CDM:\n- Blood pressure: OMOP concept_id 3004249 (Systolic), 3012888 (Diastolic)\n- HbA1c: OMOP concept_id 3004410\n- Age: Integer in years\n- Sex: OMOP gender concepts 8507 (Male), 8532 (Female)\"\"\")\n\n# Project design notes\ncreate_or_update_snippet(\n    name=\"Project_Design_Notes\",\n    snippet_type=SnippetType.DESIGN,\n    content=\"\"\"Diabetes research study collecting baseline clinical measurements.\nAll measurements follow standard clinical protocols. Blood pressure measured in sitting position after 5 minutes rest. HbA1c measured using DCCT-aligned assay.\"\"\")\n\n# Inject snippets into agents\nsnippets = orchestrator.snippet_manager.list_snippets()\norchestrator.domain_ontology.inject_snippets(snippets)\norchestrator.plain_language.inject_snippets(snippets)\nprint(f\"\\nâœ“ Injected {len(snippets)} snippets into agent context\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the data dictionary",
    "# Set AUTO_APPROVE_MODE = True for testing, False for manual review",
    "AUTO_APPROVE_MODE = True",
    "",
    "job_id = orchestrator.process_data_dictionary(",
    "    source_data=sample_data_dictionary,",
    "    source_file=\"diabetes_study_data_dictionary.csv\",",
    "    auto_approve=AUTO_APPROVE_MODE",
    ")",
    "",
    "print(f\"\\n{'='*60}\")",
    "print(f\"Job ID: {job_id}\")",
    "print(f\"Auto-approve mode: {'ENABLED' if AUTO_APPROVE_MODE else 'DISABLED'}\")",
    "print(f\"{'='*60}\")",
    "",
    "if AUTO_APPROVE_MODE:",
    "    print(\"\\nâœ“ All items automatically approved\")",
    "    print(\"   Run next cell to generate final documentation\")",
    "else:",
    "    print(\"\\nâš ï¸  Items awaiting manual review\")",
    "    print(\"   Use review queue to approve/reject items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final documentation",
    "final_documentation = orchestrator.finalize_documentation(",
    "    job_id=job_id,",
    "    output_file=\"healthcare_data_documentation.md\"",
    ")",
    "",
    "print(\"\\n=== Final Documentation Preview (first 2000 chars) ===\")",
    "print(final_documentation[:2000])",
    "if len(final_documentation) > 2000:",
    "    print(\"\\n... [truncated]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Session and Memory Management",
    "",
    "ADK-style session management with context compaction for long conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SessionManager:",
    "    \"\"\"ADK-style session management with state persistence.\"\"\"",
    "    ",
    "    def __init__(self, db_manager: DatabaseManager):",
    "        self.db = db_manager",
    "    ",
    "    def create_session(self, job_id: str, user_id: str) -> str:",
    "        \"\"\"Create a new session.\"\"\"",
    "        session_id = hashlib.md5(f\"{job_id}_{user_id}_{datetime.now().isoformat()}\".encode()).hexdigest()[:16]",
    "        query = \"INSERT INTO Sessions (session_id, job_id, user_id) VALUES (?, ?, ?)\"",
    "        self.db.execute_update(query, (session_id, job_id, user_id))",
    "        return session_id",
    "    ",
    "    def get_session_state(self, session_id: str) -> Dict:",
    "        \"\"\"Get session state.\"\"\"",
    "        query = \"SELECT state FROM Sessions WHERE session_id = ?\"",
    "        result = self.db.execute_query(query, (session_id,))",
    "        if result:",
    "            return json.loads(result[0]['state'])",
    "        return {}",
    "    ",
    "    def update_session_state(self, session_id: str, key: str, value: Any):",
    "        \"\"\"Update session state (similar to ADK tool_context.state).\"\"\"",
    "        state = self.get_session_state(session_id)",
    "        state[key] = value",
    "        query = \"UPDATE Sessions SET state = ?, updated_at = CURRENT_TIMESTAMP WHERE session_id = ?\"",
    "        self.db.execute_update(query, (json.dumps(state), session_id))",
    "    ",
    "    def add_to_history(self, session_id: str, job_id: str, role: str, content: str, metadata: Dict = None):",
    "        \"\"\"Add message to session history.\"\"\"",
    "        query = \"\"\"",
    "        INSERT INTO SessionHistory (session_id, job_id, role, content, metadata)",
    "        VALUES (?, ?, ?, ?, ?)",
    "        \"\"\"",
    "        self.db.execute_update(query, (session_id, job_id, role, content, json.dumps(metadata) if metadata else None))",
    "",
    "",
    "class ContextManager:",
    "    \"\"\"Manages working memory with compaction for long sessions.\"\"\"",
    "    ",
    "    def __init__(self, db_manager: DatabaseManager, max_tokens: int = 100000):",
    "        self.db = db_manager",
    "        self.max_tokens = max_tokens",
    "        self.compaction_threshold = int(max_tokens * 0.8)",
    "    ",
    "    def estimate_tokens(self, text: str) -> int:",
    "        \"\"\"Rough token estimation (1 token â‰ˆ 4 characters).\"\"\"",
    "        return len(text) // 4",
    "    ",
    "    def get_working_memory(self, job_id: str) -> Dict[str, Any]:",
    "        \"\"\"Get current working memory for a job.\"\"\"",
    "        query = \"SELECT * FROM SessionHistory WHERE job_id = ? ORDER BY created_at\"",
    "        history_rows = self.db.execute_query(query, (job_id,))",
    "        ",
    "        session_history = [",
    "            {",
    "                'role': row['role'],",
    "                'content': row['content'],",
    "                'timestamp': row['created_at']",
    "            }",
    "            for row in history_rows",
    "        ]",
    "        ",
    "        total_tokens = sum(self.estimate_tokens(msg['content']) for msg in session_history)",
    "        ",
    "        return {",
    "            'session_history': session_history,",
    "            'total_tokens': total_tokens,",
    "            'needs_compaction': total_tokens > self.compaction_threshold",
    "        }",
    "    ",
    "    def compact_context(self, job_id: str) -> str:",
    "        \"\"\"Compact session history using summarization (ADK context compaction pattern).\"\"\"",
    "        working_memory = self.get_working_memory(job_id)",
    "        ",
    "        if not working_memory['needs_compaction']:",
    "            return \"No compaction needed\"",
    "        ",
    "        print(\"\\nâš¡ Context compaction triggered...\")",
    "        # In production, use LLM to summarize conversation",
    "        # For now, keep last N messages",
    "        logger.info(f\"Context compaction for job {job_id}\")",
    "        return \"Context compacted\"",
    "",
    "",
    "print(\"âœ“ Session and Context management classes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. System Status and Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def display_system_status(db: DatabaseManager):\n    \"\"\"Display current system status with observability metrics.\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"ADE SYSTEM STATUS\")\n    print(\"=\"*80)\n    \n    # Jobs\n    jobs = db.execute_query(\"SELECT * FROM Jobs ORDER BY created_at DESC LIMIT 5\")\n    print(f\"\\nRecent Jobs: {len(jobs)}\")\n    for job in jobs:\n        print(f\"  [{job['job_id']}] {job['source_file']} - {job['status']}\")\n    \n    # Snippets\n    snippets = db.execute_query(\"SELECT snippet_type, COUNT(*) as count FROM Snippets GROUP BY snippet_type\")\n    print(f\"\\nSnippet Library:\")\n    for snippet in snippets:\n        print(f\"  {snippet['snippet_type']}: {snippet['count']}\")\n    \n    # Review Queue\n    review_stats = db.execute_query(\"SELECT status, COUNT(*) as count FROM ReviewQueue GROUP BY status\")\n    print(f\"\\nReview Queue:\")\n    for stat in review_stats:\n        print(f\"  {stat['status']}: {stat['count']}\")\n    \n    # Sessions\n    sessions = db.execute_query(\"SELECT COUNT(*) as count FROM Sessions\")\n    print(f\"\\nSessions: {sessions[0]['count']}\")\n    \n    print(\"\\n\" + \"=\"*80)\n\ndisplay_system_status(db)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Export and Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil",
    "",
    "def backup_database(db_path: str, backup_path: str):",
    "    \"\"\"Create a backup of the project database.\"\"\"",
    "    shutil.copy2(db_path, backup_path)",
    "    print(f\"âœ“ Database backed up to {backup_path}\")",
    "",
    "def export_documentation():",
    "    \"\"\"Export generated documentation.\"\"\"",
    "    if os.path.exists(\"healthcare_data_documentation.md\"):",
    "        with open(\"healthcare_data_documentation.md\", 'r') as f:",
    "            content = f.read()",
    "        print(f\"Documentation length: {len(content)} characters\")",
    "        return content",
    "    else:",
    "        print(\"No documentation file found\")",
    "        return None",
    "",
    "# Create backup",
    "backup_database(\"project.db\", \"project_backup.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Deploying to Vertex AI Agent Engine",
    "",
    "This section provides instructions for deploying your healthcare documentation agent to Google Cloud's Vertex AI Agent Engine for production use.",
    "",
    "### Overview",
    "",
    "Vertex AI Agent Engine provides:",
    "- **Fully managed infrastructure** with auto-scaling",
    "- **Built-in security** with IAM integration",
    "- **Production monitoring** through Cloud Console",
    "- **Session and memory services** at scale",
    "- **High availability** across regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites",
    "",
    "Before deploying, ensure you have:",
    "",
    "1. **Google Cloud Project** with billing enabled",
    "2. **Vertex AI API** enabled",
    "3. **IAM permissions** for Vertex AI Agent Engine",
    "4. **Google Cloud CLI** installed and configured",
    "",
    "```bash",
    "# Enable required APIs",
    "gcloud services enable aiplatform.googleapis.com",
    "gcloud services enable cloudbuild.googleapis.com",
    "",
    "# Set project",
    "gcloud config set project YOUR_PROJECT_ID",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create deployment directory structure",
    "import os",
    "",
    "DEPLOY_DIR = \"healthcare_agent_deploy\"",
    "",
    "# Create directory structure",
    "os.makedirs(f\"{DEPLOY_DIR}\", exist_ok=True)",
    "",
    "print(f'''",
    "ðŸ“ Deployment Structure for Vertex AI Agent Engine:",
    "",
    "{DEPLOY_DIR}/",
    "â”œâ”€â”€ agent.py                     # Main agent logic",
    "â”œâ”€â”€ requirements.txt             # Python dependencies",
    "â”œâ”€â”€ .env                         # Environment configuration",
    "â””â”€â”€ .agent_engine_config.json    # Deployment specifications",
    "",
    "This structure follows ADK deployment conventions.",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the main agent.py file for deployment",
    "agent_code = \"\"\"import os",
    "import json",
    "import vertexai",
    "from google.adk.agents import Agent, LlmAgent",
    "from google.adk.tools.tool_context import ToolContext",
    "from typing import Dict, List, Any",
    "",
    "# Initialize Vertex AI",
    "vertexai.init(",
    "    project=os.environ.get(\"GOOGLE_CLOUD_PROJECT\"),",
    "    location=os.environ.get(\"GOOGLE_CLOUD_LOCATION\", \"us-central1\"),",
    ")",
    "",
    "",
    "def parse_data_dictionary(data: str) -> Dict[str, Any]:",
    "    \"\"\"Parse a raw data dictionary into structured format.\"\"\"",
    "    lines = data.strip().split(\"\\n\")",
    "    if not lines:",
    "        return {\"status\": \"error\", \"message\": \"Empty data\"}",
    "",
    "    header = lines[0].split(\",\")",
    "    variables = []",
    "",
    "    for line in lines[1:]:",
    "        if line.strip():",
    "            values = line.split(\",\")",
    "            var_dict = dict(zip(header, values))",
    "            variables.append(var_dict)",
    "",
    "    return {",
    "        \"status\": \"success\",",
    "        \"variable_count\": len(variables),",
    "        \"variables\": variables",
    "    }",
    "",
    "",
    "def map_to_ontology(variable_name: str, data_type: str) -> Dict[str, Any]:",
    "    \"\"\"Map a variable to standard healthcare ontologies.\"\"\"",
    "    ontology_map = {",
    "        \"patient_id\": {\"omop\": \"person_id\", \"concept_id\": 0},",
    "        \"age\": {\"omop\": \"year_of_birth\", \"concept_id\": 4154793},",
    "        \"sex\": {\"omop\": \"gender_concept_id\", \"concept_id\": 4135376},",
    "        \"bp_systolic\": {\"omop\": \"measurement\", \"concept_id\": 3004249},",
    "        \"bp_diastolic\": {\"omop\": \"measurement\", \"concept_id\": 3012888},",
    "        \"hba1c\": {\"omop\": \"measurement\", \"concept_id\": 3004410, \"loinc\": \"4548-4\"},",
    "    }",
    "",
    "    mapping = ontology_map.get(variable_name.lower(), {\"omop\": \"unknown\", \"concept_id\": 0})",
    "    return {\"status\": \"success\", \"variable_name\": variable_name, \"mappings\": mapping}",
    "",
    "",
    "def generate_documentation(variable_info: Dict[str, Any]) -> Dict[str, str]:",
    "    \"\"\"Generate human-readable documentation for a variable.\"\"\"",
    "    name = variable_info.get(\"Variable Name\", \"Unknown\")",
    "    field_type = variable_info.get(\"Field Type\", \"text\")",
    "    label = variable_info.get(\"Field Label\", name)",
    "    notes = variable_info.get(\"Notes\", \"No additional notes\")",
    "",
    "    doc = f\"\"\"## Variable: {name}",
    "",
    "**Description:** {label}",
    "",
    "**Technical Details:**",
    "- Data Type: {field_type}",
    "- Cardinality: required",
    "- Notes: {notes}",
    "\"\"\"",
    "    return {\"status\": \"success\", \"documentation\": doc}",
    "",
    "",
    "def save_to_memory(tool_context: ToolContext, key: str, value: str) -> Dict[str, str]:",
    "    \"\"\"Save information to session state.\"\"\"",
    "    tool_context.state[f\"memory:{key}\"] = value",
    "    return {\"status\": \"success\", \"message\": f\"Saved {key} to memory\"}",
    "",
    "",
    "def retrieve_from_memory(tool_context: ToolContext, key: str) -> Dict[str, Any]:",
    "    \"\"\"Retrieve information from session state.\"\"\"",
    "    value = tool_context.state.get(f\"memory:{key}\", \"Not found\")",
    "    return {\"status\": \"success\", \"key\": key, \"value\": value}",
    "",
    "",
    "# Create the root agent",
    "root_agent = LlmAgent(",
    "    name=\"healthcare_documentation_agent\",",
    "    model=\"gemini-2.0-flash-exp\",",
    "    description=\"Agent for generating healthcare data documentation\",",
    "    instruction=\"\"\"You are a Healthcare Data Documentation Agent specialized in:",
    "1. Parsing data dictionaries from various formats",
    "2. Mapping variables to standard healthcare ontologies (OMOP, LOINC, SNOMED)",
    "3. Generating clear, comprehensive documentation",
    "",
    "When a user provides a data dictionary:",
    "1. Use parse_data_dictionary to extract variable information",
    "2. Use map_to_ontology for each variable to find standard codes",
    "3. Use generate_documentation to create human-readable documentation",
    "4. Use save_to_memory to store results for later reference",
    "\"\"\",",
    "    tools=[",
    "        parse_data_dictionary,",
    "        map_to_ontology,",
    "        generate_documentation,",
    "        save_to_memory,",
    "        retrieve_from_memory,",
    "    ],",
    ")",
    "\"\"\"",
    "",
    "with open(f\"{DEPLOY_DIR}/agent.py\", 'w') as f:",
    "    f.write(agent_code)",
    "",
    "print(f\"âœ“ Created {DEPLOY_DIR}/agent.py\")",
    "print(\"  - Includes healthcare-specific tools\")",
    "print(\"  - Uses ADK LlmAgent pattern\")",
    "print(\"  - Integrated session state management\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create requirements.txt for deployment",
    "requirements = \"\"\"google-adk>=1.0.0",
    "google-cloud-aiplatform>=1.38.0",
    "opentelemetry-instrumentation-google-genai",
    "vertexai",
    "\"\"\"",
    "",
    "with open(f\"{DEPLOY_DIR}/requirements.txt\", 'w') as f:",
    "    f.write(requirements)",
    "",
    "print(f\"âœ“ Created {DEPLOY_DIR}/requirements.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create .env configuration",
    "env_config = \"\"\"# Vertex AI Configuration",
    "GOOGLE_CLOUD_PROJECT=your-project-id",
    "GOOGLE_CLOUD_LOCATION=us-central1",
    "GOOGLE_GENAI_USE_VERTEXAI=1",
    "\"\"\"",
    "",
    "with open(f\"{DEPLOY_DIR}/.env\", 'w') as f:",
    "    f.write(env_config)",
    "",
    "print(f\"âœ“ Created {DEPLOY_DIR}/.env\")",
    "print(\"  âš ï¸  Remember to update GOOGLE_CLOUD_PROJECT with your project ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create .agent_engine_config.json",
    "deployment_config = {",
    "    \"min_instances\": 0,",
    "    \"max_instances\": 3,",
    "    \"resource_limits\": {",
    "        \"cpu\": \"2\",",
    "        \"memory\": \"4Gi\"",
    "    },",
    "    \"timeout_seconds\": 300,",
    "    \"environment_variables\": {",
    "        \"LOG_LEVEL\": \"INFO\"",
    "    }",
    "}",
    "",
    "with open(f\"{DEPLOY_DIR}/.agent_engine_config.json\", 'w') as f:",
    "    json.dump(deployment_config, f, indent=2)",
    "",
    "print(f\"âœ“ Created {DEPLOY_DIR}/.agent_engine_config.json\")",
    "print(f\"  - Min instances: {deployment_config['min_instances']}\")",
    "print(f\"  - Max instances: {deployment_config['max_instances']}\")",
    "print(f\"  - Resources: {deployment_config['resource_limits']['cpu']} CPU, {deployment_config['resource_limits']['memory']} Memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Using ADK CLI",
    "",
    "Once your deployment files are created, use the ADK CLI to deploy:",
    "",
    "```bash",
    "# Set your project and region",
    "export PROJECT_ID=\"your-project-id\"",
    "export REGION=\"us-central1\"",
    "",
    "# Deploy the agent",
    "adk deploy agent_engine \\",
    "    --project=$PROJECT_ID \\",
    "    --region=$REGION \\",
    "    healthcare_agent_deploy \\",
    "    --agent_engine_config_file=healthcare_agent_deploy/.agent_engine_config.json",
    "```",
    "",
    "The deployment process will:",
    "1. Build a container with your agent code",
    "2. Push to Google Container Registry",
    "3. Deploy to Vertex AI Agent Engine",
    "4. Return the deployment resource name",
    "",
    "**Expected output:**",
    "```",
    "Deploying agent to Vertex AI Agent Engine...",
    "Building container image...",
    "Pushing to Container Registry...",
    "Creating Agent Engine instance...",
    "âœ“ Agent deployed successfully!",
    "Resource name: projects/YOUR_PROJECT/locations/REGION/agents/AGENT_ID",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Your Deployed Agent",
    "",
    "After deployment, test your agent using the Vertex AI SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test code for deployed agent (run AFTER deployment)",
    "# âš ï¸ Update PROJECT_ID before running",
    "",
    "import vertexai",
    "from vertexai import agent_engines",
    "",
    "PROJECT_ID = \"your-project-id\"  # UPDATE THIS",
    "REGION = \"us-central1\"",
    "",
    "vertexai.init(project=PROJECT_ID, location=REGION)",
    "",
    "# List deployed agents",
    "print(\"Deployed Agents:\")",
    "agents_list = list(agent_engines.list())",
    "for agent in agents_list:",
    "    print(f\"  - {agent.display_name}: {agent.resource_name}\")",
    "",
    "if agents_list:",
    "    remote_agent = agents_list[0]",
    "    ",
    "    # Test data dictionary",
    "    test_data = \"\"\"Variable Name,Field Type,Field Label",
    "patient_id,text,Patient ID",
    "age,integer,Age (years)",
    "hba1c,decimal,HbA1c (%)\"\"\"",
    "    ",
    "    print(f\"\\nTesting agent: {remote_agent.display_name}\")",
    "    print(\"Sending test query...\")",
    "    ",
    "    # Synchronous query (for simple testing)",
    "    response = remote_agent.query(",
    "        message=f\"Parse this data dictionary:\\n{test_data}\",",
    "        user_id=\"test_user_001\",",
    "    )",
    "    print(f\"\\nResponse: {response}\")",
    "else:",
    "    print(\"No deployed agents found. Deploy first using adk deploy command.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring and Management",
    "",
    "#### Google Cloud Console",
    "",
    "Monitor your deployed agent through:",
    "1. **Vertex AI â†’ Agent Engine** in Cloud Console",
    "2. **Cloud Logging** for detailed logs",
    "3. **Cloud Monitoring** for metrics and alerts",
    "",
    "```bash",
    "# View agent logs",
    "gcloud logging read \"resource.type=aiplatform.googleapis.com/Agent\" --limit=50",
    "",
    "# Check agent status",
    "gcloud ai agents describe AGENT_ID --region=REGION",
    "```",
    "",
    "#### Updating the Agent",
    "",
    "To update your deployed agent:",
    "",
    "```bash",
    "# Redeploy with updated code",
    "adk deploy agent_engine \\",
    "    --project=$PROJECT_ID \\",
    "    --region=$REGION \\",
    "    healthcare_agent_deploy \\",
    "    --agent_engine_config_file=healthcare_agent_deploy/.agent_engine_config.json \\",
    "    --update",
    "```",
    "",
    "#### Cleanup",
    "",
    "Delete the agent when no longer needed to avoid charges:",
    "",
    "```python",
    "from vertexai import agent_engines",
    "",
    "# Delete specific agent",
    "agent_engines.delete(",
    "    resource_name=\"projects/PROJECT/locations/REGION/agents/AGENT_ID\", ",
    "    force=True",
    ")",
    "",
    "print(\"âœ“ Agent deleted successfully\")",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Production Considerations",
    "",
    "When deploying to production:",
    "",
    "1. **Authentication & Security**",
    "   - Use service accounts with minimal required permissions",
    "   - Enable VPC Service Controls for data protection",
    "   - Configure Cloud Armor for DDoS protection",
    "",
    "2. **Scaling**",
    "   - Set appropriate min/max instances based on expected load",
    "   - Monitor cold start times and adjust accordingly",
    "   - Use connection pooling for database connections",
    "",
    "3. **Monitoring**",
    "   - Set up alerts for error rates and latency",
    "   - Monitor token usage and costs",
    "   - Track session memory usage",
    "",
    "4. **Data Compliance**",
    "   - Ensure HIPAA compliance for healthcare data",
    "   - Implement audit logging",
    "   - Configure data retention policies",
    "",
    "5. **Cost Optimization**",
    "   - Use preemptible instances for non-critical workloads",
    "   - Set min_instances to 0 for development",
    "   - Monitor and optimize API call frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nThis notebook provides a complete implementation of an Agent Development Environment (ADE) for Healthcare Data Documentation with the following features:\n\n### Core Components\n\nâœ… **SQLite Database** - Persistent storage with sessions and memory tables  \nâœ… **Toon Notation Encoding** - 40-70% token reduction for efficient context  \nâœ… **Snippet Manager** - Named context storage and retrieval  \nâœ… **Review Queue (HITL)** - Human-in-the-loop approval workflows  \nâœ… **Multi-Agent Pipeline** - DataParser â†’ TechnicalAnalyzer â†’ DomainOntology â†’ PlainLanguage â†’ Assembler  \nâœ… **Session Management** - ADK-style state persistence  \nâœ… **Memory Services** - Long-term knowledge storage  \nâœ… **Observability** - Logging and monitoring throughout  \n\n### Production Deployment\n\nâœ… **Vertex AI Agent Engine** - Fully managed, auto-scaling infrastructure  \nâœ… **Container Deployment** - ADK CLI integration  \nâœ… **Cloud Monitoring** - Logs, metrics, and alerts  \nâœ… **Security** - IAM integration and compliance support  \n\n### Key Patterns Implemented\n\n- Retry configuration with exponential backoff\n- Rate limiting for API quota management\n- Context compaction for long conversations\n- Ontology mapping (OMOP, LOINC, SNOMED)\n- Human-readable documentation generation\n\n### Next Steps\n\n1. **Customize agents** for your specific healthcare domain\n2. **Add evaluation test cases** using ADK eval framework\n3. **Implement A2A protocol** for agent-to-agent communication\n4. **Set up continuous deployment** pipeline\n5. **Add custom observability plugins** for your metrics\n\nFor more information, see:\n- [ADK Documentation](https://google.github.io/adk-docs/)\n- [Vertex AI Agent Engine](https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/overview)\n- [OMOP CDM](https://ohdsi.github.io/CommonDataModel/)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}